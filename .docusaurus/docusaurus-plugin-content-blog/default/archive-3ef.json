{
  "blogPosts": [
    {
      "id": "kubernetes/multus-on-kind/",
      "metadata": {
        "permalink": "/kubernetes/multus-on-kind/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2024-09-21-multus-on-kind.md",
        "source": "@site/blog/2024-09-21-multus-on-kind.md",
        "title": "Multus on Kind cluster",
        "description": "Multus를 Kind cluster에 설치하는 방법",
        "date": "2024-09-21T00:00:00.000Z",
        "formattedDate": "September 21, 2024",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "multus",
            "permalink": "/tags/multus"
          },
          {
            "label": "cni",
            "permalink": "/tags/cni"
          },
          {
            "label": "kind",
            "permalink": "/tags/kind"
          },
          {
            "label": "multiple network",
            "permalink": "/tags/multiple-network"
          },
          {
            "label": "macvlan",
            "permalink": "/tags/macvlan"
          },
          {
            "label": "koko",
            "permalink": "/tags/koko"
          },
          {
            "label": "kindnet",
            "permalink": "/tags/kindnet"
          }
        ],
        "readingTime": 23.26,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "Multus on Kind cluster",
          "comments": true,
          "classes": "wide",
          "description": "Multus를 Kind cluster에 설치하는 방법",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/multus-on-kind/",
          "date": "2024-09-21T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "multus",
            "cni",
            "kind",
            "multiple network",
            "macvlan",
            "koko",
            "kindnet"
          ]
        },
        "nextItem": {
          "title": "파드 Readiness & Probe",
          "permalink": "/kubernetes/readinessandprobe/"
        }
      },
      "content": "이전 직장에서 프로젝트 중에 `Multus`를 사용하여 5G환경에서 멀티 네트워크 CNI를 사용해서 패킷을 미러링하고 해당 패킷을 분석하는 프로젝트를 진행했었는데, 당시에 여러 사정으로 아쉽게 된 프로젝트도 존재하고 해서 이번 기회에 시리즈로 정리 해보려고 한다. 이번 포스트에서는 `Kind` cluster에 `Multus`를 구성하고 단일 인스턴스에서 노드끼리 통신할 수 있도록 `koko`를 사용해서 노드간 멀티 네트워크 환경을 구성하는 방법에 대해서 정리해보려고 한다.\n\n## Requirements\n\n- [Kind](https://kind.sigs.k8s.io/): Kind(Kubernetes IN Docker)는 Docker 컨테이너를 사용하여 로컬 Kubernetes 클러스터를 실행할 수 있게 해주는 도구입니다. 주로 테스트 및 개발 목적으로 사용되며, 빠르고 쉽게 Kubernetes 환경을 구축할 수 있습니다. \n- [Multus](https://github.com/k8snetworkplumbingwg/multus-cni): Multus는 Kubernetes에서 여러 네트워크 인터페이스를 지원하기 위한 CNI 플러그인입니다. Multus를 통해 각 Pod가 여러 네트워크에 연결될 수 있으며, 다양한 네트워크 요구 사항을 충족할 수 있다. 특히 5G 워크로드에서 SR-IOV와 함께 사용될 때, Multus는 기본 네트워크 외에도 고성능 네트워크 인터페이스를 제공할 수 있다.\n    - [SR-IOV](https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin): SR-IOV(Single Root I/O Virtualization)는 하나의 물리적 네트워크 인터페이스를 여러 가상 함수(VF)로 나누어 각 Pod에 직접 할당할 수 있게 해주는 기술이다. 이를 통해 네트워크 성능을 크게 향상시킬 수 있으며, 특히 고성능 네트워킹이 필요한 5G 및 NFV(Network Function Virtualization) 환경에서 중요하게 사용되는 기술이다.\n- [Koko](https://github.com/redhat-nfvpe/koko): Koko는 컨테이너 간 네트워크 연결을 위한 도구로, 특히 여러 네트워크 네임스페이스 간의 가상 이더넷 페어를 생성하는 데 사용된다. kind cluster에서 컨테이너로 구성된 노드간 통신을 위해서 사용된다.\n- [Macvlan](https://github.com/containernetworking/plugins/tree/main/plugins/main/macvlan): Macvlan은 네트워크 인터페이스를 가상화하여 여러 개의 가상 네트워크 인터페이스를 생성할 수 있는 기술이다. 이를 통해 각 컨테이너가 고유한 MAC 주소를 가지며, 물리 네트워크와 직접 통신할 수 있다.\n\n<!--truncate-->\n\n## Architecture\n\n![koko](/img/koko.png)\n\n위 그림은 구성하려고 하는 Kind 클러스터와 네트워크 구성을 보여준다. 이 구성은 멀티 네트워크 인터페이스를 사용하여 다양한 네트워크 요구 사항을 충족시키는 Kubernetes 환경을 설정하는 방법을 보여준다. Multus를 사용하면 Pod가 여러 네트워크에 동시에 연결될 수 있어 복잡한 네트워크 토폴로지를 지원할 수 있다.\n\n1. **Kind 클러스터**: 기본적으로 Docker 컨테이너를 사용하여 노드를 생성한다.\n   - **Control Plane**: 클러스터의 제어 영역을 담당하며, `eth0` 인터페이스를 통해 Docker Bridge와 연결된다.\n   - **Worker Node 1 & 2**: 각각의 워커 노드는 Pod 형태로 실행되며, `eth0` 인터페이스를 통해 Docker Bridge와 연결된다.\n\n2. **네트워크 구성**:\n   - **Primary Interface (eth0)**: 각 노드의 기본 네트워크 인터페이스로, Kindnet CNI 플러그인을 통해 연결된다.\n   - **Additional Interface (eth1)**: Multus CNI 플러그인을 사용하여 추가 네트워크 인터페이스를 제공한다. 여기서는 `macvlan` 플러그인을 사용하여 Pod에 추가 네트워크를 제공한다.\n\n3. **Multus CNI**:\n   - Multus는 여러 CNI 플러그인을 사용할 수 있게 해주는 멀티플렉서 역할을 한다. 위 그림에서 처럼 Kindnet과 Macvlan을 함께 사용한다.\n\n4. **Docker 브리지**:\n   - 컨테이너로 생성된 각 노드의 `eth0` 인터페이스는 Docker 브리지와 연결되어 있으며, `veth` 페어를 통해 통신한다.\n\n5. **Koko**:\n   - 컨테이너 간 Point-to-Point 연결을 관리하는 데 사용되는 도구이다. 그림에서 두 워커 노드 간의 직접적인 연결을 설정하는 역할을 한다.\n   - Koko는 가상 이더넷 페어를 생성하여 서로 다른 네트워크 네임스페이스 간의 직접 연결을 가능하게 한다. 이를 통해 Kind 클러스터 내의 워커 노드 간에 추가적인 네트워크 인터페이스(eth1)를 생성하고 연결할 수 있다.\n   - Koko를 사용하면 복잡한 네트워크 토폴로지를 쉽게 구성할 수 있으며, 특히 멀티 네트워크 환경에서 유용하다.\n\n## Kind Cluster 생성\n\n이번 구성은 Intel 기반 Ubuntu 22.04 환경에서 진행되었다.\n\n`config-3node.yml` 파일을 준비한다. 기본적으로 `kindnet` CNI 플러그인을 사용하게 될 것이다. 주석처리된 구문은 커스텀 CNI 플러그인을 사용하는 경우에 사용된다.\n\n```yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: multus-cluster\nnodes:\n    - role: control-plane\n    - role: worker\n    - role: worker\n# Note: uncomment if you install cni plugin by yourself\n#networking:\n#  disableDefaultCNI: true\n```\n\n생성한 yml 파일을 사용하여 클러스터를 생성한다.\n\n```bash\n$ kind create cluster --config config-3node.yml\nCreating cluster \"multus-cluster\" ...\n ✓ Ensuring node image (kindest/node:v1.31.0) 🖼\n ✓ Preparing nodes 📦 📦 📦  \n ✓ Writing configuration 📜 \n ✓ Starting control-plane 🕹️ \n ✓ Installing CNI 🔌 \n ✓ Installing StorageClass 💾 \n ✓ Joining worker nodes 🚜 \nSet kubectl context to \"kind-multus-cluster\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-multus-cluster\n\nNot sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n\n```\n\n클러스터 정보를 확인한다.\n\n```bash\n$ kubectl cluster-info --context kind-multus-cluster\nKubernetes control plane is running at https://127.0.0.1:32935\nCoreDNS is running at https://127.0.0.1:32935/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```\n\nkubeconfig를 export 한다.\n\n```bash\n$ kind export kubeconfig --name multus-cluster\nSet kubectl context to \"kind-multus-cluster\"\n```\n\n클러스터가 성공적으로 생성되었는지 kubectl과 docker 명령어로 확인할 하면, 워커 노드 이름과 docker 컨테이너 이름이 같은 것을 확인할 수 있다.\n\n```bash\n$ kubectl get nodes \nNAME                           STATUS   ROLES           AGE     VERSION\nmultus-cluster-control-plane   Ready    control-plane   7m11s   v1.31.0\nmultus-cluster-worker          Ready    <none>          6m59s   v1.31.0\nmultus-cluster-worker2         Ready    <none>          6m59s   v1.31.0\n\ndocker ps\nCONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS         PORTS                       NAMES\n6e4420eeaf93   kindest/node:v1.31.0   \"/usr/local/bin/entr…\"   9 minutes ago   Up 9 minutes   127.0.0.1:32935->6443/tcp   multus-cluster-control-plane\n923ca6407417   kindest/node:v1.31.0   \"/usr/local/bin/entr…\"   9 minutes ago   Up 9 minutes                               multus-cluster-worker\nde238256402a   kindest/node:v1.31.0   \"/usr/local/bin/entr…\"   9 minutes ago   Up 9 minutes                               multus-cluster-worker2\n```\n\n노드 컨테이너에 sh로 접속해서 `ip a` 명령어로 인터페이스 정보를 확인해보면, 노드에는 `eth0` 인터페이스만 연결되어 있는 것을 확인할 수 있다. 이는 `kindnet` CNI 플러그인이 기본적으로 제공하는 인터페이스이다. \n\n```bash\n$ docker exec -it multus-cluster-worker sh\n# ip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n13: eth0@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fc00:f853:ccd:e793::2/64 scope global nodad \n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe12:2/64 scope link \n       valid_lft forever preferred_lft forever\n```\n\n## Multus 설치\n\nMultus는 `CRD`, `clusterrole`, `clusterrolebinding`, `serviceaccount`, `configmap`, `daemonset`을 설치하는 방식으로 설치한다. \n\n```bash\n$ kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/master/deployments/multus-daemonset.yml\ncustomresourcedefinition.apiextensions.k8s.io/network-attachment-definitions.k8s.cni.cncf.io created\nclusterrole.rbac.authorization.k8s.io/multus created\nclusterrolebinding.rbac.authorization.k8s.io/multus created\nserviceaccount/multus created\nconfigmap/multus-cni-config created\ndaemonset.apps/kube-multus-ds created\n``` \n\n## `koko`를 사용한 노드간 통신 설정\n\n`koko`는 Docker 컨테이너 또는 Linux 네임스페이스 간에 veth 장치를 사용하여 포인트 투 포인트 연결을 설정하는 도구이다. koko는 하나의 호스트에서 두 개의 컨테이너에 대해서는 veth를, 별도의 호스트에서 두 개의 컨테이너에 대해서는 vxlan로 연결을 지원한다. \n\n### koko 설치\n\n테스트를 진행할 노드에서 다음 명령어를 실행하여 `koko`를 설치한다. `koko`를 사용하여 컨테이너 호스트에서 컨테이너를 연결하는 방법은 veth를 사용한다. [Connecting containers in container host using veth](https://github.com/redhat-nfvpe/koko/blob/main/docs/Connecting-containers-in-container-host-using-veth.md) 문서를 참고하면 된다. \n\n```bash\n./koko {-c <linkname> |\n        -d <container>,<linkname>[,<IP addr>/<prefixlen>,...] |\n        -n <netns name>,<linkname>[,<IP addr>/<prefixlen>,...]|\n        -p <pid>,<linkname>[,<IP addr>/<prefixlen>,...]|\n        -c <linkname> }\n       {-d <container>,<linkname>[,<IP addr>/<prefixlen>,...] |\n        -n <netns name>,<linkname>[,<IP addr>/<prefixlen>,...]|\n        -p <pid>,<linkname>[,<IP addr>/<prefixlen>,...]|\n        -c <linkname> }\n```\n\n```bash\n$ curl -LO https://github.com/redhat-nfvpe/koko/releases/download/v0.83/koko_0.83_linux_amd64\n```\n\n다운받은 koko 파일에 실행 권한을 추가하고, sudo 권한으로 각 노드 컨테이너에 `eth1` 인터페이스를 추가하는 명령어를 실행한다. 아래 명령어는 `multus-cluster-worker`와 `multus-cluster-worker2`라는 두 Kind 노드 간에 veth 페어를 생성하고, 각 노드에 인터페이스를 할당하게 된다. \n\n```bash\n$ chmod +x koko_0.83_linux_amd64\n$ sudo ./koko_0.83_linux_amd64 -d multus-cluster-worker,eth1 -d multus-cluster-worker2,eth1\nCreate veth...done\n```\n\n\"Create veth...done\" 메시지는 실제로 새 veth가 생성되었다기 보다는 pair가 생성되어서 기존 docker bridge에 연결된 veth에 각 노드 컨테이너의 eth1 인터페이스가 매핑되어 추가된 것이다. 상세 코드는 [koko.go 파일](https://github.com/redhat-nfvpe/koko/blob/bbe26f6c7e0124815573e22a2f28ff70bfd0db61/koko.go#L595)에서 확인할 수 있다.\n\n\n## CNI 레퍼런스 플러그인 설치\n\n[CNI 레퍼런스 플러그인](https://github.com/containernetworking/plugins)은 컨테이너 네트워크 인터페이스(Container Network Interface)의 표준 구현체이다. 이 플러그인들은 Kubernetes와 같은 컨테이너 오케스트레이션 플랫폼에서 네트워크 기능을 제공하는 데 사용된다. CNI 레퍼런스 플러그인을 설치함으로써 클러스터 관리자는 다양한 네트워킹 요구사항을 충족시키고, 컨테이너 간 통신을 효율적으로 관리할 수 있다. 이번 테스트에서는 macvlan 구성을 진행하여 새로운 MAC 주소를 생성하고, 모든 트래픽을 해당 컨테이너로 전달하는 방식을 사용한다. 설치는 다음 yaml를 사용한다. \n\n```yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: cni-install-sh\n  namespace: kube-system\ndata:\n  install_cni.sh: |\n    cd /tmp\n    wget https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz\n    cd /host/opt/cni/bin\n    tar xvfzp /tmp/cni-plugins-linux-amd64-v1.5.1.tgz\n    sleep infinite\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: install-cni-plugins\n  namespace: kube-system\n  labels:\n    name: cni-plugins\nspec:\n  selector:\n    matchLabels:\n      name: cni-plugins\n  template:\n    metadata:\n      labels:\n        name: cni-plugins\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        kubernetes.io/arch: amd64\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      containers:\n      - name: install-cni-plugins\n        image: alpine\n        command: [\"/bin/sh\", \"/scripts/install_cni.sh\"]\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: cni-bin\n          mountPath: /host/opt/cni/bin\n        - name: scripts\n          mountPath: /scripts\n      volumes:\n        - name: cni-bin\n          hostPath:\n            path: /opt/cni/bin\n        - name: scripts\n          configMap:\n            name: cni-install-sh\n            items:\n            - key: install_cni.sh\n              path: install_cni.sh\n```\n\ncni 인스톨을 설치하는 스크립트는 cni-install-sh configmap에 정의되어 있다. 해당 플러그인은 노드마다 설치되어야 하므로 daemonset으로 설치한다. 글쓰는 당시의 최신 버전인 1.5.1으로 설치한다.\n\n```bash\n$ kubectl create -f cni.yaml \nconfigmap/cni-install-sh created\ndaemonset.apps/install-cni-plugins created\n```\n\n## macvlan을 사용하여 두 개의 컨테이너 생성\n\n### macvlan\n\n`macvlan`은 네트워크 인터페이스를 가상화하여 여러 개의 가상 네트워크 인터페이스를 생성할 수 있는 기술이다. 이를 통해 각 컨테이너가 고유한 MAC 주소를 가지며, 물리 네트워크와 직접 통신할 수 있다. macvlan은 Parent Inteface를 이용하여 여러개의 Child Interface를 생성한다. Child Interface는 각각 별도의 MAC Address와 macvlan Mode를 가질 수 있다. Mode에 따라서 Child Inteface간의 통신은 가능하지만, Mode에 관계없이 Parent Interface와 Child Interface는 서로 절대로 통신이 불가능한게 macvlan의 특징 중 하나이다.\n\n\n### NetworkAttachmentDefinition\n\n`NetworkAttachmentDefinition`은 Multus CNI를 사용하여 네트워크 인터페이스를 추가할 때 사용되는 오브젝트이다. 이를 통해 컨테이너에 추가적인 네트워크 인터페이스를 할당할 수 있다. 아래와 같이 macvlan을 사용하여 네트워크 인터페이스를 추가해보자.\n\n```yaml\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: macvlan-conf\nspec: \n  config: '{\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"macvlan\",\n          \"capabilities\": { \"ips\": true },\n          \"master\": \"eth1\",\n          \"mode\": \"bridge\",\n          \"ipam\": {\n            \"type\": \"static\",\n            \"routes\": [\n              {\n                \"dst\": \"0.0.0.0/0\",\n                \"gw\": \"10.1.1.1\"\n              }\n            ] \n          }\n        }, {\n          \"capabilities\": { \"mac\": true },\n          \"type\": \"tuning\"\n        }\n      ]\n    }'\n```\n\n이 `NetworkAttachmentDefinition`의 각 부분을 하나씩 살펴보자. 이 구성은 `macvlan`을 사용하여 새로운 네트워크 인터페이스를 생성하고, 정적 IP 할당 및 라우팅을 설정한다. 또한 `tuning` 플러그인을 통해 MAC 주소 설정 기능을 추가로 제공하게 된다. \n\n- name: \"macvlan-conf\"로 이 `NetworkAttachmentDefinition`의 이름을 지정한다.\n- cniVersion: 사용되는 CNI 스펙 버전을 나타냅니다. 여기서는 0.3.1 버전을 사용한다.\n- plugins: CNI 플러그인 목록을 정의한다.\n  - type: \"macvlan\" 플러그인 사용\n  - capabilities: IP 주소 할당 기능 활성화\n  - master: \"eth1\"을 마스터 인터페이스로 사용\n  - mode: \"bridge\" 모드로 동작\n  - ipam: IP 주소 관리 설정\n     - type: \"static\"으로 정적 IP 할당 사용\n     - routes: 기본 라우트 설정 (게이트웨이: 10.1.1.1)\n  - capabilities: MAC 주소 설정 기능 활성화\n  - type: \"tuning\" 플러그인은 MAC 주소 설정 기능을 활성화하기 위해 사용\n\n\n`NetworkAttachmentDefinition` 리소스를 생성한다.\n\n```bash\n$ kubectl apply -f nad.yaml \nnetworkattachmentdefinition.k8s.cni.cncf.io/macvlan-conf created\n```\n\n모든 준비가 완료되었다. 이제 멀티 네트워크 파드를 생성해보자. \n\n## 멀티 네트워크 파드 생성\n\n멀티 네트워크 파드를 생성하기 위해서는 `annotations`에 네트워크 인터페이스를 추가해야 한다. 이전에 구성했던 macvlan-conf를 추가하고, 각 파드에 할당될 IP 주소와 게이트웨이를 설정한다. 그리고 `netshoot` 이미지를 사용하여 파드를 생성한다. 또한 파드에 대한 네트워크 명령어 실행을 위해 `securityContext`를 통해 파드에 대한 권한을 설정하고, 파드가 종료될 때 자원을 정리하는 설정을 한다. 그리고 각 노드에 대한 선택과 통신 확인을 위해 `nodeSelector` 설정으로 이미 구성한 각 노드 컨테이너에 파드를 배치한다.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  annotations:\n    k8s.v1.cni.cncf.io/networks: '[\n            { \"name\": \"macvlan-conf\",\n              \"ips\": [ \"10.1.1.101/24\" ],\n              \"gateway\": [ \"10.1.1.1\" ]\n            }]'\nspec:\n  containers:\n  - name: pod1\n    image: nicolaka/netshoot\n    command: [\"tail\"]\n    args: [\"-f\", \"/dev/null\"]\n    securityContext:\n      privileged: true\n  terminationGracePeriodSeconds: 0\n  nodeSelector:\n    kubernetes.io/hostname: multus-cluster-worker\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  annotations:\n    k8s.v1.cni.cncf.io/networks: '[\n            { \"name\": \"macvlan-conf\",\n              \"ips\": [ \"10.1.1.102/24\" ],\n              \"gateway\": [ \"10.1.1.1\" ]\n            }]'\nspec:\n  containers:\n  - name: pod2\n    image: nicolaka/netshoot\n    command: [\"tail\"]\n    args: [\"-f\", \"/dev/null\"]\n    securityContext:\n      privileged: true\n  terminationGracePeriodSeconds: 0\n  nodeSelector:\n    kubernetes.io/hostname: multus-cluster-worker2\n```\n\n파드를 생성한다.\n\n```bash\n$ kubectl apply -f pod.yaml                                                                   \npod/pod1 created\npod/pod2 created\n```\n\n## 파드간 신규 네트워크 인터페이스 통신 확인\n\n먼저 파드에 할당된 네트워크 인터페이스를 확인한다. 먼저 아키텍처 구성도에 따라서 파드에 할당된 네트워크 인터페이스를 확인 할 수 있다. eth0은 노드의 기존 kindnet 네트워크 인터페이스이고, net1은 multus에 의해 새로 생성된 macvlan 네트워크 인터페이스이다.\n\n```bash\n$ kubectl exec -it pod1 -- ip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host proto kernel_lo \n       valid_lft forever preferred_lft forever\n2: eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n    link/ether ba:aa:3b:d1:7c:50 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.244.1.2/24 brd 10.244.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::b8aa:3bff:fed1:7c50/64 scope link proto kernel_ll \n       valid_lft forever preferred_lft forever\n3: net1@if20: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n    link/ether a6:64:d5:ee:c4:91 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.1.1.101/24 brd 10.1.1.255 scope global net1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a464:d5ff:feee:c491/64 scope link proto kernel_ll \n       valid_lft forever preferred_lft forever\n\n$ kubectl exec -it pod2 -- ip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host proto kernel_lo \n       valid_lft forever preferred_lft forever\n2: eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n    link/ether fa:82:35:f3:28:62 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.244.2.2/24 brd 10.244.2.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::f882:35ff:fef3:2862/64 scope link proto kernel_ll \n       valid_lft forever preferred_lft forever\n3: net1@if19: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n    link/ether 7e:af:c9:aa:18:8b brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 10.1.1.102/24 brd 10.1.1.255 scope global net1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::7caf:c9ff:feaa:188b/64 scope link proto kernel_ll \n       valid_lft forever preferred_lft forever\n```\n\n각 파드에서 신규로 추가된 네트워크 인터페이스로의 통신을 확인한다.\n\n```bash\n$ kubectl exec -it pod1 -- ping 10.1.1.102\nPING 10.1.1.102 (10.1.1.102) 56(84) bytes of data.\n64 bytes from 10.1.1.102: icmp_seq=1 ttl=64 time=0.065 ms\n64 bytes from 10.1.1.102: icmp_seq=2 ttl=64 time=0.034 ms\n64 bytes from 10.1.1.102: icmp_seq=3 ttl=64 time=0.035 ms\n\n$ kubectl exec -it pod2 -- ping 10.1.1.101\nPING 10.1.1.101 (10.1.1.101) 56(84) bytes of data.\n64 bytes from 10.1.1.101: icmp_seq=1 ttl=64 time=0.027 ms\n64 bytes from 10.1.1.101: icmp_seq=2 ttl=64 time=0.032 ms\n64 bytes from 10.1.1.101: icmp_seq=3 ttl=64 time=0.026 ms\n```\n\n원래 CNI를 통해 할당된 네트워크 인터페이스는 파드간 통신도 확인할 수 있다.\n\n```bash\n$ kubectl exec -it pod1 -- ping 10.244.2.2\nPING 10.244.2.2 (10.244.2.2) 56(84) bytes of data.\n64 bytes from 10.244.2.2: icmp_seq=1 ttl=62 time=0.136 ms\n64 bytes from 10.244.2.2: icmp_seq=2 ttl=62 time=0.081 ms\n64 bytes from 10.244.2.2: icmp_seq=3 ttl=62 time=0.079 ms\n\n$ kubectl exec -it pod2 -- ping 10.244.2.1                                                    \nPING 10.244.2.1 (10.244.2.1) 56(84) bytes of data.\n64 bytes from 10.244.2.1: icmp_seq=1 ttl=64 time=0.041 ms\n64 bytes from 10.244.2.1: icmp_seq=2 ttl=64 time=0.045 ms\n64 bytes from 10.244.2.1: icmp_seq=3 ttl=64 time=0.031 ms\n```\n\n## 정리\n\n이 포스트에서는 Kind 클러스터에 Multus를 설치하고 koko를 사용하여 노드 간 멀티 네트워크 환경을 구성하는 방법에 대해 알아봤다. 주요 내용을 요약하면 다음과 같다.\n\n1. Kind를 사용하여 하나의 호스트에서 Kubernetes 클러스터를 생성한다.\n2. Multus CNI를 설치하여 다중 네트워크 인터페이스 지원을 추가한다.\n3. koko 도구를 사용하여 노드 간 직접 통신을 위한 veth 페어를 생성한다.\n4. CNI 레퍼런스 플러그인을 설치하여 macvlan 네트워크를 구성한다.\n5. NetworkAttachmentDefinition을 생성하여 Multus에 macvlan 네트워크를 정의한다.\n6. 테스트용 파드를 배포하여 추가된 네트워크 인터페이스를 통한 통신을 확인한다.\n\n이 구성을 통해 단일 Kind 클러스터 내에서 다중 네트워크 환경을 시뮬레이션할 수 있었다. 이는 복잡한 네트워크 토폴로지가 필요한 애플리케이션 개발 및 테스트에 유용할 수 있다. 특히 5G 네트워크나 NFV 환경과 같이 고성능 네트워킹이 필요한 시나리오에서 이러한 설정이 도움이 될 수 있다.\n\n이 접근 방식은 개발 및 테스트 환경에서 유용하지만, 프로덕션 환경에서는 추가적인 고려사항과 최적화가 필요할 수 있다. 네트워크 보안, 성능, 확장성 등의 측면에서 더 깊이 있는 검토가 필요할 것이다.\n\n초반에 멀티 네트워크 CNI를 사용해서 패킷을 미러링하고 해당 패킷을 분석하는 프로젝트를 진행했었다는 내용을 언급했었는데, 다음 포스팅에서는 multus와 컨테이너 미러링 도구로 패킷을 미러링하고 pcap exporter를 사용해서 패킷의 5-tuple 정보를 분석하는 방법에 대해서 적어보려고 한다."
    },
    {
      "id": "kubernetes/readinessandprobe/",
      "metadata": {
        "permalink": "/kubernetes/readinessandprobe/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2024-09-08-probe.md",
        "source": "@site/blog/2024-09-08-probe.md",
        "title": "파드 Readiness & Probe",
        "description": "파드 Readiness & Probe를 통한 상태 모니터링",
        "date": "2024-09-08T00:00:00.000Z",
        "formattedDate": "September 8, 2024",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Probe",
            "permalink": "/tags/probe"
          },
          {
            "label": "livenessProbe",
            "permalink": "/tags/liveness-probe"
          },
          {
            "label": "readinessProbe",
            "permalink": "/tags/readiness-probe"
          },
          {
            "label": "startupProbe",
            "permalink": "/tags/startup-probe"
          },
          {
            "label": "NetworkReadiness",
            "permalink": "/tags/network-readiness"
          }
        ],
        "readingTime": 16.41,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "파드 Readiness & Probe",
          "comments": true,
          "classes": "wide",
          "description": "파드 Readiness & Probe를 통한 상태 모니터링",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/readinessandprobe/",
          "date": "2024-09-08T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "Probe",
            "livenessProbe",
            "readinessProbe",
            "startupProbe",
            "NetworkReadiness"
          ]
        },
        "prevItem": {
          "title": "Multus on Kind cluster",
          "permalink": "/kubernetes/multus-on-kind/"
        },
        "nextItem": {
          "title": "커널 강화 도구 사용하기",
          "permalink": "/kubernetes/kernelhardening/"
        }
      },
      "content": "Kubernetes에서 파드의 Readiness 및 프로브는 파드의 상태를 모니터링하고 트래픽을 효율적으로 관리하기 위한 중요한 메커니즘이다. 프로브는 파드가 요청을 처리할 준비가 되었는지를 판단하는 데 사용되며, Kubernetes가 파드를 관리하고 업데이트하는 데 중요한 역할을 한다.\n\n## 파드 Readiness 및 프로브\n\n파드 Readiness는 따라 파드가 트래픽을 처리할 준비가 되었는지를 나타내는 추가적인 지표이다. 파드 Readiness는 외부 소스에서 파드 주소가 Endpoints 객체에 표시되는지를 결정한다. Kubernetes에서 파드를 관리하는 다른 리소스들, 예를 들어 Deployment 같은 것들은 파드 Readiness를 고려하여 롤링 업데이트 시 의사 결정을 한다. 롤링 배포 중에 새 파드가 준비되었지만 서비스, 네트워크 정책 또는 로드 밸런서가 어떤 이유로 인해 아직 새 파드에 대해 준비되지 않은 경우가 있을 수 있다. 만약 파드의 Readiness 프로브가 실패하면, 해당 파드의 IP 주소는 `Endpoints` 객체에서 제거되어 서비스가 그 파드로 트래픽을 라우팅하지 않는다. 이는 서비스 중단을 방지하기 위한 메커니즘이다. Readiness 프로브는 파드 `.Status.Phase` 에 영향을 줄 수 있으며, Kubelet이 이를 실행하여 성공 또는 실패에 따라 파드 상태를 업데이트한다. \n\n파드에 프로브가 명시되어 있지 않으면 Kubernetes가 기본적으로 해당 프로브의 상태를 성공으로 간주한다. 이런 경우 Kubernetes가 컨테이너를 Ready 상태로 간주하여 서비스의 Endpoints에 포함시키고 트래픽을 받을 수 있도록 한다. 그러나 이는 실제로 컨테이너가 준비되지 않았을 때도 트래픽을 받을 수 있어 사용자 경험에 부정적인 영향을 미칠 수 있다. 따라서, 애플리케이션의 상태를 정확히 반영하기 위해 적절한 프로브를 설정하는 것이 중요하다.\n\n<!--truncate-->\n\n다음은 파드 단계별 설명이다. \n\n- `Pending`: 클러스터에서 파드를 수락했지만 하나 이상의 컨테이너가 설정되어 실행할 준비가 되지 않은 것을 말함. 파드가 스케줄을 기다리는 시간과 네트워크를 통해 컨테이너 이미지를 다운로드하는 데 소요되는 시간이 포함된다. \n- `Running`: 파드가 노드에 스케줄되었고 모든 컨테이너가 생성된 경우이다. 하나 이상의 컨테이너가 여전히 실행 중이거나 시작 또는 재시작 중인 경우가 있다. 일부 컨테이너는 `CrashLoopBackoff` 와 같이 실패한 상태일 수 있다.\n- `Succeeded`: 파드의 모든 컨테이너가 성공적으로 종료되었으며 다시 시작되지 않는다.\n- `Failed`: 파드의 모든 컨테이너가 종료되었고 하나 이상의 컨테이너가 실패로 종료되었을 경우이다. 컨테이너가 `nonzero` 상태로 종료되었거나 시스템에 의해 종료된 경우이다.\n- `Unknown`: 특정 이유로 파드의 상태를 확인할 수 없는 경우다. 이 단계는 일반적으로 파드가 실행되어야 하는 Kubelet과의 통신 오류로 인해 발생한다. \n\nKubelet은 Kubernetes 노드에서 실행되는 에이전트로, 파드의 개별 컨테이너에 대해 다양한 헬스 체크를 수행한다. 이 헬스 체크는 `livenessProbe`, `readinessProbe`, `startupProbe` 세 가지 유형으로 나뉜다. 각 프로브는 특정 진단을 수행하며, 그 결과는 다음 세 가지 중 하나로 나타난다.\n\n- `Success`: 컨테이너가 진단을 통과했음\n- `Failure`: 컨테이너가 진단을 통과하지 못했음\n- `Unknown`: 진단 자체가 실패하여 아무런 조치를 취할 수 없는 상태\n\n이러한 프로브는 HTTP 요청, 바이너리 명령 실행, TCP 연결을 통해 수행될 수 있다. 예를 들어, HTTP 프로브는 특정 엔드포인트에 HTTP GET 요청을 보내 응답 코드가 성공(예: 200 OK)인지를 확인한다. TCP 프로브는 특정 포트가 열려 있는지를 확인하며, 명령 프로브는 컨테이너 내에서 명령을 실행하여 성공 여부를 판단한다. 프로브가 `failureThreshold`보다 더 많이 실패하면, 검사에 실패한 것으로 간주한다. \n\n### Liveness 프로브\n\nLiveness 프로브는 잘못 사용하거나 잘못 구성하면 예기치 않은 장애를 쉽게 일으킬 수 있다. Liveness 프로브를 주로 사용하는 경우는 컨테이너를 다시 시작해야 할 시기를 Kubelet에 알려주는 것이다. 애플리케이션이 살아있고 응답 가능한 상태인지를 확인한다. 만약 이 프로브가 실패하면, 컨테이너는 재시작된다. 이는 애플리케이션이 데드락 상태에 빠졌을 때에도 유용하다.\n\n하지만 잘못 사용하면 위험한 전략이 될 수 있다. 예를 들어, 웹 메인 페이지를 로드하는 Liveness 프로브를 생성한 상태에서 외부의 시스템 변경으로 인해 메인 페이지가 404 또는 500 에러를 반환한다고 가정해 보자. 이러한 시나리오에서 Liveness 프로브는 컨테이너를 재시작한다. 컨테이너를 다시 시작한다고 해서 시스템의 다른 곳에서 문제가 해결되지 않기 때문에 오히려 장애를 악화시킬 수 있다. 컨테이너 `CrashLoopBackoff`가 발생하게 되는데 있는데, 이는 실패한 컨테이너를 재시작하는 데 시간을 계속 증가시킨다. \n\n- HTTP probe: 컨테이너 IP 주소로 HTTP GET 요청을 수행하며 200에서 399 사이의 성공적인 HTTP 응답 코드를 기대\n- TCP Socket probe: TCP 연결이 성공했다고 가정\n- Exec probe: 컨테이너 커널 네임스페이스에서 임의의 명령을 실행하고 성공적인 종료 코드(0)를 기대\n- gRPC probe: 상태 확인을 위해 gRPC의 자체 기능 활용\n\n프로브 동작 외에도 다음 매개변수를 사용하여 상태 확인 동작에 영향을 줄 수 있다.\n- `initialDelaySeconds`: 첫 번째 liveness probe가 확인될 때까지 대기할 시간(초)을 지정\n- `periodSeconds`: liveness probe 검사 간격(초)\n- `timeoutSeconds`:프로브 검사가 실패로 간주되기 전에 반환될 때까지 허용되는 최대 시간\n- `failureThreshold`: 컨테이너가 정상이 아닌 것으로 간주되어 다시 시작해야 할 때까지 프로브 검사가 연속으로 실패해야 하는 빈도를 지정\n\n### Readiness 프로브\n\nReadiness 프로브는 정상이 아닌 컨테이너를 삭제하고 새 컨테이너로 교체하여 애플리케이션을 건강하게 유지하는 데 도움이  될 수 있다. 그러나 때로 컨테이너가 비정상적인 경우 다시 시작해도 문제가 해결되지 않는 경우가 있다. 대표적인 예로 애플리케이션이 데이터베이스와 같은 종속성을 사용할 수 있기를 기다리는 경우이다. 컨테이너에 과부하가 걸려 latency가 증가하는 경우 잠시 동안 추가 부하로부터 스스로를 보호하고 부하가 감소할 때까지 준비가 되지 않았다는 것을 표시해야 한다.\n\nhttps://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ 에서 readinessGates 를 통해 추가적인 AWS ALB 상태를 체크하고 정상일때 Readiness Probe 체크가 되도록 아래와 같이 추가 피드백이나 신호를 파드 상태에 주입할 수 있다.\n\n#### Custom Pod Readiness Gates for AWS Load Balancer controller\nhttps://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.8/deploy/pod_readiness_gate/\n\n\n### Startup 프로브\n애플리케이션을 시작하는 데 오랜시간(몇 분이상)이 걸리는 상황에서 Kubernetes는 Startup 프로브를 제공한다. Liveness 프로브와 동일한 형식으로 구성되지만 프로브 동작과 타이밍 파라미터에 대해 다른 값을 허용한다. `periodSeconds` 및 `failureThreshold` 설정을 통해 오래 걸리는 애플리케이션 시작을 고려하기 위해 해당 Liveness 프로브에 비해 훨씬 더 큰 값으로 구성된다. `startupProbe`가 성공한 이후에만 `livenessProbe`와 `readinessProbe`를 구성할 수 있다. \n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-startup-check\nspec:\n  containers:\n  - image: quay.io/wildfly/wildfly \n    name: wildfly\n    startupProbe:\n      exec:\n        command: [ \"stat\", \"/opt/jboss/wildfly/standalone/tmp/startup-marker\" ]  \n        initialDelaySeconds: 60    \n        periodSeconds: 60\n        failureThreshold: 15\n    livenessProbe:\n      httpGet:\n        path: /health\n        port: 9990\n        periodSeconds: 10          \n        failureThreshold: 3\n```\n\n- `exec`: `/opt/jboss/wildfly/standalone/tmp/startup-marker` 파일의 존재 여부를 확인한다. 이 파일이 존재하면 컨테이너가 시작된 것으로 간주\n- `initialDelaySeconds`: 컨테이너가 15분 후(60초 x 15회, 최초 딜레이 60초) 에도 startupProbe를 통과하지 못했을 때 컨테이너를 다시 시작하도록 지정하는 매개변수\n- `periodSeconds`: 프로브가 실행되는 주기로, 60초마다 프로브가 실행\n- `failureThreshold`: 실패 허용 횟수로, 15번 실패하면 컨테이너가 비정상으로 간주\n\n위 예시는 startupProbe가 미들웨어인 Wildfly 컨테이너가 시작되었는지 확인한다. 컨테이너가 시작되기 전까지 다른 프로브(liveness, readiness)는 실행되지 않는다. 예시와 같이 컨테이너를 시작하는 데 몇 분이 걸리지만 시작 후 컨테이너가 상태가 좋지 않으면 빠르게 종료하는 케이스로 주로 사용한다. 이후에 livenessProbe는 컨테이너의 지속적인 실행 상태를 확인한다. /health 엔드포인트에 대한 HTTP 요청을 통해 Wildfly 서버가 정상적으로 작동 중인지 확인한다.\n\n아래 예시는 Golang 웹 서버에는 8080 포트에서 /healthz 경로로 HTTP GET을 수행하는 livenessProbe 프로브가 있고, readinessProbe 프로브는 동일한 포트에서 /를 체크한다.\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: go-web\n  labels:\n    test: liveness\nspec:\n  containers:\n  - name: go-web\n    image: go-web:v0.0.1\n    ports:\n    - containerPort: 8080\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 5\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 5\n      periodSeconds: 5\n```\n\n### readiness gates\n\nReadiness gates를 사용하면 파드 내부의 애플리케이션이 언제 준비되었는지 확인할 수 있다. Kubernetes 1.14부터 사용 가능했고, Readiness gates를 사용하기 위해 Kubelet이 파드 Readiness를 평가하는 추가 조건으로 파드의 스펙에 `readinessGates`를 추가한다.Readiness 게이트는 파드의 `status.condition` 필드의 현재 상태에 의해 제어되며, Kubelet이 파드의 `status.conditions` 필드에서 해당 조건을 찾을 수 없는 경우 조건의 상태는 기본값이 False로 설정된다.\n\n아래 예에서 볼 수 있듯이, `feature-1` Readiness gates는 `False`이고 `feature-2`는 `True`이므로 파드의 상태는 `False`가 된다. \n\n```\nkind: Pod\n...\nspec:\n  readinessGates:\n    - conditionType: www.example.com/feature-1\n    - conditionType: www.example.com/feature-2\n...\nstatus:\n  conditions:\n    - lastProbeTime: null\n      lastTransitionTime: 2024-08-25T00:00:00Z\n      status: \"False\"\n      type: Ready\n    - lastProbeTime: null\n      lastTransitionTime: 2024-08-25T00:00:00Z\n      status: \"False\"\n      type: www.example.com/feature-1\n    - lastProbeTime: null\n      lastTransitionTime: 2024-08-25T00:00:00Z\n      status: \"True\"\n      type: www.example.com/feature-2\n  containerStatuses:\n    - containerID: docker://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n      ready: true\n```\n\n사용자 정의 조건을 사용하는 파드의 경우, 해당 파드는 다음 문이 모두 적용되는 경우에만 준비된 것으로 평가된다.\n- 파드의 모든 컨테이너가 준비 상태\n- `readinessGates`에 지정된 모든 조건이 `True`\n\n### 파드 Network Readiness\n\n파드 Network Readiness는 Kubernetes v1.29에서 beta가 된 readiness로 Kubernetes에서 파드가 네트워크 요청을 처리할 준비가 되었는지를 확인하는 중요한 조건이다. 파드가 클러스터 내에서 트래픽을 수신할 수 있는 상태인지 판단하는 데 사용되고 파드가 네트워크 준비 상태가 되면, 해당 파드는 서비스의 로드 밸런싱 풀에 추가되어 트래픽을 처리할 수 있게 된다. \n\n`PodReadyToStartContainersCondition`는 [feature gate](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)가 활성화되면(1.31 버전 부터는 기본으로 탑재될 예정) 파드의 `status.conditions`에 `PodReadyToStartContainers` 조건이 추가된다. \n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  containers:\n  - name: example-container\n    image: example-image\nstatus:\n  conditions:\n  - type: PodReadyToStartContainers\n    status: \"True\"\n```\n\n위 예시에서 `PodReadyToStartContainers` 조건이 True로 되어 있으면, kubelet은 컨테이너 이미지를 풀링하고 컨테이너를 생성할 준비가 되었음을 의미한다.\n\n## 결론\nKubernetes의 Readiness 및 프로브는 파드의 상태를 모니터링하고 트래픽을 효율적으로 관리하는 데 필수적이다. 적절한 프로브 설정을 통해 애플리케이션의 가용성과 안정성을 높일 수 있다. 프로브를 설정할 때는 애플리케이션의 특성과 요구 사항에 맞게 구성하는 것이 중요하다."
    },
    {
      "id": "kubernetes/kernelhardening/",
      "metadata": {
        "permalink": "/kubernetes/kernelhardening/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2024-08-31-kernelhardening.md",
        "source": "@site/blog/2024-08-31-kernelhardening.md",
        "title": "커널 강화 도구 사용하기",
        "description": "AppArmor와 seccomp를 사용한 커널 강화",
        "date": "2024-08-31T00:00:00.000Z",
        "formattedDate": "August 31, 2024",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "AppArmor",
            "permalink": "/tags/app-armor"
          },
          {
            "label": "seccomp",
            "permalink": "/tags/seccomp"
          },
          {
            "label": "security",
            "permalink": "/tags/security"
          },
          {
            "label": "CKS",
            "permalink": "/tags/cks"
          }
        ],
        "readingTime": 15.94,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "커널 강화 도구 사용하기",
          "comments": true,
          "classes": "wide",
          "description": "AppArmor와 seccomp를 사용한 커널 강화",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/kernelhardening/",
          "date": "2024-08-31T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "AppArmor",
            "seccomp",
            "security",
            "CKS"
          ]
        },
        "prevItem": {
          "title": "파드 Readiness & Probe",
          "permalink": "/kubernetes/readinessandprobe/"
        },
        "nextItem": {
          "title": "prometheus grafana 스택으로 k6 테스트 결과 확인하기",
          "permalink": "/kubernetes/k6-prometheus/"
        }
      },
      "content": "컨테이너 내부에서 실행 중인 애플리케이션이나 프로세스는 시스템 호출(system call)을 수행할 수 있다. 대표적인 예로 HTTP 요청을 수행하는 curl 명령을 들 수 있다. 시스템 호출은 커널에 서비스를 요청하기 위해 사용자 공간에서 실행되는 추상화된 프로그래밍인데, 커널 강화 도구를 사용하여 허용되는 시스템 호출을 제한할 수 있으며, CKS 시험에서는 AppArmor와 seccomp라는 두 가지 도구를 명시적으로 언급하고 있다. 이 두 도구는 컨테이너화된 환경에서 보안을 강화하는 데 중요한 역할을 하며, Kubernetes와의 통합을 통해 보다 안전한 클러스터 운영을 지원한다. 이 두 도구와 쿠버네티스와 통합하는 메커니즘에 대해 설명한다.\n\n## AppArmor\n[AppArmor](https://apparmor.net/)는 Linux 시스템에서 실행되는 프로그램에 대한 액세스 제어 기능을 제공한다. AppArmor는 경로 기반으로 작동하며, 프로필을 통해 특정 프로그램이나 컨테이너가 필요한 접근만 허용하도록 설정할 수 있다. Kubernetes에서는 AppArmor 프로필을 Pod 또는 컨테이너 수준에서 지정할 수 있으며, securityContext를 통해 적용한다. 이 도구는 user space에서 호출되는 애플리케이션과 기본 시스템 기능 사이에 추가적인 보안 계층을 구현한다. 네트워크 호출 또는 파일 시스템 상호 작용을 제한할 수 있다. 많은 Linux 배포판(예: Debian, Ubuntu, openSUSE)이 AppArmor를 기본으로 제공한다. AppArmor를 지원하지 않는 Amazon Linux와 같은 배포판은 AppArmor와 유사한 접근 방식을 취하는 [SELinux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux)를 대신 사용할 수 있다.  \n\n<!--truncate-->\n\n### 프로파일(profile) 이해\n프로그램이 수행할 수 있는 작업과 수행할 수 없는 작업을 정의하는 규칙은 AppArmor 프로필에 정의된다. 모든 프로필을 적용하려면 먼저 AppArmor에 로드해야 한다. AppArmor는 로드된 프로파일을 확인할 수 있는 명령줄 도구를 제공한다. `aa-status` 명령을 실행하면 로드된 모든 프로파일의 요약을 확인할 수 있다. AppArmor에는 Linux 서비스를 보호하기 위한 기본 애플리케이션 프로파일 세트가 이미 포함되어 있음을 확인할 수 있다.\n\n```\n$ sudo aa-status\napparmor module is loaded.\n43 profiles are loaded.\n41 profiles are in enforce mode.\n   /snap/snapd/21759/usr/lib/snapd/snap-confine\n   ...\n2 profiles are in complain mode.\n   snap.amazon-ssm-agent.amazon-ssm-agent\n   snap.amazon-ssm-agent.ssm-cli\n0 profiles are in kill mode.\n0 profiles are in unconfined mode.\n9 processes have profiles defined.\n2 processes are in enforce mode.\n   /usr/sbin/chronyd (429)\n   /usr/sbin/chronyd (434)\n7 processes are in complain mode.\n   /snap/amazon-ssm-agent/7993/amazon-ssm-agent (104231) snap.amazon-ssm-agent.amazon-ssm-agent\n   /snap/amazon-ssm-agent/7993/ssm-agent-worker (104261) snap.amazon-ssm-agent.amazon-ssm-agent\n   /snap/amazon-ssm-agent/7993/ssm-session-worker (138449) snap.amazon-ssm-agent.amazon-ssm-agent\n   /usr/bin/dash (138466) snap.amazon-ssm-agent.amazon-ssm-agent\n   /usr/bin/sudo (138468) snap.amazon-ssm-agent.amazon-ssm-agent\n   /usr/bin/sudo (138469) snap.amazon-ssm-agent.amazon-ssm-agent\n   /usr/sbin/aa-status (138470) snap.amazon-ssm-agent.amazon-ssm-agent\n0 processes are unconfined but have a profile defined.\n0 processes are in mixed mode.\n0 processes are in kill mode.\n```\n프로파일 모드는 일치하는 이벤트가 발생할 경우 런타임에 규칙의 처리를 결정한다. AppArmor는 두 가지 유형의 프로필 모드를 구분한다.\n\n#### Enforce\n시스템이 규칙을 적용하고 위반을 리포트하고 시스템 로그에 기록한다. 이 모드를 사용하면 프로그램이 특정 호출을 하지 못하도록 방지할 수 있다.\n\n#### Complain\n시스템에서 규칙을 적용하지는 않지만 위반 사항을 로그에 기록한다. 이 모드는 프로그램의 시스템 호출을 발견하려는 경우에 유용하다.\n\n아래 프로파일은 파일 쓰기 액세스를 제한하기 위해 k8s-deny-write 파일에 사용자 지정 프로필을 정의하는 부분이다. 해당 파일은 워크로드를 실행하는 모든 워커 노드의 /etc/apparmor.d 디렉터리에 배치해야 한다. 자세한 문법은 [AppArmor](https://gitlab.com/apparmor/apparmor/-/wikis/QuickProfileLanguage) 위키를 참조하자.\n\n```\n#include <tunables/global>\n\nprofile k8s-deny-write flags=(attach_disconnected) { //profile 키워드 뒤의 식별자는 프로필의 이름이다.\n  #include <abstractions/base> \n  file, //파일 작업에 적용한다.\n  deny /** w, //모든 파일 쓰기를 거부한다.\n}\n```\n\n### 사용자 지정 프로필 설정\nAppArmor에 프로파일을 로드하려면 워커 노드에서 다음 명령을 실행한다.\n\n```\nsudo apparmor_parser /etc/apparmor.d/k8s-deny-write\n```\n\n해당 명령은 기본적으로 `Enforce` 모드를 사용한다. `Complain` 모드에서 프로파일을 로드하려면 -C 옵션을 사용한다. 다시 aa-status 명령을 수행하면 기본 프로필과 함께 프로필을 보여준다. `Enforce` 모드를 사용하는 것을 확인할 수 있다. \n\n```\n$ sudo aa-status\napparmor module is loaded.\n49 profiles are loaded.\n45 profiles are in enforce mode.\n   ...\n   docker-default\n   k8s-deny-write\n   ...\n```\n\nAppArmor는 유틸리티 패키지의 일부로 추가 명령을 지원하는 도구를 설치 할 수 있다. 설치가 완료되면 aa-enforce 명령을 사용하여 `Enforce` 모드에서 프로필을 로드하고 aa-complain 명령을 사용하여 `Complain` 모드에서 프로필을 로드할 수 있다. \n\n```    \nsudo apt-get update\nsudo apt-get install apparmor-utils\n```\n\n### 컨테이너에 프로파일 적용하기\n파드에 AppArmor 규칙을 적용하기 전에 다음과 같은 전제조건을 확인해야 한다. \n* 컨테이너 런타임 지원: AppArmor 규칙을 적용하려면 사용 중인 컨테이너 런타임이 AppArmor를 지원해야 한다. Docker와 같은 대부분의 현대적인 컨테이너 런타임은 AppArmor를 지원한다.\n* 워커 노드에 AppArmor 설치: AppArmor가 제대로 작동하려면 파드를 실행하는 모든 워커 노드에 AppArmor가 설치되어 있어야 한다.\n* 프로파일 로드 확인: AppArmor 프로파일이 시스템에 로드되어 있는지 확인해야 한다. \n\n컨테이너에 프로파일을 적용하려면 아래 예시 yaml 파일과 같이 특정 어노테이션을 설정해야 한다. 어노테이션 키는 `container.apparmor.security.beta. kubernetes.io/<컨테이너 이름>` 형식의 키를 사용해야 한다. 이 경우 컨테이너 이름은 `test`이다. 전체 키는 `container.apparmor.security.beta.kubernetes.io/test`이다. 어노테이션의 값은 localhost/<프로필 이름> 패턴을 따른다. 여기서 사용하려는 사용자 정의 프로파일은 k8s-deny-write이다. 구성 옵션에 대한 자세한 내용은 [쿠버네티스 문서](https://kubernetes.io/docs/tutorials/security/apparmor/)를 참조한다.\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-apparmor\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/test: localhost/k8s-deny-write \nspec:\n  containers:\n  - name: test\n    image: busybox:1.28\n    command: [\"sh\", \"-c\", \"echo 'Test AppArmor!' && sleep 100h\"]\n```\n\n`kubectl apply` 명령을 실행하고 파드가 “`Running`” 상태로 전환될 때까지 기다린다:\n```\n$ kubectl apply -f apparmor.yaml\npod/test-apparmor created\n\n$ kubectl get pod test-apparmor\nNAME             READY   STATUS    RESTARTS   AGE\ntest-apparmor   1/1     Running   0          4s\n```\n\n이제 컨테이너에 셸을 넣고 파일 쓰기 작업을 수행할 수 있다.\n```\n$ kubectl exec -it test-apparmor -- /bin/sh\n/ # touch test.txt\ntouch: test.txt: Permission denied\n```\nAppArmor는 컨테이너의 파일시스템에 파일을 쓰지 못하게 한다. 작업을 수행하려고 하면 “Permission denied”라는 메시지가 표시된다.\n\n## Seccomp 사용\nSeccomp는 Linux 커널의 보안 기능으로, \"Secure Computing Mode\"의 줄임말이다. seccomp는 user space에서 커널로의 호출을 제한할 수 있는 또 다른 Linux 커널 기능이다. seccomp 프로파일에 시스템 호출과 그 인수를 제한하는 규칙을 정의하게 된다. seccomp를 사용하면 Linux 커널 취약점을 악용할 위험을 줄일 수 있다. Kubernetes에서는 seccomp 프로필을 Pod의 securityContext를 통해 적용할 수 있고, 기본적으로 RuntimeDefault 프로필을 사용하여 보안 민감도가 높은 시스템 호출을 차단할 수 있다. 쿠버네티스의 seccomp에 대한 자세한 내용은 [쿠버네티스 문서](https://kubernetes.io/docs/tutorials/security/seccomp/)를 참조한다.\n\n### 컨테이너에 기본 컨테이너 런타임 프로파일 적용하기\n도커 엔진이나 컨테이너와 같은 컨테이너 런타임은 기본 seccomp 프로파일과 함께 제공된다. 기본 seccomp 프로파일은 애플리케이션에서 가장 일반적으로 사용되는 `syscalls`을 허용하는 동시에 위험하다고 간주되는 `syscalls`의 사용을 금지한다. 쿠버네티스는 파드를 생성할 때 기본 컨테이너 런타임 프로파일을 컨테이너에 적용하지 않지만, SeccompDefault [feature gate](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)를 사용하여 이를 활성화할 수 있다. 또는 `spec.securityContext.seccompProfile`을 사용하여 seccomp 프로파일 유형을 `RuntimeDefault`로 설정하여 파드 단위로 이 기능을 선택할 수 있다. \n\n```\napiVersion: v1 \nkind: Pod \nmetadata:\n  name: test-seccomp\nspec:\n  securityContext: \n    seccompProfile:\n      type: RuntimeDefault \n  containers:\n  - name: test\n    image: busybox:1.28\n    command: [\"sh\", \"-c\", \"echo 'Test seccomp!' && sleep 100h\"]\n```\n\n기본 컨테이너 런타임 프로파일을 적용한다. `kubectl apply` 명령을 사용하여 파드를 시작한다.\n\n```\n$ kubectl apply -f seccomp.yaml\npod/test-seccomp created\n\n$ kubectl get pod test-seccomp\nNAME            READY   STATUS      RESTARTS   AGE\nhello-seccomp   1/1     Running     0          4s\n```\n\n컨테이너에서 실행되는 `echo` 명령은 기본 seccomp 프로파일에 의해 보안 관점에서 문제가 없는 것으로 본다.\n\n```\n$ kubectl logs hello-seccomp\nTest seccomp!\n```\n\n`RuntimeDefault`로 `syscalls`이 허용되었고 그 결과 표준 출력에 \"Test seccomp!”라는 메시지를 볼 수 있다.\n\n### 사용자 지정 프로필 설정\n기본 컨테이너 런타임 프로파일 외에 사용자 정의 프로파일을 생성하고 설정할 수 있다. 이러한 파일의 표준 디렉터리는 `/var/lib/kubelet/seccomp`이다. 사용자 정의 프로파일은 하위 디렉터리 프로파일에 구성한다. 디렉터리가 없는 경우 디렉터리를 생성한다.\n\n```\n    sudo mkdir -p /var/lib/kubelet/seccomp/profiles\n```\n\n프로필 디렉터리에 있는 violation.json 파일에 사용자 정의 프로필을 생성한다. \n\n```\n{\n    \"defaultAction\": \"SCMP_ACT_ALLOW\",\n    \"architectures\": [\n        \"SCMP_ARCH_X86_64\",\n        \"SCMP_ARCH_X86\",\n        \"SCMP_ARCH_X32\"\n    ],\n    \"syscalls\": [\n        {\n            \"names\": [\n                \"mkdir\"\n            ],\n            \"action\": \"SCMP_ACT_ERRNO\"\n        }\n    ]\n}\n```\n\n간단히 말해서, 이 규칙 세트는 mkdir의 사용을 허용하지 않는다. 아래 규칙은 모든 `syscalls`을 허용하고, `syscalls` 리스트에 특별히 정의된 것만 거부하는 블랙리스트 방식이다. 반대로 화이트리스트 방식은 `syscalls`에 `SCMP_ACT_ALLOW`, `SCMP_ACT_ERRNO` 위치를 바꿔서 사용하면 된다. \n\n기본 조치는 모든 시스템 호출에 적용된다. 여기서는 블랙리스트 방식으로 `SCMP_ACT_ALLOW`를 사용하는 모든 시스템 호출을 허용하고, `SCMP_ACT_ERRNO` 액션은 mkdir syscall의 실행을 방지하도록 규칙을 작성했다. \n\n사용자 정의 프로파일을 /var/lib/kubelet/seccomp 디렉터리에 배치해도 자동으로 규칙이 파드에 적용되지는 않기 때문에 파드에 프로파일을 적용해야 한다.\n\n### 컨테이너에 사용자 정의 프로파일 적용하기\n사용자 정의 프로파일을 적용하는 것은 기본 컨테이너 런타임 프로파일을 적용하는 것과 비슷한 패턴을 따르지만 약간의 차이가 있다. 보안 프로파일의 seccompProfile 속성을 violation.json 파일을 가리키고 유형을 Localhost로 설정한다.\n\n```\napiVersion: v1 \nkind: Pod \nmetadata:\n  name: test-seccomp2 \nspec:\n  securityContext: \n    seccompProfile:\n      type: Localhost // 노드의 프로필을 참조\n      localhostProfile: profiles/violation.json  //하위 디렉터리 프로필에 violation.json이라는 이름의 프로필을 적용\n  containers:\n  - name: test2\n    image: busybox:1.28\n    command: [\"sh\", \"-c\", \"echo 'Test seccomp!' && sleep 100h\"] \n    securityContext:\n      allowPrivilegeEscalation: false\n```\n\n커스텀 프로파일을 적용한다. `kubectl apply` 명령을 사용하여 파드를 시작한다. 파드가 \"Running\" 상태로 전환될 때까지 기다린다:\n\n```\n$ kubectl apply -f seccomp.yaml \npod/test-seccomp2 created\n\n$ kubectl get pod test-seccomp2\nNAME            READY   STATUS    RESTARTS   AGE\ntest-seccomp2   1/1     Running   0          40s\n```\n\n컨테이너에 셸을 실행하여 seccomp가 적용된 규칙을 제대로 적용했는지 확인한다.\n```\n$ kubectl exec -it test-seccomp2 -- /bin/sh\n/ # mkdir test\nmkdir: can't create directory 'test': Operation not permitted\n```\n출력에서 볼 수 있듯이, 이 작업은 mkdir 명령을 실행하려고 할 때 오류 메시지를 렌더링한다. 사용자 지정 프로필의 규칙을 위반했다고 볼 수 있다.\n\n## 요약\n컨테이너에서 실행되는 애플리케이션과 프로세스가 시스템 호출을 하는 것은 매우 일반적이다. 보안 측면을 다루는 것은 호스트 시스템 수준에서 확인해야 할 게 많기 때문에 AppArmor 및 seccomp와 같은 Linux 커널 강화 도구를 사용하여 이러한 시스템 호출을 제한할 수 있다. 이런 도구들을 잘 활용하면 Kubernetes와 통합하여 보다 안전한 클러스터 운영을 할 수 있다."
    },
    {
      "id": "kubernetes/k6-prometheus/",
      "metadata": {
        "permalink": "/kubernetes/k6-prometheus/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2023-04-01-k6-prometheus copy.md",
        "source": "@site/blog/2023-04-01-k6-prometheus copy.md",
        "title": "prometheus grafana 스택으로 k6 테스트 결과 확인하기",
        "description": "k6으로 진행한 부하 테스트 결과를 remote write 형태로 prometheus에 전달하고 grafana 대시보드로 확인하는 방법을 알아본다.",
        "date": "2023-04-08T00:00:00.000Z",
        "formattedDate": "April 8, 2023",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "grafana",
            "permalink": "/tags/grafana"
          },
          {
            "label": "loadtest",
            "permalink": "/tags/loadtest"
          },
          {
            "label": "prometheus",
            "permalink": "/tags/prometheus"
          },
          {
            "label": "performance test",
            "permalink": "/tags/performance-test"
          },
          {
            "label": "k6",
            "permalink": "/tags/k-6"
          }
        ],
        "readingTime": 10.12,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "prometheus grafana 스택으로 k6 테스트 결과 확인하기",
          "comments": true,
          "classes": "wide",
          "description": "k6으로 진행한 부하 테스트 결과를 remote write 형태로 prometheus에 전달하고 grafana 대시보드로 확인하는 방법을 알아본다.",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/k6-prometheus/",
          "date": "2023-04-08T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "grafana",
            "loadtest",
            "prometheus",
            "performance test",
            "k6"
          ]
        },
        "prevItem": {
          "title": "커널 강화 도구 사용하기",
          "permalink": "/kubernetes/kernelhardening/"
        },
        "nextItem": {
          "title": "kyverno를 활용한 Kubernetes 정책 엔진 적용",
          "permalink": "/kubernetes/kyverno/"
        }
      },
      "content": "애플리케이션의 부하 테스트를 k6를 이용하여 진행하고 결과를 prometheus 및 grafana로 전달하여 확인하는 방법을 작성한다.\n\n## k6\n\nhttps://github.com/grafana/k6\n\nk6는 오픈소스 부하테스트 도구로, Go와 자바스크립트를 이용해 성능 테스트를 할 수 있는 도구이다. k6은 다양한 프로토콜(HTTP, WebSocket 등)을 지원하며, 설치와 사용이 매우 간편하다. 이전에 k6 테스트를 작성해 본 적이 없다면 [Running k6](https://k6.io/docs/getting-started/running-k6)를 확인하고 작동 방식을 파악하는 것부터 시작하는 것이 좋다. 또한, 친철하게 한글로 설명되어 있는 tutorial 링크도 참조한다.\n\n[https://github.com/schooldevops/k6-tutorials/blob/main/GettingStarts/01_intro_install.md](https://github.com/schooldevops/k6-tutorials/blob/main/GettingStarts/01_intro_install.md)\n\n<!--truncate-->\n\n## Prometheus Stack\n\n지난번 구성했던 것도 동일하게 kube-prometheus-stack을 설치한다. \n\n[kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)은 Kubernetes 클러스터 내에서 Prometheus와 Grafana를 사용하여 모니터링을 구성하는 데 사용되는 편리한 차트이다. \n\n```sh\n$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\n$ helm install -n monitoring kube-prometheus-stack prometheus-community/kube-prometheus-stack\n```\n\nPrometheus remote write 출력을 통해 k6는 테스트 결과 메트릭을 Prometheus remote write 엔드포인트로 전송할 수 있다. 뒤에 진행할 k6에서 Prometheus 쪽으로 메트릭을 전달하기 위해서는 두가지 기능(flag)인 [Remote Write Receiver](https://prometheus.io/docs/prometheus/latest/feature_flags/#remote-write-receiver)와 [Native Histograms](https://prometheus.io/docs/prometheus/latest/feature_flags/#native-histograms)을 활성화 해야한다.\n\n- [Remote Write Receiver](https://prometheus.io/docs/prometheus/latest/feature_flags/#remote-write-receiver): remote write 활성화\n- [Native Histograms](https://prometheus.io/docs/prometheus/latest/feature_flags/#native-histograms): 고해상도 히스토그램을 위한 기능 활성화 (experimental)\n\n두가지 flag를 활성화 하기 위해 위에 설치한 `kube-prometheus-stack` 차트에 위 두가지 기능을 추가하여 다시 배포한다.\n\n```yaml\nprometheus:\n  serviceMonitor:\n    enableFeatures: \n    - remote-write-receiver\n    - native-histograms\n```\n\n```\n$ helm upgrade -n monitoring kube-prometheus-stack prometheus-community/kube-prometheus-stack -f value.yaml\n```\n\n## k6-operator 설치 및 구성\n\n쿠버네티스에서 k6를 구성하는 방법은 직접 k6 바이너리를 컨테이너로 빌드해서 사용할 수도 있고 단일 인스턴스에서 실행하는 방법도 있지만 쿠버네티스 기반으로 동작하고 여러 인스턴스에서 동시에 부하 테스트를 진행하려는 요구사항이 있을 경우에는 k6-operator 로 구성하는 것을 추천한다.\n\nhttps://github.com/grafana/k6-operator\n\n기본적으로 바이너리 및 플러그인 빌드를 하고 배포하는 형태를 취하다 보니 [`Go`](https://golang.org/doc/install), [`Kustomize`](https://github.com/kubernetes-sigs/kustomize/), [`Kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), `Make` 등이 설치되어 있어야 한다. \n\n설치 명령은 `MacOs` 기준으로 설명한다. 일단 레포지토리를 로컬로 가져오고 나서 `make deploy` 명령을 수행하면 operator 구성이 완료된다. \n\n```\n$ git clone https://github.com/grafana/k6-operator && cd k6-operator\n$ make deploy\n```\n## k6 스크립트 작성\n\n아직 테스트 인스턴스 구성이 되어 있지 않기 때문에 테스트를 첫 단계 진행할 스크립트를 작성한다.  \n\n```js\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport const options = {\n  stages: [\n    { target: 200, duration: '30s' },\n    { target: 0, duration: '30s' },\n  ],\n};\n\nexport default function () {\n  const result = http.get('https://test-api.k6.io/public/crocodiles/');\n  check(result, {\n    'http response status code is 200': result.status === 200,\n  });\n}\n```\n\n스크립트를 살펴보면서 각각 30초 동안 실행되는 두 단계를 설정했다. 첫 번째 단계는 30초 동안 200 virtual users(이하 VUs)까지 선형적으로 증가시킨다. 두 번째 단계에서는 30초에 걸쳐 다시 VUs를 0으로 감소시킨다. function() 에서는 테스트를 진행할 URL에 대해 HTTP GET 요청을 실행하고 HTTP 상태 200으로 응답하는지 확인한다.\n\n```sh\nk6 run scripts.js\n\n          /\\      |‾‾| /‾‾/   /‾‾/\n     /\\  /  \\     |  |/  /   /  /\n    /  \\/    \\    |     (   /   ‾‾\\\n   /          \\   |  |\\  \\ |  (‾)  |\n  / __________ \\  |__| \\__\\ \\_____/ .io\n\n  execution: local\n     script: scripts.js\n     output: -\n\n  scenarios: (100.00%) 1 scenario, 200 max VUs, 1m30s max duration (incl. graceful stop):\n           * default: Up to 200 looping VUs for 1m0s over 2 stages (gracefulRampDown: 30s, gracefulStop: 30s)\n\n\n     ✓ http response status code is 200\n\n     checks.........................: 100.00% ✓ 2893      ✗ 0\n     data_received..................: 3.5 MB  58 kB/s\n     data_sent......................: 416 kB  6.9 kB/s\n     http_req_blocked...............: avg=49.33ms min=1µs      med=3µs   max=3.45s    p(90)=16µs    p(95)=377.18ms\n     http_req_connecting............: avg=13.09ms min=0s       med=0s    max=199.77ms p(90)=0s      p(95)=186.36ms\n     http_req_duration..............: avg=2.14s   min=190.5ms  med=1.32s max=5.79s    p(90)=5.39s   p(95)=5.57s\n       { expected_response:true }...: avg=2.14s   min=190.5ms  med=1.32s max=5.79s    p(90)=5.39s   p(95)=5.57s\n     http_req_failed................: 0.00%   ✓ 0         ✗ 2893\n     http_req_receiving.............: avg=81.97µs min=19µs     med=61µs  max=2.43ms   p(90)=141.8µs p(95)=205.4µs\n     http_req_sending...............: avg=22.37µs min=5µs      med=16µs  max=687µs    p(90)=38µs    p(95)=57µs\n     http_req_tls_handshaking.......: avg=36.2ms  min=0s       med=0s    max=3.2s     p(90)=0s      p(95)=190.18ms\n     http_req_waiting...............: avg=2.14s   min=190.38ms med=1.32s max=5.79s    p(90)=5.39s   p(95)=5.57s\n     http_reqs......................: 2893    48.176768/s\n     iteration_duration.............: avg=2.19s   min=190.7ms  med=1.39s max=6.05s    p(90)=5.4s    p(95)=5.58s\n     iterations.....................: 2893    48.176768/s\n     vus............................: 1       min=1       max=200\n     vus_max........................: 200     min=200     max=200\n\n\nrunning (1m00.0s), 000/200 VUs, 2893 complete and 0 interrupted iterations\ndefault ✓ [======================================] 000/200 VUs  1m0s\n```\n\n## k6 스크립트 컨피그맵 생성\n\n테스트 스크립트가 정상적으로 완료되었는 확인한 뒤 쿠버네티스 클러스터에 먼저 컨피그맵을 생성한다. 이름은 이후 k6 runner 파드에서 사용하는 컨피그맵을 참조하도록 식별할 수 있는 값(예시에서는 `test-script`)으로 지정한다. \n\n```sh\n$ kubectl create configmap test-script --from-file /home/ec2-user/environment/k6/scritps.js \nconfigmap/test-script created\n```\n\n### k6 prometheus 이미지 빌드\n\n기본적으로 k6-operator는 테스트 작업의 컨테이너 이미지로 `grafana/k6:latest`를 사용한다. xk6로 빌드된 확장을 사용하려면 자체 이미지를 생성하고 k6 쿠버네티스 배포 리소스에서 이미지 속성을 바꿔서 배포해야 한다. 다음 Dockerfile을 사용하여 https://github.com/grafana/xk6-output-prometheus-remote 로 빌드한 컨테이너 이미지를 생성할 수 있다.\n\n\n```dockerfile\n# Build the k6 binary with the extension\nFROM golang:1.18.1 as builder\n\nRUN go install go.k6.io/xk6/cmd/xk6@latest\nRUN xk6 build --output /k6 --with github.com/grafana/xk6-output-prometheus-remote@latest\n\n# Use the operator's base image and override the k6 binary\nFROM grafana/k6:latest\nCOPY --from=builder /k6 /usr/bin/k6\n```\n\n```sh\n$ docker build -t k6-prometheus:v1 .\n```\n\n빌드한 이미지를 원하는 레지스트리에 push하고 image 경로를 기록해 놓는다.\n\n## k6 리소스 생성\n\n생성이 되고 나면 `k6` 커스텀 리소스(CRD)를 동일한 네임스페이스에 만든다. 부하 테스트에 필요한 내용들을 포함하고 있다.\n\n- `arguments: -o xk6-prometheus-rw --tag testid=alb`\n  - Prometheus의 실행인자로 위에서 빌드한 이미지의 옵션 값, `tag`의 경우 그라파나 대시보드에 구분자로 사용할 값\n- `K6_PROMETHEUS_RW_SERVER_URL`\n  - remote write endpoint로 위에 구성한 prometheus endpoint를 기재\n- `K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM`\n  - 고해상도 히스토그램 형태의 데이터를 보내기 위한 환경 변수 값 (기본값: false)\n- `image: k6-prometheus:v1`\n  - 위에서 빌드한 Prometheus remote write 기능이 포함된 이미지 레지스트리 경로를 입력\n- `configMap`\n  - 컨피그맵으로 등록한 부하 스크립트\n\n```yaml\napiVersion: k6.io/v1alpha1\nkind: K6\nmetadata:\n  name: k6-sample\nspec:\n  arguments: -o xk6-prometheus-rw --tag testid=test\n  parallelism: 1\n  runner:\n    env:\n    - name: K6_PROMETHEUS_RW_SERVER_URL\n      value: http://kube-prometheus-stack-prometheus.monitoring:9090/api/v1/write\n    - name: K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM\n      value: \"true\"\n    image: k6-prometheus:v1\n  script:\n    configMap:\n      file: scritps.js\n      name: test-script\n```\n\n클러스터에서 두 개 이상의 테스트 스크립트를 사용하려면 각 스크립트에 대해 이 프로세스를 반복하여 맵에 다른 이름을 지정하면 된다. k6 리소스를 배포하게 되면 이제부터 배포 테스트 결과 메트릭 값이 프로메테우스로 바로 전달이 된다.  \n\n## Grafana 대시보드 등록\n\n[`18030` grafana 대시보드](https://grafana.com/grafana/dashboards/18030-test-result/)를 import 하면 다음 그림과 같은 대시보드를 통해 현재 리퀘스트 현황(성공, 실패), 지연시간, 데이터 전송량, VUs 등을 최대 프로메테우스가 수집 최대 주기 기준의 준 실시간 수준으로 확인이 가능하다.\n\n![dashboard](/img/k6-dashboard.png)\n\n## 정리\n\n이번 4주차 스터디에는 Grafana가 포함된 Prometheus Stack에 k6 테스트 결과 메트릭을 전달하고, 대시보드로 확인하는 방법을 정리했다. 이번 글에서는 다루지 않았지만 k6는 브라우저에 녹화된 스크립트 기반으로 테스트가 가능하기 때문에 다양한 시나리오로 테스트를 만족스럽게 진행할 수 있었다. locust라는 강력한 오픈소스 기반 부하 테스트 도구도 있지만 이번 글과 같이 다양한 플러그인을 직접 빌드하고 연동하여 사용할 수 있다는 점이 조금 더 매력적으로 다가왔다. \n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/kyverno/",
      "metadata": {
        "permalink": "/kubernetes/kyverno/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2023-04-08-kyverno.md",
        "source": "@site/blog/2023-04-08-kyverno.md",
        "title": "kyverno를 활용한 Kubernetes 정책 엔진 적용",
        "description": "kyverno를 활용한 Kubernetes 정책 엔진 적용",
        "date": "2023-04-08T00:00:00.000Z",
        "formattedDate": "April 8, 2023",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "kyverno",
            "permalink": "/tags/kyverno"
          },
          {
            "label": "policy",
            "permalink": "/tags/policy"
          },
          {
            "label": "security",
            "permalink": "/tags/security"
          }
        ],
        "readingTime": 10.125,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "kyverno를 활용한 Kubernetes 정책 엔진 적용",
          "comments": true,
          "classes": "wide",
          "description": "kyverno를 활용한 Kubernetes 정책 엔진 적용",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/kyverno/",
          "date": "2023-04-08T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "kyverno",
            "policy",
            "security"
          ]
        },
        "prevItem": {
          "title": "prometheus grafana 스택으로 k6 테스트 결과 확인하기",
          "permalink": "/kubernetes/k6-prometheus/"
        },
        "nextItem": {
          "title": "Advanced Argo Rollout",
          "permalink": "/kubernetes/argo-rollout-advanced/"
        }
      },
      "content": "kyverno 도구를 이용해서 쿠버네티스의 정책 엔진으로 활용하여 보안을 포함한 다양한 정책을 적용하는 방법을 알아본다.  \n\n## kyverno\n\nKyverno는 그리스어로 \"지배하다, 다스리다\"의 뜻으로 쿠버네티스 네이티브로 설계된 정책 엔진(policy engine)이다. Kyverno 정책을 활용하여 쿠버네티스 리소스가 정책에 부합하는지 검사하고, 필요하면 리소스를 변경하거나 생성할 수 있다. Kyverno 구조 및 동작 방식을 간단히 정리한다.\n\n[OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper)는 독자적인 policy 언어인 Rego를 사용하여 policy를 정의하만 \nkyverno는 쿠버네티스 리소스로 관리되며 정책을 작성하는 데 새로운 언어가 필요하지 않다.  OPA Gatekeeper와 유사하게 [pre-defined policy library](https://kyverno.io/policies) 를 제공한다.\n\n\n<!--truncate-->\n\n## kyverno 구조 및 동작 방식\n\nKyverno는 클러스터에서 [dynamic admission controller](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/)로 동작한다. Kyverno는 kube-apiserver로부터 admission HTTP callback들의 확인 또는 변경을 수신하고, 정책 수용과 요청 거절을 반환하기 위한 정책을 적용한다. 따라서 kubectl, git, kustomize와 같은 익숙한 도구들을 사용하여 정책을 관리할 수 있다.  \n\n정책을 통해 쿠버네티스 리소스의 유효성 검사, 변경, 생성, 정리를 수행할 수 있을 뿐만 아니라 OCI 이미지 공급망(supply chain) 보안을 보장한다.\n\nKyverno CLI는 CI/CD 파이프라인의 일부로 정책을 테스트하고 리소스를 검증하는 데 사용할 수 있다.\n\n![Pasted image 20230408005656.png](https://kyverno.io/images/kyverno-architecture.png)\n\n고가용성 Kyverno 설치는 여러 개의 replica를 실행할 수 있으며, 각 복제본에는 서로 다른 기능을 수행하는 여러 컨트롤러가 있다.  \n\n- Webhook : kube-apiserver의 AdmissionReview 요청을 처리\n- Monitor : webhook에 필요한 설정을 생성하고 관리\n- PolicyController : policy 리소스를 감시하며, 설정된 스캔 간격에 따라서 백그라운드 스캔을 시작\n- GenerateController : 생성된 리소스들의 생명주기를 관리\n\nKyverno는 최신 버전의 manifest 또는 Helm을 사용하여 설치할 수 있다. Kyverno는 쿠버네티스 프로젝트와 동일한 지원 정책인 N-2 정책을 따르며, 세 가지 최신 마이너 릴리스가 유지된다.  \n\n### YAML을 사용하여 Kyverno 설치\n\n단일 설치 매니페스트를 사용하여 Kyverno를 설치할 수도 있지만 프로덕션 설치로 고가용성으로 구성할 경우 헬름 차트로 해야 한다.\n\n```sh\nkubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.9.0/install.yaml\n```\n\n### [Helm을 사용하여 Kyverno 설치](https://kyverno.io/docs/installation/#install-kyverno-using-helm)\n\n헬름을 사용하여 Kyverno를 설치하려면 먼저 Kyverno 헬름 리포지토리를 추가한다.\n\n```sh\nhelm repo add kyverno https://kyverno.github.io/kyverno/\n```\n\n리포지토리를 업데이트 하고 차트를 검색한다.\n\n```sh\nhelm repo update\nhelm search repo kyverno -l\n```\n\n고가용성을 위해 replica를 3으로 설치한다. \n\n```sh\nhelm install kyverno kyverno/kyverno -n kyverno --create-namespace --set replicaCount=3\n```\n\nKyverno 파드 보안 표준 정책을 설치하려면 Kyverno를 설치한 후 아래 헬름 명령어를 실행한다.\n\n```sh\nhelm install kyverno-policies kyverno/kyverno-policies -n kyverno\n```\n\n![Pasted image 20230408020119.png](https://kyverno.io/images/Kyverno-Policy-Structure.png)\n\n정책에 대해서 조금 자세히 살펴보면, Policy는 Rule들의 집합이고, 각 rule은 하나의 [match](https://kyverno.io/docs/writing-policies/match-exclude/)와 [mutate](https://kyverno.io/docs/writing-policies/mutate/), [validate](https://kyverno.io/docs/writing-policies/validate/), [generate](https://kyverno.io/docs/writing-policies/generate) resource 또는 [verifyImages](https://kyverno.io/docs/writing-policies/verify-images) 중 하나로 구성된다. Policy의 종류(kind)는 클러스터 전체에 적용되는 ClusterPolicy와 namespace에 적용되는 Policy가 있다.\n\n정책은 정의된 순서대로 적용된다. admission control에서 mutation 규칙은 validation 규칙 전에 적용되기 때문에 mutation에 의한 결과를 validation할 수 있다. 모든 mutation 규칙은 모든 정책에서 validation 규칙이 적용되기 전에 적용된다.\n\n여러가지 정책들이 있지만 간단하게 테스트를 진행하는 조건으로 `latest` 태그가 달린 이미지는 배포되지 않게 하는 것을 진행해 본다. \n\nhttps://kyverno.io/policies/best-practices/disallow_latest_tag/disallow_latest_tag/\n\n`latest` 태그는 변경 가능하며 이미지가 변경되면 예기치 않은 오류가 발생할 수 있다. 일반적으로 애플리케이션 파드의 특정 버전에 매핑되는 변경 불가능한 태그를 사용하는 것이 좋다. 이 정책은 이미지가 태그를 지정하고 해당 태그가 `latest`로 호출되지 않는지 확인한다. \n\n아래 ClusterPolicy 에서 `spec.validationFailureAction`는 유효성 검사 정책 규칙 실패 시 어떤 액션을 수행할지 정하는 부분이다. `enforce` 는 정책내의 규칙(latest 태그가 있는 파드가 배포될 경우)에 해당하는 이벤트가 발생하게 되면 강제적용되고, 기본값인 `audit`의 경우에는 감사로그만 생성이 된다. \n\n```yaml\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: disallow-latest-tag\n  annotations:\n    policies.kyverno.io/title: Disallow Latest Tag\n    policies.kyverno.io/category: Best Practices\n    policies.kyverno.io/minversion: 1.6.0\n    policies.kyverno.io/severity: medium\n    policies.kyverno.io/subject: Pod\n    policies.kyverno.io/description: >-\n      The ':latest' tag is mutable and can lead to unexpected errors if the\n      image changes. A best practice is to use an immutable tag that maps to\n      a specific version of an application Pod. This policy validates that the image\n      specifies a tag and that it is not called `latest`.      \nspec:\n  validationFailureAction: enforce\n  background: true\n  rules:\n  - name: require-image-tag\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"An image tag is required.\"\n      pattern:\n        spec:\n          containers:\n          - image: \"*:*\"\n  - name: validate-image-tag\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"Using a mutable image tag e.g. 'latest' is not allowed.\"\n      pattern:\n        spec:\n          containers:\n          - image: \"!*:latest\"\n\n```\n\n테스트로 nginx:latest 이미지를 생성하면 admission webhook 단계에서 요청을 거절하여 deployment 배포가 실패한 것을 확인할 수 있다. \n\n```sh\n$ kubectl create deployment nginx --image=nginx:latest\n\nerror: failed to create deployment: admission webhook \"validate.kyverno.svc-fail\" denied the request: \n\npolicy Deployment/default/nginx for resource violation: \n\ndisallow-latest-tag:\n  autogen-validate-image-tag: 'validation error: Using a mutable image tag e.g. ''latest''\n    is not allowed. rule autogen-validate-image-tag failed at path /spec/template/spec/containers/0/image/'\n```\n\nevent를 확인해보면 다음과 같이 nginx 파드 오브젝트가 Killing 되는것을 확인할 수 있다.\n\n```sh\n$ kubectl get events -A --sort-by=.metadata.creationTimestamp --field-selector type!=Warning\nNAMESPACE   LAST SEEN   TYPE     REASON    OBJECT                       MESSAGE\ndefault     1m         Normal   Killing   pod/nginx-7597c656c9-rlvnr   Stopping container nginx\n```\n\n이번에는 정책 위반 시 `audit` 옵션으로 바꾸고 똑같이 배포를 진행해보면 정상적으로 배포는 진행되지만, event나 describe로 확인해보면 다음과 같이 정책 위반 Warning 메세지를 확인할 수 있다.\n\n```sh\nEvents:\n  Type     Reason           Age    From               Message\n  ----     ------           ----   ----               -------\n  Normal   Scheduled        2m29s  default-scheduler  Successfully assigned default/nginx-7597c656c9-w897t to i-00b3ebc1b78f19739\n  Normal   Pulling          2m28s  kubelet            Pulling image \"nginx:latest\"\n  Normal   Pulled           2m21s  kubelet            Successfully pulled image \"nginx:latest\" in 6.218701714s\n  Normal   Created          2m21s  kubelet            Created container nginx\n  Normal   Started          2m21s  kubelet            Started container nginx\n  Warning  PolicyViolation  118s   kyverno-scan       policy disallow-latest-tag/validate-image-tag fail: validation error: Using a mutable image tag e.g. 'latest' is not allowed. rule validate-image-tag failed at path /spec/containers/0/image/\n```\n\n예시로 이미지 태그를 제어하는 정책을 가지고 테스트를 진행하였지만, 이외에도 많은 정책들을 적용해볼 수 있다.\n\n- [Validate](https://kyverno.io/docs/writing-policies/validate/) : Kubernetes API Server로 들어오는 요청 중 필수 설정(예, 특정 라벨이 누락되거나, 인그레스 생성시 인증서가 누락된 경우)이 누락된 경우 차단\n- [Mutate](https://kyverno.io/docs/writing-policies/mutate/) : 운영환경에서 일부 설정 오류로 인한 장애가 발생하지 않도록 예방\n- [Disallow Capabilities](https://kyverno.io/policies/pod-security/baseline/disallow-capabilities/disallow-capabilities/):  정책에 나열된 Linux Capability 이외의 기능을 추가하는 것은 허용하지 않음\n\n### 정리\n\n1.25버전 부터 Pod Security Policies(PSPs)가 deprecated 되면서 admission controller와 정책 엔진 영역에서 활용이 가능한 솔루션이 필요하게 되었는데, 스터디에서 소개해준 polaris는 깔끔한 대시보드와 정책 엔진으로서 다양한 기능들을 활용할 수 있다. polaris는 오픈소스 기준에서도 보안에 대한 취약점과 설정을 쉽게 알아내고 리포트를 뽑아내는 측면이 강하다면, kyverno는 정책을 Kubernetes 리소스로 native하게 관리할 수 있기 때문에 kubectl, git, kustomize와 같은 도구를 사용하여 정책을 조금더 적극적으로 감사하거나 차단하는 형태의 관리가 가능하다.\n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/argo-rollout-advanced/",
      "metadata": {
        "permalink": "/kubernetes/argo-rollout-advanced/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2023-03-25-argo-rollout-advanced copy.md",
        "source": "@site/blog/2023-03-25-argo-rollout-advanced copy.md",
        "title": "Advanced Argo Rollout",
        "description": "argo-rollout의 세부적인 전략을 알아보고 다양한 notification을 설정한다.",
        "date": "2023-03-25T00:00:00.000Z",
        "formattedDate": "March 25, 2023",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "kOps",
            "permalink": "/tags/k-ops"
          },
          {
            "label": "argo",
            "permalink": "/tags/argo"
          },
          {
            "label": "argo-rollout",
            "permalink": "/tags/argo-rollout"
          },
          {
            "label": "notification",
            "permalink": "/tags/notification"
          },
          {
            "label": "grafana",
            "permalink": "/tags/grafana"
          },
          {
            "label": "slack",
            "permalink": "/tags/slack"
          }
        ],
        "readingTime": 14.985,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "Advanced Argo Rollout",
          "comments": true,
          "classes": "wide",
          "description": "argo-rollout의 세부적인 전략을 알아보고 다양한 notification을 설정한다.",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/argo-rollout-advanced/",
          "date": "2023-03-25T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "kOps",
            "argo",
            "argo-rollout",
            "notification",
            "grafana",
            "slack"
          ]
        },
        "prevItem": {
          "title": "kyverno를 활용한 Kubernetes 정책 엔진 적용",
          "permalink": "/kubernetes/kyverno/"
        },
        "nextItem": {
          "title": "kOps with Cilium",
          "permalink": "/kubernetes/kops-cilium/"
        }
      },
      "content": "kOps 환경에 구축한 ArgoCD와 Argo Rollout를 좀더 잘 쓰기 위한 몇가지 내용들을 간단하게 기록용으로 작성하였다.\n\n## GitOps\n\n간단하게만 정리하면, GitOps는 Git 리포지토리를 단일 정보 소스(SSOT,Single Source Of Truth)로 사용하여 인프라를 코드로 제공하는 것이다. GitOps는 인프라 구성을 위해 Git을 버전 제어 시스템으로 사용하는 코드형 인프라 (IaC) 가 진화한 것이라고 생각하면 된다. IaC는 원하는 시스템 상태를 정의하고 시스템의 실제 상태를 추적하여 인프라 관리에 대한 선언적 접근 방식을 따르는 경우가 많다. 그래서 배포에 관련된 모든 것을 선언형 기술서로 작성한다음 Config 레포지토리에서 저장을 하고, 해당 정보와 실제 배포된 환경간의 상태 차이가 발생하지 않도록 유지하는 것을 GitOps 업무 패턴이라고 할 수 있다. \n\n또한 개발자에게 익숙한 Git 기반 워크플로우를 사용하여 애플리케이션 개발에서 배포, 애플리케이션 라이프사이클 관리, 인프라 구성에 이르는 기존 프로세스를 기반으로 확장을 할 수 있다. 애플리케이션 라이프사이클 전반에 걸쳐 모든 변경 사항이 Git 리포지토리에서 추적되고, GitOps는 이러한 변경 사항을 자동으로 인프라에 적용하는 것을 말한다. \n\n이처럼 GitOps의 장점중에 가장 중요한 부분은 신뢰할 수 있는 정보가 공유되고, 인프라 구성하는 모든 변경 사항에 대한 추적과 관리가 가능해진다는 점일 것이다.\n\n<!--truncate-->\n\n## ArgoCD, Argo Rollouts\n\nArgoCD는 위에서 이야기한 것처럼 쿠버네티스 리소스와 Git에 저장된 선언형 기술서(manifest)를 동일하게 유지시켜 주고 CD환경을 구성할 수 있는 GitOps 구현체 오픈소스이다. 즉, GitOps 패턴을 따라 Git 저장소를 사용하여 애플리케이션 배포를 관리할 수 있는 도구이다.\n\n롤아웃은 쿠버네티스 환경에서 새로운 버전의 애플리케이션을 배포할 때 이전 버전의 애플리케이션과 새로운 버전의 애플리케이션을 조절하고 사용자에게 무중단으로 서비스를 제공할 수 있도록 하는 것을 말한다.\n\nArgo Rollouts는 쿠버네티스에서 롤아웃 관리를 위한 오픈소스 도구 중 하나이다. 쿠버네티스의 서비스 메시 및 인그레스 컨트롤러와 통합하여 트래픽을 새 버전으로 점진적으로 이동시키는 기능을 제공한다. 쿠버네티스의 커스텀 리소스 정의(CRD)를 사용하여 블루-그린 배포, 카나리 배포, 카나리 분석 등의 형태로 정의하고, 거의(?) 무중단으로 새로운 버전의 애플리케이션을 배포할 수 있게 한다. 새로운 버전의 애플리케이션이 제대로 작동하지 않을 경우, 롤백을 수행하여 이전 버전의 애플리케이션으로 빠른 시간내에 되돌릴 수 있다.\n\n## Argo Rollout Specification\n\nArgo Rollout Specification은 Argo Rollouts 도구에서 사용하는 롤아웃 전략을 정의하는 YAML 파일을 말한다. 각각의 애플리케이션에 맞게 작성되어야 하고 Kubernetes API를 기반으로 구성되며, 롤아웃 전략의 모든 내용을 포함하여 작성하게 된다. \n\nArgo Rollout Specification 파일은 다음과 같은 내용을 포함한다.\n\n1. 롤아웃 전략: Canary, Blue-Green, AB Testing 등 다양한 롤아웃 전략\n2. 배포 전략: 롤아웃 속도, 배포 시간 등의 새로운 버전의 애플리케이션을 배포하는 방법을 정의\n3. 서비스 설정: 롤아웃 중에 사용할 서비스를 정의\n4. 롤아웃 이력: 이전 롤아웃 이력을 추적하고 검색\n\nBlue-Green 전략은 두 개의 버전의 애플리케이션을 동시에 유지하면서, 사용자 트래픽을 새로운 버전으로 전환하는 전략으로 이전 버전은 Blue 환경이라고 하고, 새로운 버전은 Green 환경이라고 말한다. Blue-Green 배포 전략은 롤백이 매우 쉬우며, 사용자 트래픽의 전환도 매우 빠르게 수행된다. 따라서, 많은 기업에서 Blue-Green 배포 전략을 사용하여 새로운 버전의 애플리케이션을 롤아웃하고 있고 일반적인 방식이라 보면 된다.\n\n다음은 Argo Rollout에서 Blue-Green 배포 전략을 정의하는 YAML 파일의 예시이다.  \n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\nspec:\n  strategy:\n    blueGreen:\n      activeService: myapp-green\n      previewService: myapp-blue\n      autoPromotionEnabled: true\n      # autoPromotionSeconds: *int32  \n  template:\n    spec:\n      containers:\n        - name: myapp\n          image: myapp:v2\n  selector:\n    matchLabels:\n      app: myapp\n```\n\n- `activeService`: Green 환경에 배포된 애플리케이션에 대한 서비스 이름으로 Blue-Green 전략에서는 필수 필드\n- `previewService`: Blue 환경에 배포된 애플리케이션에 대한 서비스 이름으로 Blue-Green 전략에서는 optional 필드\n- `autoPromotionEnabled`: 롤아웃이 완료된 후, 자동으로 Green 환경으로 전환할지 여부\n- `template`: 새로운 버전의 애플리케이션을 정의\n- `selector`: 롤아웃 중인 애플리케이션을 선택하기 위한 레이블 셀렉터\n- `autoPromotionSeconds`: Rollout 이 일시 정지 상태에 들어간 이후 해당 필드 설정 시간이 지난후 자동으로 새로운 ReplicaSet을 활성 서비스로 자동으로 promote 하는 옵션이다.AutoPromotionEnabled 필드가 True로 설정되면이 필드는 무시된다.\n\nanti-affinity는 다른 노드 간의 Pod 분산을 위해 사용되고, pod anti-affinity는 동일한 물리적 호스트에서 실행되는 여러 Pod 간의 분산을 위해 사용되는 방법이다. \n\nanti-affinity struct를 Blue-Green 또는 Canary 전략에 추가할 수 있다. 여러 노드에 애플리케이션을 배포할 때, 동일한 파드들이 하나의 노드에 모이지 않도록 하는 전략이다. 이를 통해 단일 노드 장애로부터 애플리케이션을 보호할 수 있다. 아래의 예시는 새 버전은 독립된 scaled 노드로 배치하는 하고 특정 zone에만 배치하는 전략이다.\n\n```yaml\nspec:\n  strategy:\n    blueGreen:\n      antiAffinity:  \n        requiredDuringSchedulingIgnoredDuringExecution:  \n          nodeSelectorTerms:  \n          - matchExpressions:  \n              - key: topology.kubernetes.io/zone  \n              operator: In  \n              values:  \n              - ap-northeast-2a\n```\n\n아래 예시는 podAntiAffinity 필드를 사용하여 anti-affinity 전략을 설정하는데 `requiredDuringSchedulingIgnoredDuringExecution`를 사용하여 노드 스케줄링 중에 이전 파드와 동일한 노드에 배치하지 않도록 필요한 설정을 지정하는 전략이다. \n\n```yaml\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - myapp\n            topologyKey: \"kubernetes.io/hostname\"\n```\n\n## Argo Rollouts Notification\n\nhttps://argoproj.github.io/argo-rollouts/features/notifications/\n\nArgo Rollouts Notification은 롤아웃 이벤트에 대한 알림 메시지를 Slack, Email, Webhook 등 다양한 방법으로 전송할 수 있다. 이를 통해 롤아웃 이벤트에 대한 모니터링 및 대응이 가능해진다.  \n\n해당 기능은 1.1.0 이상 버전의 Rollout을 구성해야만 Notification 기능을 사용할 수 있다.  \n\n구성된 환경에 추가적으로 Argo Rollouts Notification 리소스를 생성하고, 해당 리소스에서 각 통합 서비스에 대한 설정을 작성한다. 이렇게 작성된 설정을 통해 롤아웃 이벤트가 발생할 때 마다, 해당 설정에 따라 알림 메시지가 전송되는 구조이다.  \n\n알림 처리를 담당하는 Argo CD Notification 컨트롤러를 설치한다.  \n```sh\nkubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml\n```\n\n같은 네임스페이스, 기본 구성되는 argo-rollouts 내의 ConfigMap을 참조하게 된다. 다음 명령을 실행하면 `argo-rollouts` 네임스페이스에 `argo-rollouts-notification-configmap`라는 이름의 ConfigMap이 만들어지게 되고, 이 ConfigMap에는 다음과 같은 트리거와 템플릿을 담겨 있으며 필요시 Custom 트리거와 템플릿을 추가할 수 있다. argo-rollouts pod는 정의된 트리거를 감지하면 트리거와 링크되어 있는 템플릿으로 메시지를 구성하여 알림을 발송하게 된다.  \n```sh\nkubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/manifests/notifications-install.yaml\n```\n\n## Slack 연동\n\nSlack에서 다음과 같은 권한으로 Oauth 설정을 하고 생성된 access token을 Secret으로 저장한다.  \n\n![argo-secret](/img/argo-secret.png)\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: argo-rollouts-notification-secret\nstringData:\n  slack-token: <Oauth-access-token>\n```\n\n위에서 설정한 Oauth Secret을 위에서 구성한  ConfigMap에 추가한다. \n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argo-rollouts-notification-configmap\ndata:\n  service.slack: |\n    token: $slack-token\n...\n```\n\n### Templates\n\n알림 템플릿은 알림 콘텐츠를 생성하는 상태 비저장 함수이다. 이 템플릿은 html/template golang 패키지를 활용한다. 템플릿은 재사용할 수 있으며 여러 트리거에서 참조할 수 있다.\n\n```yaml\n...\ndata:\n  template.rollout-completed: |\n    message: Rollout {{.rollout.metadata.name}} has been completed.\n    email:\n      subject: Rollout {{.rollout.metadata.name}} has been completed.\n    slack:\n      attachments: |\n          [{\n            \"title\": \"{{ .rollout.metadata.name}}\",\n            \"color\": \"#18be52\",\n            \"fields\": [\n            {\n              \"title\": \"Strategy\",\n              \"value\": \"{{if .rollout.spec.strategy.blueGreen}}BlueGreen{{end}}{{if .rollout.spec.strategy.canary}}Canary{{end}}\",\n              \"short\": true\n            }\n            {{range $index, $c := .rollout.spec.template.spec.containers}}\n              {{if not $index}},{{end}}\n              {{if $index}},{{end}}\n              {\n                \"title\": \"{{$c.name}}\",\n                \"value\": \"{{$c.image}}\",\n                \"short\": true\n              }\n            {{end}}\n            ]\n          }]\n```\n\n이외의 다른 여러가지 템플릿 구성은 [https://github.com/argoproj/argo-rollouts/tree/master/manifests/notifications](https://github.com/argoproj/argo-rollouts/tree/master/manifests/notifications) 에서 확인할 수 있다.  \n\n### Triggers\n\n트리거 되는 경우에 어떤 template으로 보낼지 지정하는 구문이다. 아래 예시는 위에서 정한 template 이름인 `rollout-completed`를 보낸다는 설정이다.\n\n```yaml\n...\ndata:\n  trigger.on-rollout-completed: |\n    - send: [rollout-completed]\n\n```\n\n## Rollout template에 notification annotation 추가\n\n신규로 생성하거나 기존의 Rollout 리소스에 notification annotation를 추가해야 관련된 rollout 이벤트가 발생하면 위에 설정한 슬랙의 `my_channel`로 alert를 보내게 된다.\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\n  annotations:\n    notifications.argoproj.io/subscribe.on-scaling-replica-set.slack: my_channel\nspec:\n  strategy:\n    blueGreen:\n      activeService: myapp-green\n      previewService: myapp-blue\n      autoPromotionEnabled: true\n  template:\n    spec:\n      containers:\n        - name: myapp\n          image: myapp:v2\n  selector:\n    matchLabels:\n      app: myapp\n```\n\n## Grafana로 Rollout notification 받기\n\n추가적으로 Grafana annotation api([https://grafana.com/docs/grafana/latest/http_api/annotations/](https://grafana.com/docs/grafana/latest/http_api/annotations/))를 활용하여 rollout 관련 이벤트 발생시 해당 시점의 이벤트 정보를 Grafana 시계열 차트에 자동으로 등록을 하는 방식으로 메트릭 기반으로 배포 전후 상태를 관측할 수 있도록 구성하는것이 목적이다.\n\nSlack과 동일한 방식으로 grafana에서 발급한 API Key를 Secret에 등록한다.  \n\n```yaml\napiVersion: v1  \nkind: Secret  \nmetadata:  \n  name: argo-rollouts-notification-secret  \nstringData:  \n  grafana-api-key: {{grafana-api-key}}\ntype: Opaque\n```\n\n기존 ConfigMap에 grafana API 엔드포인트와 위에서 설정한 apiKey를 설정한다.\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argo-rollouts-notification-configmap\ndata:\n  service.grafana: |                                                \n    apiUrl: http://prometheus-grafana.prometheus.svc.cluster.local/api\n    apiKey: $grafana-api-key\n  service.slack: |\n    token: $slack-token\n...\n```\n\n기존 rollout 에 위에 선언한 grafana에 알림 전달을 위한 annotatinos을 추가한다.  \n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\n  annotations:\n    notifications.argoproj.io/subscribe.on-analysis-run-running.grafana: analysis-run  \n    notifications.argoproj.io/subscribe.on-app-sync-status.grafana: appsync  \n    notifications.argoproj.io/subscribe.on-rollout-completed.grafana: rollout  \n    notifications.argoproj.io/subscribe.on-scaling-replica-set.grafana: scalling\n...\n```\n\nGrafana 설정으로 이동해서 아래 그림과 같이 Settings - Annotation 메뉴로 이동하여 Query - Filter by 항목을 Tag로 변경하고 설정을 저장한다  \n\n![rollout](/img/grafana-annotation.png)\n\n이후 Rollout에 변경 이벤트가 발생하면 아래 그림과 같이 해당 시점에 annotation이 표기되는것을 확인할 수 있다. 이를 통해 특정 배포 버전에 대한 전,후 비교가 가능해진다.  \n\n![grafana](/img/rollout-annotation.png)\n\n\n## 정리\n\n이번 3주차 스터디에는 구성된 환경에 Argo CD, Argo Rollout을 구성하는 테스트를 진행했고, 이번 포스팅에서는 Argo Rollout Notification을 통해 운영자가 Rollout 이벤트가 다양한 이벤트가 발생했을때 Slack으로 알림을 받아보거나 자동으로 Grafana 대시보드 상에 annotation을 표기하는 실습을 진행했다.\n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/kops-cilium/",
      "metadata": {
        "permalink": "/kubernetes/kops-cilium/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2023-03-18-kops-cillium.md",
        "source": "@site/blog/2023-03-18-kops-cillium.md",
        "title": "kOps with Cilium",
        "description": "Cilium CNI 기반 kOps 구성",
        "date": "2023-03-18T00:00:00.000Z",
        "formattedDate": "March 18, 2023",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "kOps",
            "permalink": "/tags/k-ops"
          },
          {
            "label": "Cloud9",
            "permalink": "/tags/cloud-9"
          },
          {
            "label": "CloudFormation",
            "permalink": "/tags/cloud-formation"
          },
          {
            "label": "CNI",
            "permalink": "/tags/cni"
          },
          {
            "label": "Networking",
            "permalink": "/tags/networking"
          },
          {
            "label": "Cilium",
            "permalink": "/tags/cilium"
          }
        ],
        "readingTime": 11.42,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "kOps with Cilium",
          "comments": true,
          "classes": "wide",
          "description": "Cilium CNI 기반 kOps 구성",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/kops-cilium/",
          "date": "2023-03-18T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "kOps",
            "Cloud9",
            "CloudFormation",
            "CNI",
            "Networking",
            "Cilium"
          ]
        },
        "prevItem": {
          "title": "Advanced Argo Rollout",
          "permalink": "/kubernetes/argo-rollout-advanced/"
        },
        "nextItem": {
          "title": "kOps with Cloud9",
          "permalink": "/kubernetes/kops-cloud9/"
        }
      },
      "content": "Cilium을 기반으로 하는 kOps 클러스터를 생성하고 네트워크 구성이 어떻게 되는지 확인해보고자 한다. Cilium는 Linux 커널 내에 강력한 보안 가시성 및 제어 로직을 동적으로 삽입할 수 있는 BPF라는 Linux 커널 기술을 사용하는 CNI이다. \n\nhttps://cilium.io/\nhttps://kops.sigs.k8s.io/networking/cilium/\n\n## kOps with Cloud9\n\n글을 작성하는 날짜 기준(23년 3월 18일), 버전은 1.25.11 으로 진행을 한다. 지난번 구성과 동일하게 Bastion은 Cloud9에서 구성을 진행하였다. 지난번 작성한 Cloud9 기반 인스턴스는 아래 링크에서 확인할 수 있다.\n\n[https://github.com/jinwoongk/aws-cloud9-bootstrapping-example/blob/main/example_instancestack.yaml](https://github.com/jinwoongk/aws-cloud9-bootstrapping-example/blob/main/example_instancestack.yaml)\n\n<!--truncate-->\n\n## Cilium CNI 기반 kOps 배포\n\nCloud9이 구성되고 나면 해당 환경에서 기본 구성할 Cluster yaml을 작성한다. yaml의 크기 때문에 부분을 나눠서 설명한다.\n\n```yaml\napiVersion: kops.k8s.io/v1alpha2\nkind: Cluster\nmetadata:\n  creationTimestamp: null\n  name: prosv.kr\nspec:\n  kubeProxy:\n    enabled: false\n    metricsBindAddress: 0.0.0.0\n  networking:\n    cilium:\n      enableNodePort: true\n      ipam: eni\n      hubble:\n        enabled: true\n...\n```\n\nkOps를 구성할 때는 Cluster CRD를 작성하는것이 기본인데 클러스터 이름은 현재 사용중인 테스트 도메인을 입력한다.   \n그리고 Cilium 을 기본 CNI으로 구성할 예정이기 때문에 Kube-Proxy는 기본적으로 비활성화 한다. Cilium이 AWS에서 할당된 managed IP 주소를 프로비저닝하고 AWS VPC와 마찬가지로 각 파드에 직접 연결하도록 `networking.cilium.ipam=eni` 설정을 추가하고, hubble을 활성화하기 위한 `networking.cilium.hubble.enabled: true` 설정도 추가한다.  \nhttps://docs.cilium.io/en/stable/network/concepts/ipam/eni/\n\n\ncilium 설정에서의 다양한 옵션은 다음을 참조할 수 있다.  \nhttps://kops.sigs.k8s.io/networking/cilium/\n\n\n```yaml\n...\n  api:\n    dns: {}\n  authorization:\n    rbac: {}\n  channel: stable\n  cloudProvider: aws\n  certManager:\n    enabled: true\n  awsLoadBalancerController:\n    enabled: true\n  externalDns:\n    provider: external-dns\n  metricsServer:\n    enabled: true\n  kubeDNS:\n    provider: CoreDNS\n    nodeLocalDNS:\n      enabled: true\n      memoryRequest: 5Mi\n      cpuRequest: 25m\n  configBase: s3://jinwoong-k8s-s3/prosv.kr\n...\n```\n\n위에서 추가한 설정은 다음과 같다.\n\n- [certManager 활성화](https://kops.sigs.k8s.io/addons/#cert-manager)\n- [awsLoadBalancerController 활성화](https://kops.sigs.k8s.io/addons/#aws-load-balancer-controller) 및 [externalDns 구성](https://kops.sigs.k8s.io/cluster_spec/#externaldns)\n- [metricsServer 활성화](https://kops.sigs.k8s.io/addons/#metrics-server)\n- [kubeDNS 구성](https://kops.sigs.k8s.io/cluster_spec/#kubedns)\n- [기본 config 저장소 s3 지정](https://kops.sigs.k8s.io/state/#s3-state-store)\n\n```yaml\n...\n  etcdClusters:\n  - cpuRequest: 200m\n    etcdMembers:\n    - encryptedVolume: true\n      instanceGroup: control-plane-ap-northeast-2a\n      name: a\n    memoryRequest: 100Mi\n    name: main\n  - cpuRequest: 100m\n    etcdMembers:\n    - encryptedVolume: true\n      instanceGroup: control-plane-ap-northeast-2a\n      name: a\n    memoryRequest: 100Mi\n    name: events\n  iam:\n    allowContainerRegistry: true\n    legacy: false\n  kubelet:\n    anonymousAuth: false\n    maxPods: 100\n...\n```\n위에서 추가한 설정은 다음과 같다.\n\n- [etcd 클러스터 구성](https://kops.sigs.k8s.io/cluster_spec/#etcdclusters)\n  - instancGroup CRD에서 원하는 머신타입, 기본 노드 이미지, 노드 개수 등을 구성할 수 있다.\n- [ECR 접근을 위한 IAM 구성](https://kops.sigs.k8s.io/iam_roles/#access-to-aws-ec2-container-registry-ecr)\n- [kubelet 구성](https://kops.sigs.k8s.io/cluster_spec/#kubelet)\n\n\n```yaml\n...\n  kubernetesApiAccess:\n  - 0.0.0.0/0\n  - ::/0\n  kubernetesVersion: 1.24.11\n  masterPublicName: api.prosv.kr\n  networkCIDR: 172.30.0.0/16\n  nonMasqueradeCIDR: 100.64.0.0/10\n  sshAccess:\n  - 0.0.0.0/0\n  - ::/0\n  subnets:\n  - cidr: 172.30.32.0/19\n    name: ap-northeast-2a\n    type: Public\n    zone: ap-northeast-2a\n  - cidr: 172.30.64.0/19\n    name: ap-northeast-2c\n    type: Public\n    zone: ap-northeast-2c\n  topology:\n    dns:\n      type: Public\n    masters: public\n    nodes: public\n...\n```\n\n위에서 추가한 설정은 다음과 같다.\n\n- [kube-API 접근제어 설정](https://kops.sigs.k8s.io/cluster_spec/#kubernetesapiaccess)\n- [기존 VPC내 CIDR 사용을 위한 구성](https://kops.sigs.k8s.io/run_in_existing_vpc/)\n- [네트워크 토폴로지 구성](https://kops.sigs.k8s.io/topology/)\n\n\nInstanceGroup 과 SSHCredential 를 작성한다.\n\n```yaml\napiVersion: kops.k8s.io/v1alpha2\nkind: InstanceGroup\nmetadata:\n  creationTimestamp: null\n  labels:\n    kops.k8s.io/cluster: prosv.kr\n  name: control-plane-ap-northeast-2a\nspec:\n  image: 099720109477/ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-20230302\n  instanceMetadata:\n    httpPutResponseHopLimit: 3\n    httpTokens: required\n  machineType: t3.medium\n  maxSize: 1\n  minSize: 1\n  role: Master\n  subnets:\n  - ap-northeast-2a\n```\n\n```yaml\napiVersion: kops.k8s.io/v1alpha2\nkind: InstanceGroup\nmetadata:\n  creationTimestamp: null\n  labels:\n    kops.k8s.io/cluster: prosv.kr\n  name: nodes-ap-northeast-2a\nspec:\n  image: 099720109477/ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-20230302\n  instanceMetadata:\n    httpPutResponseHopLimit: 1\n    httpTokens: required\n  machineType: t3.medium\n  maxSize: 1\n  minSize: 1\n  role: Node\n  subnets:\n  - ap-northeast-2a\n```\n\n```yaml\napiVersion: kops.k8s.io/v1alpha2\nkind: InstanceGroup\nmetadata:\n  creationTimestamp: null\n  labels:\n    kops.k8s.io/cluster: prosv.kr\n  name: nodes-ap-northeast-2c\nspec:\n  image: 099720109477/ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-20230302\n  instanceMetadata:\n    httpPutResponseHopLimit: 1\n    httpTokens: required\n  machineType: t3.medium\n  maxSize: 1\n  minSize: 1\n  role: Node\n  subnets:\n  - ap-northeast-2c\n```\n\n```yaml\napiVersion: kops.k8s.io/v1alpha2\nkind: SSHCredential\nmetadata:\n  creationTimestamp: null\n  labels:\n    kops.k8s.io/cluster: prosv.kr\n  name: admin\nspec:\n  publicKey: ssh-rsa AAAA.....\n    root@ip-172-31-50-160\n```\n\n작성한 모든 파일을 하나의 파일로 저장한 후 다음과 같이 kops CLI로 배포를 진행한다. 전체 파일은 다음 링크 예시를 참조한다.  \nhttps://raw.githubusercontent.com/ddiiwoong/kops-config/main/kops.yaml\n\n\n```sh\nkops create -f kops.yaml\n\nkops update cluster --name prosv.kr --ssh-public-key ~/.ssh/id_rsa.pub --yes\n```\n\n배포과 완료된 이후에 구성된 파드를 확인해보면 대부분은 기존 VPC CNI 구성요소와 동일하지만 몇가지 차이점은 노드마다 구성되어 있던 kube-proxy 대신 cilium 데몬셋 파드와 cilium 오퍼레이터 파드가 구성된것을 확인할 수 있다.  \n\n```sh\n$ kubectl get pod -n kube-system\nNAME                                            READY   STATUS    RESTARTS      AGE\n...\ncilium-l6j9d                                    1/1     Running   0             24h\ncilium-operator-697c58f5d5-8kk8m                1/1     Running   0             24h\ncilium-q9j27                                    1/1     Running   0             24h\ncilium-rx8zx                                    1/1     Running   0             24h\n...\nexternal-dns-598d5f5c76-pqp6r                   1/1     Running   0             24h\nhubble-relay-85df7fbcbf-79n52                   1/1     Running   0             24h\nhubble-relay-85df7fbcbf-rfchd                   1/1     Running   0             24h\nhubble-ui-5986c56d45-wn95w                      3/3     Running   0             23h\n...\n```\n\n## CNI 테스트\n\n테스트용 netshoot 파드를 생성한다.\n\n```yaml\ncat <<EOF | kubectl create -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot-pod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: netshoot-pod\n  template:\n    metadata:\n      labels:\n        app: netshoot-pod\n    spec:\n      containers:\n      - name: netshoot-pod\n        image: nicolaka/netshoot\n        command: [\"tail\"]\n        args: [\"-f\", \"/dev/null\"]\n      terminationGracePeriodSeconds: 0\nEOF\n```\n\n배포한 netshoot 파드의 IP를 확인한다.\n\n```sh\n# kubectl get pod -o=custom-columns=NAME:.metadata.name,IP:.status.podIP\nNAME                            IP\nnetshoot-pod-7757d5dd99-9mb7s   172.30.47.42\nnetshoot-pod-7757d5dd99-df68s   172.30.68.92\n```\n\nhubble 접근을 위해 https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/ 을 참조해서 hubble, cilium CLI를 Cloud9에 설치한다. Hubble API에 액세스하려면 로컬 머신에서 Hubble 서비스에 대한 포트 포워드를 생성한다.\n\n```sh\n$ cilium hubble port-forward&\nForwarding from 0.0.0.0:4245 -> 4245\nForwarding from [::]:4245 -> 4245\n\n$ hubble status\nHealthcheck (via localhost:4245): Ok\nCurrent/Max Flows: 12,285/12,285 (100.00%)\nFlows/s: 12.41\nConnected Nodes: 3/3\n```\n\n외부로 통신하는 트래픽을 발생시키고 `hubble observe` 명령을 통해 flow를 확인할 수 있다.\n\n```\n$ kubectl exec -it netshoot-pod-7757d5dd99-k4pf7 -- curl -s wttr.in/seoul                                              \n\n$ hubble observe --pod netshoot-pod-7757d5dd99-k4pf7                                                                   \nMar 18 17:24:57.786: default/netshoot-pod-7757d5dd99-k4pf7:35019 (ID:22269) -> 169.254.20.10:53 (world) to-stack FORWARDED (UDP)\nMar 18 17:24:57.788: default/netshoot-pod-7757d5dd99-k4pf7:35019 (ID:22269) <- 169.254.20.10:53 (world) to-endpoint FORWARDED (UDP)\nMar 18 17:24:57.788: default/netshoot-pod-7757d5dd99-k4pf7:51562 (ID:22269) -> 169.254.20.10:53 (world) to-stack FORWARDED (UDP)\nMar 18 17:24:57.789: default/netshoot-pod-7757d5dd99-k4pf7:51562 (ID:22269) <- 169.254.20.10:53 (world) to-endpoint FORWARDED (UDP)\nMar 18 17:24:57.789: default/netshoot-pod-7757d5dd99-k4pf7:57074 (ID:22269) -> 169.254.20.10:53 (world) to-stack FORWARDED (UDP)\nMar 18 17:24:57.790: default/netshoot-pod-7757d5dd99-k4pf7:57074 (ID:22269) <- 169.254.20.10:53 (world) to-endpoint FORWARDED (UDP)\nMar 18 17:24:57.790: default/netshoot-pod-7757d5dd99-k4pf7:49895 (ID:22269) -> 169.254.20.10:53 (world) to-stack FORWARDED (UDP)\nMar 18 17:24:57.791: default/netshoot-pod-7757d5dd99-k4pf7:49895 (ID:22269) <- 169.254.20.10:53 (world) to-endpoint FORWARDED (UDP)\nMar 18 17:24:57.792: default/netshoot-pod-7757d5dd99-k4pf7:42920 (ID:22269) -> 169.254.20.10:53 (world) to-stack FORWARDED (UDP)\nMar 18 17:24:57.794: default/netshoot-pod-7757d5dd99-k4pf7:42920 (ID:22269) <- 169.254.20.10:53 (world) to-endpoint FORWARDED (UDP)\nMar 18 17:24:58.050: default/netshoot-pod-7757d5dd99-k4pf7:40550 (ID:22269) -> 5.9.243.187:80 (world) to-stack FORWARDED (TCP Flags: SYN)\nMar 18 17:24:58.528: default/netshoot-pod-7757d5dd99-k4pf7:40550 (ID:22269) -> 5.9.243.187:80 (world) to-stack FORWARDED (TCP Flags: ACK, FIN)\nMar 18 17:24:58.766: default/netshoot-pod-7757d5dd99-k4pf7:40550 (ID:22269) <- 5.9.243.187:80 (world) to-endpoint FORWARDED (TCP Flags: ACK, FIN)\nMar 18 17:24:58.766: default/netshoot-pod-7757d5dd99-k4pf7:40550 (ID:22269) -> 5.9.243.187:80 (world) to-stack FORWARDED (TCP Flags: ACK)\n```\n\n이번에는 최대 파드 개수를 체크하기 위해 replica를 20개로 증가시킨다. 하지만 아래와 같이 몇개의 파드가 `ContainerCreating` 상태인 것을 확인할 수 있다.\n\n```sh\n$ kubectl scale deployment/netshoot-pod --replicas=20\ndeployment.apps/netshoot-pod scaled\n$ kubectl get pod\nNAME                            READY   STATUS              RESTARTS   AGE\nnetshoot-pod-7757d5dd99-2x29s   0/1     c   0          9s\nnetshoot-pod-7757d5dd99-5j9qc   1/1     Running             0          34m\nnetshoot-pod-7757d5dd99-9996z   1/1     Running             0          26m\nnetshoot-pod-7757d5dd99-9dnkg   1/1     Running             0          34m\nnetshoot-pod-7757d5dd99-9k92w   0/1     ContainerCreating   0          23s\nnetshoot-pod-7757d5dd99-9mb7s   1/1     Running             0          43m\nnetshoot-pod-7757d5dd99-c2292   1/1     Running             0          35m\nnetshoot-pod-7757d5dd99-cf7dv   1/1     Running             0          34m\nnetshoot-pod-7757d5dd99-df68s   1/1     Running             0          43m\nnetshoot-pod-7757d5dd99-h9fjm   0/1     ContainerCreating   0          23s\nnetshoot-pod-7757d5dd99-hrkcs   0/1     ContainerCreating   0          23s\nnetshoot-pod-7757d5dd99-jkc6v   1/1     Running             0          34m\nnetshoot-pod-7757d5dd99-jkx7j   1/1     Running             0          23s\nnetshoot-pod-7757d5dd99-k4pf7   1/1     Running             0          26m\nnetshoot-pod-7757d5dd99-lbzvs   1/1     Running             0          35m\nnetshoot-pod-7757d5dd99-nb4pf   1/1     Running             0          34m\nnetshoot-pod-7757d5dd99-r4bhf   1/1     Running             0          26m\nnetshoot-pod-7757d5dd99-t8rt6   1/1     Running             0          26m\nnetshoot-pod-7757d5dd99-x2d62   1/1     Running             0          26m\nnetshoot-pod-7757d5dd99-zm7vc   1/1     Running             0          35m\n```\n\n파드의 상태를 `describe` 명령으로 살펴보면 cilium agent가 AWS IPAM 과 연동되어 IP 부족으로 인해 할당이 되지 않음을 알 수 있다.\n\n```sh\n$ kubectl describe pod netshoot-pod-7757d5dd99-9k92w\n...\n  Warning  FailedCreatePodSandBox  3m43s (x85 over 21m)  kubelet            (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"eaced767b8747e09656744a3249a5e1c092fea7ba45302abdbf8086e3297c07d\": plugin type=\"cilium-cni\" name=\"cilium\" failed (add): unable to allocate IP via local cilium agent: [POST /ipam][502] postIpamFailure  No more IPs availabl\n```\n\nAWS VPC CNI 환경에서 최대 파드 사용 가능 계산식은 ENI 자체 IP를 제외하고 `aws-node` 와 `kube-proxy` 파드를 고려하여 다음과 같은 공식으로 계산을 하게 된다.  \nhttps://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt\n\n```\n((MaxENI * (IPv4addr - 1)) + 2)\nt3.medium 경우 : ((3 * (6 - 1) + 2 ) = 17개 >> aws-node 와 kube-proxy 2개 제외하면 15개\n```\n\nIPAM을 설정한 Cilium은 [AWS ENI](https://docs.cilium.io/en/stable/network/concepts/ipam/eni/) 기준을 따라간다. 공식 문서에서는 아직 확인할 수 없지만 [GitHub issue](https://github.com/cilium/cilium/issues/10426)로 추정해보면 `health`, `router` `CiliumInternalIP` 3개의 IP를 제외하기 때문에 t3.medium의 경우라면 3개의 ENI 당 6개의 아이피를 할당할 수 있기 때문에 실제 구성 가능한 개수는 12개 생성이 가능하다.\n\n```\n(MaxENI * (IPv4addr - 1))\nt3.medium 경우 : ((3 * (6 - 1)) = 15개 >> health, router, CiliumInternalIP 3개 제외하면 12개 파드가 생성이 가능하다. \n```\n\n이를 ciliumnode CRD를 통해서도 확인이 가능하다. \n\n첫번째 인터페이스에 할당된 `InternalIP`, `ExternalIP` 와 내부적으로 사용하는 `CiliumInternalIP` 를 확인할 수 있다.\n\n```yaml\nkubectl get ciliumnode i-00b3ebc1b78f19739 -o yaml\n\napiVersion: v1\nitems:\n- apiVersion: cilium.io/v2\n  kind: CiliumNode\n  metadata:\n    ...\n  spec:\n    addresses:\n    - ip: 172.30.60.14\n      type: InternalIP\n    - ip: 13.124.209.231\n      type: ExternalIP\n    - ip: 172.30.44.153\n      type: CiliumInternalIP\n\n```\n\n`spec.ipam`에서는 위에서 언급한 대로 15개의 할당가능한 IP를 보여준다.\n\n\n```yaml\n    ...\n    ipam:\n      podCIDRs:\n      - 100.96.2.0/24\n      pool:\n        172.30.32.43:\n          resource: eni-0717341e6470f76ce\n        172.30.32.217:\n          resource: eni-0483a896a7c0b49f3\n        172.30.36.179:\n          resource: eni-0717341e6470f76ce\n        172.30.38.188:\n          resource: eni-0483a896a7c0b49f3\n        172.30.38.221:\n          resource: eni-0483a896a7c0b49f3\n        172.30.42.57:\n          resource: eni-03b280d75b6d6cb09\n        172.30.44.103:\n          resource: eni-0717341e6470f76ce\n        172.30.44.153:\n          resource: eni-03b280d75b6d6cb09\n        172.30.44.201:\n          resource: eni-0717341e6470f76ce\n        172.30.47.42:\n          resource: eni-0483a896a7c0b49f3\n        172.30.47.95:\n          resource: eni-03b280d75b6d6cb09\n        172.30.53.208:\n          resource: eni-03b280d75b6d6cb09\n        172.30.55.231:\n          resource: eni-0717341e6470f76ce\n        172.30.61.93:\n          resource: eni-0483a896a7c0b49f3\n        172.30.63.46:\n          resource: eni-03b280d75b6d6cb09\n      pre-allocate: 8\n    ...\n```\n\n`status.ipam`에서는 현재 할당되어 있는 12개의 IP현황을 볼수 있다.\n\n```yaml\n    ipam:\n      operator-status: {}\n      used:\n        172.30.32.43:\n          owner: default/netshoot-pod-7757d5dd99-x2d62\n          resource: eni-0717341e6470f76ce\n        172.30.32.217:\n          owner: kube-system/metrics-server-5f65d889cd-kg8sb\n          resource: eni-0483a896a7c0b49f3\n        172.30.36.179:\n          owner: default/netshoot-pod-7757d5dd99-9996z\n          resource: eni-0717341e6470f76ce\n        172.30.38.188:\n          owner: default/netshoot-pod-7757d5dd99-9dnkg\n          resource: eni-0483a896a7c0b49f3\n        172.30.38.221:\n          owner: kube-system/coredns-68cd66b8cc-2bmzj\n          resource: eni-0483a896a7c0b49f3\n        172.30.42.57:\n          owner: kube-system/ebs-csi-node-t9dk5\n          resource: eni-03b280d75b6d6cb09\n        172.30.44.103:\n          owner: default/netshoot-pod-7757d5dd99-5j9qc\n          resource: eni-0717341e6470f76ce\n        172.30.44.153:\n          owner: router\n          resource: eni-03b280d75b6d6cb09\n        172.30.44.201:\n          owner: default/netshoot-pod-7757d5dd99-c2292\n          resource: eni-0717341e6470f76ce\n        172.30.47.42:\n          owner: default/netshoot-pod-7757d5dd99-9mb7s\n          resource: eni-0483a896a7c0b49f3\n        172.30.53.208:\n          owner: default/netshoot-pod-7757d5dd99-zm7vc\n          resource: eni-03b280d75b6d6cb09\n        172.30.55.231:\n          owner: kube-system/hubble-relay-85df7fbcbf-79n52\n          resource: eni-0717341e6470f76ce\n        172.30.61.93:\n          owner: default/netshoot-pod-7757d5dd99-jkc6v\n          resource: eni-0483a896a7c0b49f3\n        172.30.63.46:\n          owner: health\n          resource: eni-03b280d75b6d6cb09\n```\n\n## 정리\n\n이번에는 kOps 구성을 Cilium CNI 기반으로 구성하고 hubble을 통해 flow를 확인하고 AWS IPAM API 연동을 통한 파드 IP할당 테스트를 진행해봤다.\n\n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/kops-cloud9/",
      "metadata": {
        "permalink": "/kubernetes/kops-cloud9/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2023-03-08-kops-cloud9.md",
        "source": "@site/blog/2023-03-08-kops-cloud9.md",
        "title": "kOps with Cloud9",
        "description": "Cloud9 기반 kOps 환경 구성",
        "date": "2023-03-08T00:00:00.000Z",
        "formattedDate": "March 8, 2023",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "kOps",
            "permalink": "/tags/k-ops"
          },
          {
            "label": "Cloud9",
            "permalink": "/tags/cloud-9"
          },
          {
            "label": "CloudFormation",
            "permalink": "/tags/cloud-formation"
          },
          {
            "label": "HPA",
            "permalink": "/tags/hpa"
          }
        ],
        "readingTime": 9.205,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "kOps with Cloud9",
          "comments": true,
          "classes": "wide",
          "description": "Cloud9 기반 kOps 환경 구성",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/kops-cloud9/",
          "date": "2023-03-08T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "kOps",
            "Cloud9",
            "CloudFormation",
            "HPA"
          ]
        },
        "prevItem": {
          "title": "kOps with Cilium",
          "permalink": "/kubernetes/kops-cilium/"
        },
        "nextItem": {
          "title": "EKS CNI Custom Networking",
          "permalink": "/kubernetes/eks-cni-custom/"
        }
      },
      "content": "## kOps\n\n-   **K**ubernetes **Op**eration**s** (kOps) - Production Grade k8s Installation, Upgrades and Management\n\nhttps://kops.sigs.k8s.io/\nhttps://github.com/kubernetes/kops\n\nkOps는 클라우드 플랫폼(aws, gcp, azure 등)에서 쉽게 k8s 를 설치할 수 있도록 도와주는 도구로 서버 인스턴스와 네트워크 리소스 등을 클라우드에서 자동으로 생성해 k8s 를 설치하는 도구이고 kOps는 AWS 의 다양한 서비스와 유연하게 연동되어 사용 가능한게 장점이다. \n\n글을 작성하는 날짜 기준(23년 3월 5일), 버전은 1.25.3 으로 진행을 한다.\n\n<!--truncate-->\n\n## Bastion 환경 구성\n\nBasition은 Cloud9으로 구성을 했다. https://github.com/aws-samples/aws-cloud9-bootstrapping-example 를 참고하여 구성했고 여러 툴을 bootstrap 형태로 구성하기 위해 추가적으로 여러 도구들을 설치한다.\n\n- kubectl\n- kops\n- awscli v2\n- k9s\n- helm\n- jq, git, htop, tree, gettext, bash-completion 등\n\n자세한 내용은 https://github.com/jinwoongk/aws-cloud9-bootstrapping-example/blob/main/example_instancestack.yaml 에서 확인할 수 있다. 해당 yaml 에서 본인이 주로 접속하는 Console 접속 정보(ARN)을 아래 `OwnerArn` 구문과 같이 업데이트를 진행한다. \n\n```yaml\n################## INSTANCE #####################\n  ExampleC9InstanceProfile:\n    Type: AWS::IAM::InstanceProfile\n    Properties:\n      Path: \"/\"\n      Roles:\n      - Ref: ExampleC9Role\n\n  ExampleC9Instance:\n    Description: \"-\"\n    DependsOn: ExampleC9BootstrapAssociation\n    Type: AWS::Cloud9::EnvironmentEC2\n    Properties:\n      Description: AWS Cloud9 instance for Examples\n      AutomaticStopTimeMinutes: 3600\n      InstanceType:\n        Ref: ExampleC9InstanceType\n      Name:\n        Ref: AWS::StackName\n      # OwnerArn: !If [Create3rdPartyResources, !Ref ExampleOwnerArn, !Ref \"AWS::NoValue\" ]\n      OwnerArn: \"arn:aws:sts::265664683898:assumed-role/AWSReservedSSO_AWSAdministratorAccess_574b6756fcc31821/jinwoong\"\n```\n\n해당 yaml 을 가지고 아래 명령으로 Cloudformation을 실행하게 되면 여러가지 유틸이 설치된 상태의 Cloud9 인스턴스가 생성되게 된다. \n\n```sh\naws cloudformation deploy  --stack-name kops-test --template ./example_instancestack.yaml\n```\n\nCloud9을 위해 생성된 권한을 활용하기 위해 Cloud9 설정에서 AWS managed temporary credentials 기능을 비활성화 하자.\n\n![cloud9](/img/cloud9_credential.png)\n\n\n## kOps 배포\n\nAWS 환경에 kOps를 배포하기 위해서는 아래와 같은 IAM 권한이 필요하다. \n\n- AmazonEC2FullAccess\n- AmazonRoute53FullAccess \n- AmazonS3FullAccess \n- IAMFullAccess \n- AmazonVPCFullAccess \n- AmazonSQSFullAccess \n- AmazonEventBridgeFullAccess\n\nkOps는 Cluster State 저장을 위한 S3 버킷과 DNS 레코드를 활용해서 클러스터 생성을 하게 되는데 편의를 위해 개인 사용중인 도메인의 NS서버를 Route53에 연결하고 이미 생성해놓은 버킷을 가지고 배포를 진행한다.\n\n```sh\n# 배포 시 참고할 정보를 환경 변수에 저장\n## export NAME=<자신의 퍼블릭 호스팅 메인 주소>\n## export KOPS_STATE_STORE=s3://(버킷 이름)\nexport KOPS_CLUSTER_NAME=prosv.kr\nexport KOPS_STATE_STORE=s3://jinwoong-k8s-s3\nexport AWS_PAGER=\"\"\nexport REGION=ap-northeast-2\n```\n\nCloud9 환경에서 `kops` 명령을 통해 클러스터를 구성하게 되는데 `VPC CNI`를 사용하기 위해 `amazonvpc`로 구성하고 `1.24` 버전의 master 노드 1대, work 노드 3대를 생성한다. \n\n```sh\n$ kops create cluster --zones=\"$REGION\"a,\"$REGION\"c \\\n--networking amazonvpc --cloud aws \\\n--master-size t3.medium --node-size t3.medium --node-count=3 \\\n--network-cidr 172.30.0.0/16 --ssh-public-key ~/.ssh/id_rsa.pub \\\n--name=$KOPS_CLUSTER_NAME --discovery-store=s3://jinwoong-k8s-s3 \\\n--kubernetes-version \"1.24.10\" -y\n```\n\n클러스터가 생성되는 동안 유효성을 체크하기 위해 10분간 대기하고 구성이 완료되면 아래와 같이 초기 구성한 클러스터가 ready 상태인지 알 수 있다. `kops get` 명령을 통해 다른 정보들을 확인할 수 있다. \n\n```sh\n$ kops validate cluster --wait 10m\n\nValidating cluster prosv.kr\n\nINSTANCE GROUPS\nNAME                    ROLE    MACHINETYPE     MIN     MAX     SUBNETS\nmaster-ap-northeast-2a  Master  t3.medium       1       1       ap-northeast-2a\nnodes-ap-northeast-2a   Node    t3.medium       2       2       ap-northeast-2a\nnodes-ap-northeast-2c   Node    t3.medium       1       1       ap-northeast-2c\n\nNODE STATUS\nNAME                    ROLE    READY\ni-053ca67a20cd7bf7f     node    True\ni-0a7dfc0939b58c2fc     node    True\ni-0aa3db1bfe96bf280     master  True\ni-0b665798ce9dd857f     node    True\n\nYour cluster prosv.kr is ready\n\n$ kops get cluster\nNAME            CLOUD   ZONES\nprosv.kr        aws     ap-northeast-2a,ap-northeast-2c\n\n$ kops get instances\n\nID                      NODE-NAME               STATUS          ROLES   STATE   INTERNAL-IP     INSTANCE-GROUP                          MACHINE-TYPE\ni-053ca67a20cd7bf7f     i-053ca67a20cd7bf7f     UpToDate        node            172.30.49.113   nodes-ap-northeast-2a.prosv.kr          t3.medium\ni-0a7dfc0939b58c2fc     i-0a7dfc0939b58c2fc     UpToDate        node            172.30.77.157   nodes-ap-northeast-2c.prosv.kr          t3.medium\ni-0aa3db1bfe96bf280     i-0aa3db1bfe96bf280     UpToDate        master          172.30.63.194   master-ap-northeast-2a.masters.prosv.kr t3.medium\ni-0b665798ce9dd857f     i-0b665798ce9dd857f     UpToDate        node            172.30.54.137   nodes-ap-northeast-2a.prosv.kr          t3.medium\n```\n\n기본 CRI 컨테이너 런타임이 containerd 인 것도 확인할 수 있다. 각 노드가 External-IP를 할당받은 것을 확인할 수 있다.\n\n```sh\n$ kubectl get nodes -o wide\nNAME                  STATUS   ROLES           AGE   VERSION    INTERNAL-IP     EXTERNAL-IP      OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME\ni-053ca67a20cd7bf7f   Ready    node            42m   v1.24.10   172.30.49.113   43.201.14.97     Ubuntu 20.04.5 LTS   5.15.0-1028-aws   containerd://1.6.10\ni-0a7dfc0939b58c2fc   Ready    node            42m   v1.24.10   172.30.77.157   43.201.77.155    Ubuntu 20.04.5 LTS   5.15.0-1028-aws   containerd://1.6.10\ni-0aa3db1bfe96bf280   Ready    control-plane   44m   v1.24.10   172.30.63.194   43.200.6.4       Ubuntu 20.04.5 LTS   5.15.0-1028-aws   containerd://1.6.10\ni-0b665798ce9dd857f   Ready    node            42m   v1.24.10   172.30.54.137   13.125.115.198   Ubuntu 20.04.5 LTS   5.15.0-1028-aws   containerd://1.6.10\n```\n\nkubectl cli 플러그인 매니저인 쿠버네티스 `krew`를 설치하고 여러가지 도구를 설치한다. 설치되는 플러그인 말고도 다양한 플러그인들은 아래 링크에서 확인이 가능하다.\n- https://krew.sigs.k8s.io/plugins/\n\n```sh\n$ kubectl krew install ctx ns df-pv get-all ktop neat oomd view-secret\n```\n\n\n## Add-on 구성\n\nExternalDNS와 같은 add-on을 구성하고 Kubernetes 서비스 또는 인그레스 생성 시 도메인을 설정하면, AWS(Route 53)와 같은 클라우드 DNS 서비스에서 A 레코드(TXT 레코드)를 자동 생성/삭제할 수 있다. Route53에서 동작을 위해 다음과 같은 IAM Policy가 필요하다.  \n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n```\n\n해당 json으로 IAM Policy를 생성하고 master, worker 노드 instance profile에 추가한다.\n\n```sh\naws iam create-policy --policy-name AllowExternalDNSUpdates --policy-document file://externaldns-aws-r53-policy.json\n\nexport ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\n\naws iam attach-role-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AllowExternalDNSUpdates --role-name masters.$KOPS_CLUSTER_NAME\n\naws iam attach-role-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AllowExternalDNSUpdates --role-name nodes.$KOPS_CLUSTER_NAME\n```\n\n이 상태에서 `externalDNS`를 구성한다.  `externalDNS` 와 같은 여러 애드온들은 kOps와 쿠버네티스 라이프사이클에 따라 구성, 관리된다. 기본적으로 API 서버는 메트릭 서버 TLS 인증서를 확인하지 않기 때문에 TLS를 사용하려면 클러스터 spec에서 `certManager`도 같이 추가한다. 또한 HPA 구성을 위해 `metricServer` 도 구성을 진행한다.\n\n\n```yaml\n$ kops edit cluster\n...\nspec:\n  certManager:\n    enabled: true\n  externalDns:\n    provider: external-dns\n  metricsServer:\n    enabled: true\n    insecure: false\n...\n\n$ kops update cluster --yes && echo && sleep 3 && kops rolling-update cluster --yes\n```\n\n일단 php-apache deployment를 시작한다. \n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: php-apache\nspec:\n  selector:\n    matchLabels:\n      run: php-apache\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: php-apache\n    spec:\n      containers:\n      - name: php-apache\n        image: registry.k8s.io/hpa-example\n        ports:\n        - containerPort: 80\n        resources:\n          limits:\n            cpu: 500m\n          requests:\n            cpu: 200m\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: php-apache\n  labels:\n    run: php-apache\nspec:\n  ports:\n  - port: 80\n  selector:\n    run: php-apache\n  type: LoadBalancer\n```\n\n바로 HPA 테스트를 위해 간단하게 사용률을 기반으로 스케일링이 동작하도록 구성하고 CPU 로드를 주입한다. `load-generator`에서는 0.01초 단위로 리퀘스트를 보내게 된다. \n\nhttps://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\n\n```sh\n$ kubectl autoscale deployment php-apache --cpu-percent=10 --min=1 --max=4\n\n$ kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://php-apache; done\"\nIf you don't see a command prompt, try pressing enter.\nOK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!\n```\n\n다음으로, HPA가 부하 증가에 어떻게 반응하는지 확인할 수 있다.\n\n```sh\n$ kubectl get hpa php-apache --watch\nNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nphp-apache   Deployment/php-apache   93%/10%   1         4         4          1m\n```\n\n마지막으로 CLB에 ExternalDNS로 도메인 연결을 하기 위해서는 expose를 원하는 Service에 다음과 같이 annotation을 추가하는 방법으로 외부 도메인을 쉽게 연결할 수 있다.\n\n```sh\n$ kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/hostname=nginx.$KOPS_CLUSTER_NAME\"\n```\n\n\n## 정리\n\n항상 EKS만 활용하는 프로젝트를 진행하다보니 스터디 후에 오랜만에 직접 구성하는 경험을 해보니 트러블슈팅하는 재미가 있어서 좋은것 같다. 이런저런 add-on을 추가적으로 구성해서 더 해보고 싶지만 역시나 시간이 없는 관계로 일단 여기 까지 구성을 하고 이번에 설치한 Cloud9으로 배포되는 클러스터 구성은 CKS를 준비하고 팀 내부 세션을 진행하는 도구로 활용을 해볼 예정이다.\n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/eks-cni-custom/",
      "metadata": {
        "permalink": "/kubernetes/eks-cni-custom/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2022-12-16-eks-cni-custom.md",
        "source": "@site/blog/2022-12-16-eks-cni-custom.md",
        "title": "EKS CNI Custom Networking",
        "description": "Terraform null_resource를 활용하여 EKS CNI network를 구성하는 방법",
        "date": "2022-12-16T00:00:00.000Z",
        "formattedDate": "December 16, 2022",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "EKS",
            "permalink": "/tags/eks"
          },
          {
            "label": "CNI",
            "permalink": "/tags/cni"
          },
          {
            "label": "Terraform",
            "permalink": "/tags/terraform"
          }
        ],
        "readingTime": 10.79,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "EKS CNI Custom Networking",
          "comments": true,
          "classes": "wide",
          "description": "Terraform null_resource를 활용하여 EKS CNI network를 구성하는 방법",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/eks-cni-custom/",
          "date": "2022-12-16T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "EKS",
            "CNI",
            "Terraform"
          ]
        },
        "prevItem": {
          "title": "kOps with Cloud9",
          "permalink": "/kubernetes/kops-cloud9/"
        },
        "nextItem": {
          "title": "EKS Anywhere Advanced Usages",
          "permalink": "/kubernetes/eks-anywhere-adv/"
        }
      },
      "content": "## EKS CNI Networking 제약사항\n\n[https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html#custom-networking-automatically-apply-eniconfig](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html#custom-networking-automatically-apply-eniconfig)  \n  \nAWS에는 기본 ENI가 포함된 서브넷에서 사용할 수 있는 IP개수는 제한되어 있다. 파드의 수 제한이 발생할 수 있기 때문에 secondary ENI에 다른 서브넷을 사용하여 가용 IP개수를 늘릴수 있다. 또한 보안상의 이유로 파드는 노드의 기본 네트워크 인터페이스와 다른 서브넷 또는 보안 그룹을 사용해야 할 수 있다.   \n  \nCNI Custom Networking이 활성화가 되면 파드는 다른 서브넷에 생성이 되고, 노드 서브넷의 아이피를 사용하지 않는다.\n\nEKS에서 파드 대역을 분리 하기 위해서 CNI Custom Networking 설정을 진행한다. 해당 env 값을 변경하면 즉시 aws-node 가 교체된다.\n\n<!--truncate-->\n  \n```\nkubectl set env daemonset aws-node -n kube-system AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true\n```\n\n파드를 배포하려는 각 서브넷에 대해서 아래와 같은 형식으로 ENIConfig 리소스를 생성해야 한다. 사용하려는 서브넷과 SG을 선언해야 기본적인 구성이 가능하다. \n\n```\napiVersion: crd.k8s.amazonaws.com/v1alpha1  \nkind: ENIConfig  \nmetadata:  \nname: \"ap-northeast-2a\"  \nspec:  \nsubnet: \"${subnet_a}\"  \nsecurityGroups:  \n- ${NODE_SG}  \n  \napiVersion: crd.k8s.amazonaws.com/v1alpha1  \nkind: ENIConfig  \nmetadata:  \nname: \"ap-northeast-2b\"  \nspec:  \nsubnet: \"${subnet_c}\"  \nsecurityGroups:  \n- ${NODE_SG}  \n  \napiVersion: crd.k8s.amazonaws.com/v1alpha1  \nkind: ENIConfig  \nmetadata:  \nname: \"ap-northeast-2c\"  \nspec:  \nsubnet: \"${subnet_d}\"  \nsecurityGroups:  \n- ${NODE_SG}\n```\n\n`aws-node` `DaemonSet`를 업데이트하여 클러스터에 생성된 모든 노드에 가용 영역의 `ENIConfig`를 자동으로 적용할 수 있다.\n\n```\nkubectl set env daemonset aws-node -n kube-system ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone\n```\n\n이때 몇가지 제약사항이 생기게 된다.\n\n- 해당 설정을 배포하고 난 이후 추가된 노드에 대해서만 새로운 서브넷 IP로 할당이 되기 때문에, 기존 노드를 재시작하거나 삭제를 해야 파드에 새로운 IP가 할당이 된다.\n\n- CNI Custom Networking 설정에서는 다중 서브넷을 할당할 수 없고 서브넷 별로 custom name으로 설정하고 모든 node 각각에 입력한 별도의 annotation을 추가해야한다. ([https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html#custom-networking-deploy-nodes](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html#custom-networking-deploy-nodes))\n\n- AWS의 인스턴스 타입별 ENI 개수 제한으로 Pod 생성이 불가능할 수 있으므로 Kubelet BootstrapArguments 파라미터를 launch template 에서 사전에 정의된 값으로 변경하여 노드가 생성되도록 한다.\n\n-   MAX PODS = (Number of network interfaces × [the number of IP addresses per network interface – 1]) + 2\n-   [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI)\n\n```\nMIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\"\n\n--==MYBOUNDARY==\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n#!/bin/bash\n/etc/eks/bootstrap.sh {Cluster Name} --use-max-pods false --kubelet-extra-args '--max-pods=110'--==MYBOUNDARY==--\\\n```\n\n이러한 제약사항들을 처리하기 위해서 CNI Custom Networking 구현을 Terraform Pipeline으로 구현을 진행한다.\n\n## Terraform 으로 CNI 구성하는 스크립트 실행\n\nEKS를 프로비저닝 하는 Terraform 코드는 많이 공개되어 있기 때문에 이번 포스트에서는 생략하기로 하고 VPC Custom CNI를 구성하는 부분만 살펴보자.\n\n기본적으로 VPC CNI를 배포하게 되면 aws-node 재시작이 되기 때문에 Workload NodeGroup이 구성되기 이전에 아래 스크립트를 `null_resource` 리소스를 사용하여 다음과 같은 순서로 Batch를 실행하도록 했다.\n\n-  kubectl 설치\n-  AWS Authentication (Assume role)\n-  kubeconfig update\n-  VPC CNI Custom manifest (버전 v1.11.2) 적용 - [https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)\n-  기본 템플릿 : [https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/v1.11.2/config/master/aws-k8s-cni.yaml](https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/v1.11.2/config/master/aws-k8s-cni.yaml)\n-  `ENABLE_POD_ENI=true` : Security Group for Pods를 위한 구성으로 Node에 trunk ENI 추가를 하기 위한 `vpc.amazonaws.com/has-trunk-attached` 를 추가한다. \n\t-   Pod ENI를 활성화한 경우 Branch ENI를 생성하는 부분에서 ENI 생성 [제약사항](https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/pkg/aws/vpc/limits.go)이 발생할수 있다.\n-  `DISABLE_TCP_EARLY_DEMUX=true` : `ENABLE_POD_ENI`가 true로 설정된 경우 kubelet이 TCP를 통해 포드 보안 그룹을 사용하는 포드에 연결하려면 `DISABLE_TCP_EARLY_DEMUX`를 initcontainers 컨테이너(amazon-k8s-cni-init)를 통해 true로 설정해야 한다.\n-  `AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true` : worker 노드에서 파드가 별도의 서브넷 및 보안 그룹을 사용할 수 있다고 지정한다. IPAMD가 ENI 할당을 위해 worker 노드의 `ENIConfig`에서 지정한 보안 그룹과 VPC 서브넷을 사용하게 된다. \n-  `ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone` - `ENIConfig`와 매칭되도록 노드에서 사용할 가용영역을 zone 별로 구성하는 label을 추가한다.\n\t-   `ENIConfig` 라는 커스텀 리소스를 생성한다. 해당 설정은 파드용 IP대역을 위한 secondary ENI에 적용될 값으로 subnet id와 sg 값을 spec에 지정해야 한다. 아래 subnet과 sg id는 terraform에서 EKS클러스터가 프로비저닝 될때 결과를 env로 저장된 값을 가져오는 방식으로 처리 할수 있도록 하였다.\n\t-   `ENIConfig`를 사용할 가용 영역과 동일하게 `ENIConfig`의 name을 지정하는 것이 좋다. 만약 custom name으로 설정하게 되면 모든 node 각각에 입력한 별도의 annotation을 추가해야 한다. ([https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html#custom-networking-deploy-nodes](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/cni-custom-network.html#custom-networking-deploy-nodes))\n-  `ENABLE_PREFIX_DELEGATION=true` 파라미터를 사용하여 네트워크 인터페이스에 접두사를 할당하도록 VPC CNI 플러그인을 구성한다. \n-  `WARM_IP_TARGET=1`, `MINIMUM_IP_TARGET=30`\n\t-  AWS VPC CNI에는 두 가지 구성 요소가 있다. CNI 플러그인 바이너리 (/opt/cni/bin/aws-cni)및 로컬IP 주소를 관리하는 `ipamd`는 aws-node라는 kubernetes daemonset 으로 실행되며 인스턴스에 연결된 모든 ENI 및 IP를 추적하는 모든 노드에 파드를 추가한다.\n\t-  CNI 플러그인은 호스트 네트워크 연결과 올바른 네트워크 인터페이스를 파드의 네임스페이스에 추가하는 역할을 한다. CNI 플러그인은 원격 프로시저 호출(gRPC)을 통해 `IPAMD`와 통신한다.\n\t-  `WARM_IP_TARGET`은 ENI 전체를 Warm Pool로 가져가지 않고 특정 IP 개수만큼만 Warm Pool을 확보하겠다라는 설정이다. `WARM_IP_TARGET=1`로 설정되면 Pod이 생성될 때마다 Secondary IP를 1개씩 추가로 할당한다.\n\t-  Pod의 생성과 종료가 빈번한 경우, 인스턴스에 IP를 Attach/Detach 하는 API call의 수가 늘어나 ENI를 추가하거나 IP를 추가하는 throttling을 방지하기 위해 `MINIMUM_IP_TARGET`을 함께 설정한다. 노드에 일반적으로 유지되는 Pod의 수를 `MINIMUM_IP_TARGET`으로 지정해 놓으면 해당 수만큼 ENI에 미리 IP가 할당된다. \n\t-  `WARM_PREFIX_TARGET=1` 는 위 설정을 적용하면 재정의 된다.\n\n-  `AWS_VPC_K8S_CNI_EXTERNALSNAT=true` : 파드별 ENI에 할당된 IP주소를 제어할수 있도록 Node IP로 SNAT 되지않도록 설정하는 구성이다.\n-  `ENIConfig` 구성을 위해 Pod가 사용할 대역을 변수(`subnet_a`,`subnet_b`,`subnet_c`)로 받아 kubectl로 설정한다.\n\n```sh\n#!/bin/bash  \n \nKUBE_CONFIG=\"$(mktemp)\"  \naws eks update-kubeconfig --name \"${CLUSTER_NAME}\" --kubeconfig \"${KUBE_CONFIG}\"  \n  \n## Apply VPC CNI manifests (origin: https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/v1.11.2/config/master/aws-k8s-cni.yaml)  \n# Security group for pod env configurations ; ENABLE_POD_ENI=true  \n# https://github.com/aws/amazon-vpc-cni-k8s#disable_tcp_early_demux-v173 ; DISABLE_TCP_EARLY_DEMUX=true  \n# Custom networking ; AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true  \n# Custom networking ; ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone  \n# Custom networking ; WARM_IP_TARGET=1  \n# Custom networking ; MINIMUM_IP_TARGET=30  \n# Custom networking ; WARM_PREFIX_TARGET=1  \n# Fewer API calls required to EC2 control plane ; ENABLE_PREFIX_DELEGATION=true  \n# Source NAT disabled ; AWS_VPC_K8S_CNI_EXTERNALSNAT=true  \n  \nKUBE_CONFIG=\"$(mktemp)\"\naws eks update-kubeconfig --name \"${CLUSTER_NAME}\" --kubeconfig \"${KUBE_CONFIG}\"\n\n## Apply VPC CNI manifests (origin: https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/v1.11.2/config/master/aws-k8s-cni.yaml)\n# Security group for pod env configurations ; ENABLE_POD_ENI=true\n# https://github.com/aws/amazon-vpc-cni-k8s#disable_tcp_early_demux-v173 ; DISABLE_TCP_EARLY_DEMUX=true\n# Custom networking ; AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG=true\n# Custom networking ; ENI_CONFIG_LABEL_DEF=topology.kubernetes.io/zone\n# Custom networking ; WARM_IP_TARGET=1\n# Custom networking ; MINIMUM_IP_TARGET=30\n# Custom networking ; WARM_PREFIX_TARGET=1\n# Fewer API calls required to EC2 control plane ; ENABLE_PREFIX_DELEGATION=true\n# Source NAT disabled ; AWS_VPC_K8S_CNI_EXTERNALSNAT=true\n\nkubectl --kubeconfig \"${KUBE_CONFIG}\" apply -f ./scripts/aws-k8s-cni.yaml\n\n# set up ENIConfig \nsubnet_a=$(echo ${SUBNETS} |awk -F\",\" '{print $1}')\nsubnet_b=$(echo ${SUBNETS} |awk -F\",\" '{print $2}')\nsubnet_c=$(echo ${SUBNETS} |awk -F\",\" '{print $3}')\n\ncat <<EOF | kubectl --kubeconfig \"${KUBE_CONFIG}\" apply -f -\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata:\n  name: \"ap-northeast-2a\"\nspec:\n  subnet: \"${subnet_a}\"\n  securityGroups:\n  - ${CLUSTER_SG}\nEOF\n\ncat <<EOF | kubectl --kubeconfig \"${KUBE_CONFIG}\" apply -f -\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata:\n  name: \"ap-northeast-2b\"\nspec:\n  subnet: \"${subnet_b}\"\n  securityGroups:\n  - ${CLUSTER_SG}\nEOF\n\ncat <<EOF | kubectl --kubeconfig \"${KUBE_CONFIG}\" apply -f -\napiVersion: crd.k8s.amazonaws.com/v1alpha1\nkind: ENIConfig\nmetadata:\n  name: \"ap-northeast-2c\"\nspec:\n  subnet: \"${subnet_c}\"\n  securityGroups:\n  - ${CLUSTER_SG}\nEOF\n\n# remobe creds\nrm \"${KUBE_CONFIG}\"\n```\n\n다음은 위 스크립트를 실행하는 테라폼 코드의 예시이다.\n\n- 기본적으로 null_resource를 사용한다. \n\t- https://registry.terraform.io/providers/hashicorp/null/latest/docs/resources/resource\n- 위에서 작성한 스크립트를 network.sh 에서 환경변수를 받아서 실행할 수 있도록 구성한다. \n- eks 모듈의 의존성을 걸어 eks 클러스터 배포가 완료된 이후에 해당 스크립트가 동작하도록 한다. \n\n```terraform\n# Configure CNI custom network\nresource \"null_resource\" \"cni_patch\" {\n  triggers = {\n    cluster_name  = local.cluster_name\n    cluster_sg    = module.eks.cluster_primary_security_group_id\n    intra_subnets = join(\",\", module.vpc.intra_subnets)\n    content       = file(\"${path.module}/scripts/network.sh\")\n  }\n  provisioner \"local-exec\" {\n    environment = {\n      CLUSTER_NAME = self.triggers.cluster_name\n      CLUSTER_SG   = self.triggers.cluster_sg\n      SUBNETS      = self.triggers.intra_subnets\n    }\n    command     = \"${path.cwd}/scripts/network.sh\"\n    interpreter = [\"bash\"]\n  }\n  depends_on = [\n    module.eks\n  ]\n}\n```\n\n## 정리\n\n초반에 언급한 EKS CNI Custom Networking 제약사항을 처리하고 null_resource로 EKS CNI를 배포할 때 원하는 형태로 수정하여 `aws_node` 재기동 없이 배포하는 방법을 작성해봤다. \n\n샘플 코드는 아래 저장소에서 확인할 수 있다.\nhttps://github.com/jinwoongk/eks-cni-custom\n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/eks-anywhere-adv/",
      "metadata": {
        "permalink": "/kubernetes/eks-anywhere-adv/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2022-03-18-eks-anywhere-adv.md",
        "source": "@site/blog/2022-03-18-eks-anywhere-adv.md",
        "title": "EKS Anywhere Advanced Usages",
        "description": "EKS Anywhere 로 구성된 쿠버네티스 클러스터 활용 ",
        "date": "2022-03-18T00:00:00.000Z",
        "formattedDate": "March 18, 2022",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "vsphere",
            "permalink": "/tags/vsphere"
          },
          {
            "label": "eks anywhere",
            "permalink": "/tags/eks-anywhere"
          },
          {
            "label": "eks connector",
            "permalink": "/tags/eks-connector"
          },
          {
            "label": "eks",
            "permalink": "/tags/eks"
          },
          {
            "label": "opentelemetry",
            "permalink": "/tags/opentelemetry"
          },
          {
            "label": "cilium",
            "permalink": "/tags/cilium"
          },
          {
            "label": "observability",
            "permalink": "/tags/observability"
          },
          {
            "label": "prometheus",
            "permalink": "/tags/prometheus"
          },
          {
            "label": "grafana",
            "permalink": "/tags/grafana"
          },
          {
            "label": "loki",
            "permalink": "/tags/loki"
          },
          {
            "label": "microservice",
            "permalink": "/tags/microservice"
          },
          {
            "label": "istio",
            "permalink": "/tags/istio"
          }
        ],
        "readingTime": 17.06,
        "truncated": true,
        "authors": [
          {
            "name": "Jinwoong Kim",
            "title": "Technologist and Cloud Consultant",
            "url": "https://www.linkedin.com/in/ddiiwoong/",
            "imageURL": "https://s.gravatar.com/avatar/e8bfebcbeacb5b9a0c90614e792febf2?s=80",
            "key": "jinwoong"
          }
        ],
        "frontMatter": {
          "layout": "single",
          "title": "EKS Anywhere Advanced Usages",
          "comments": true,
          "classes": "wide",
          "description": "EKS Anywhere 로 구성된 쿠버네티스 클러스터 활용 ",
          "authors": "jinwoong",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/eks-anywhere-adv/",
          "date": "2022-03-18T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "vsphere",
            "eks anywhere",
            "eks-anywhere",
            "eks connector",
            "eks",
            "opentelemetry",
            "cilium",
            "observability",
            "prometheus",
            "grafana",
            "loki",
            "microservice",
            "istio"
          ]
        },
        "prevItem": {
          "title": "EKS CNI Custom Networking",
          "permalink": "/kubernetes/eks-cni-custom/"
        },
        "nextItem": {
          "title": "EKS Anywhere on vSphere Homelab",
          "permalink": "/kubernetes/eks-anywhere/"
        }
      },
      "content": "지난글에 이어 이번에는 홈랩으로 구성한 EKS Anywhere 클러스터에서 운영, 개발환경 구성을 위한 여러가지 도구를 설치해보는 과정을 정리하고자 한다. \n\n지난 포스팅은 다음 링크에서 확인할 수 있다.\n> 이전글 - [vSphere homelab 환경에서 EKS Anywhere 구성하기](https://ddii.dev/kubernetes/eks-anywhere/)\n\n<!--truncate-->\n\n## EKS Connector\n\nEKS Connector를 통해 EKS Anywhere를 포함한 외부의 모든 클러스터를 AWS 콘솔에 연결할 수 있다. eks connector 컨테이너를 통해 proxy 형태로 외부 쿠버네티스 클러스터의 API서버에서 정보를 가져와 AWS System Manager로 전달하는 비동기 방식으로 워크로드 관리를 진행한다.\n\n- 유저가이드 - [https://docs.aws.amazon.com/eks/latest/userguide/eks-connector.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-connector.html)  \n- AWS 블로그 - [https://aws.amazon.com/blogs/containers/connect-any-kubernetes-cluster-to-amazon-eks/](https://aws.amazon.com/blogs/containers/connect-any-kubernetes-cluster-to-amazon-eks/)\n\n### 클러스터 등록\n\n![overview](https://d2908q01vomqb2.cloudfront.net/fe2ef495a1152561572949784c16bf23abb28057/2021/09/15/Screen-Shot-2021-09-03-at-4.28.27-PM.png)\n\nEKS Anywhere를 포함한 외부 K8s 클러스터는 AWS CLI, eksctl, 콘솔을 사용해서 등록(register)이 가능하다. eksctl로 등록을 진행하면 먼저 관련된 Role을 생성해주고 또한 ClusterRole, ClusterRoleBinding 및 Connector StatefulSet을 배포할 수 있도록 자동으로 manifest를 생성해주기 때문에 eksctl로 클러스터 등록을 진행하는 것이 편하다. AWS Managed IAM 역할인 `AWSServiceRoleForAmazonEKSConnector`을 생성하는 과정을 진행하기 때문에 에러가 발생을 한다. 에러가 난 이후에 AWS 콘솔이나 CLI를 통해 확인하면 새롭게 생성된 `AWSServiceRoleForAmazonEKSConnector` IAM 역할과 해당 역할에 연결된 Policy인 `AmazonEKSConnectorServiceRolePolicy`를 확인할 수 있다. \n\n```sh\n> eksctl register cluster --name homelab --provider eks_anywhere --profile jinwoong\n\n2022-03-09 18:50:44 [ℹ]  creating IAM role \"eksctl-20220309185044427640\"\n2022-03-09 18:50:49 [ℹ]  deleting IAM role \"eksctl-20220309185044427640\"\nError: error registering cluster: error calling RegisterCluster: ResourcePropagationDelayException: Service linked role 'arn:aws:iam::492935017096:role/aws-service-role/eks-connector.amazonaws.com/AWSServiceRoleForAmazonEKSConnector' is propagating, please retry later.\n```\n\n실패를 진행한 후에 다시 등록 시도를 하면 아래 출력로그와 같이 명령어를 실행한 디렉토리에 EKS Connector 관련 리소스 manifests(eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yaml)를 확인할 수 있다. \n\n```\n> eksctl register cluster --name homelab --provider eks_anywhere --profile jinwoong\n2022-03-08 22:59:15 [ℹ️]  creating IAM role \"eksctl-20220308225915739385\"\n2022-03-08 22:59:29 [ℹ️]  registered cluster \"homelab\" successfully\n2022-03-08 22:59:29 [ℹ️]  wrote file eks-connector.yaml to /Users/jinwoong/study/eks-anywhere-homelab\n2022-03-08 22:59:29 [ℹ️]  wrote file eks-connector-clusterrole.yaml to /Users/jinwoong/study/eks-anywhere-homelab\n2022-03-08 22:59:29 [ℹ️]  wrote file eks-connector-console-dashboard-full-access-group.yaml to /Users/jinwoong/study/eks-anywhere-homelab\n2022-03-08 22:59:29 [!]  note: \"eks-connector-clusterrole.yaml\" and \"eks-connector-console-dashboard-full-access-group.yaml\" give full EKS Console access to IAM identity \"arn:aws:iam::492935017096:user/jinwoong\", edit if required; read https://docs.aws.amazon.com/eks/latest/userguide/connector-grant-access.html for more info\n2022-03-08 22:59:29 [ℹ️]  run kubectl apply -f eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yaml before 11 Mar 22 13:59 UTC to connect the cluster\n```\n\neks-connector.yaml 에는 `Namespace`, `Secret`, `Role`, `ServiceAccount`, `RoleBinding`, `ConfigMap`과 배포될 `StatefulSet` 이 담겨있어 해당 파일은 그대로 사용하면 된다.\n\n하지만 실제 콘솔에서 노드와 워크로드 정보를 보기 위해서 eks-connector-console-dashboard-full-access-group.yaml에 콘솔 접속이 필요한 IAM 사용자와 접근을 위한 특정 네임스페이스 정보를 `ClusterRole` 및 `ClusterRoleBinding`에 추가해야 한다. \n\n자세한 내용은 [https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/connector-grant-access.html](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/connector-grant-access.html)에서 확인이 가능하다. \n\n위에서 생성된 manifests가 EKS Anywhere 클러스터에 적용되면 StatefulSet 형태로 배포된 EKS Connect 에이전트는 Amazon EventBridge를 통해 EKS에 알림을 보내는 System Manager 서비스에 연결한다. 이후 EKS Connect는 연결된 클러스터의 kubernetes API 서버에 EKS 콘솔 요청을 전달하므로 생성한 서비스 어카운트를 EKS Connect 역할과 연결하여 AWS IAM 엔티티를 가장(impersonate) 할 수있는 권한을 부여하게 된다. \n\nstatefulset을 좀더 살펴보면 위에서 이야기하는 에이전트 역할을 하는 `connector-agent` 와 프록시 역할을 하는 `connector-proxy` 컨테이너 두개가 떠있는걸 볼 수 있다. 사용자가 콘솔에서 해당 클러스터의 노드나 워크로드 정보를 조회하게 되면 EKS는 System Manager 서비스에게 세션을 요청하고 이후 System Manager가 비동기 방식으로 `connector-agent` 컨테이너를 invoke하면 `connector-proxy` 컨테이너 위에서 마운트된 서비스 어카운트로 인증을 하고 가장(impersonation)한 상태로 EKS Anywhere 클러스트의 API를 호출해서 관련된 내용을 가져와서 보여주게 된다.\n\n![connector](https://d2908q01vomqb2.cloudfront.net/fe2ef495a1152561572949784c16bf23abb28057/2021/09/15/Screen-Shot-2021-09-05-at-7.51.33-AM.png)\n\n\n```sh\n> kubectl describe statefulset eks-connector -n eks-connector\nName:               eks-connector\nNamespace:          eks-connector\nCreationTimestamp:  Wed, 09 Mar 2022 22:53:23 +0900\nSelector:           app=eks-connector\nLabels:             app=eks-connector\nAnnotations:        <none>\nReplicas:           2 desired | 2 total\nUpdate Strategy:    RollingUpdate\n  Partition:        0\nPods Status:        2 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  ...\n  Containers:\n   connector-agent:\n    Image:      public.ecr.aws/amazon-ssm-agent/amazon-ssm-agent:3.1.90.0\n    Port:       <none>\n    Host Port:  <none>\n    Environment:\n      AWS_EC2_METADATA_DISABLED:  true\n    Mounts:\n      /etc/amazon/ssm/amazon-ssm-agent.json from eks-agent-config (rw,path=\"amazon-ssm-agent.json\")\n      /etc/amazon/ssm/seelog.xml from eks-agent-config (rw,path=\"seelog.xml\")\n      /var/eks/shared from eks-connector-shared (rw)\n      /var/lib/amazon/ssm/Vault from eks-agent-vault (rw)\n   connector-proxy:\n    Image:      public.ecr.aws/eks-connector/eks-connector:0.0.3\n    Port:       <none>\n    Host Port:  <none>\n    Args:\n      server\n    Environment:\n      POD_NAME:   (v1:metadata.name)\n    Mounts:\n      /var/eks/shared from eks-connector-shared (rw)\n      /var/lib/amazon/ssm/Vault from eks-agent-vault (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from service-account-token (rw)\n  ...\n```\n## Istio 환경 구성\n\nIstio 환경에서 앱을 배포하고 Tracing 가시성 확보를 하는 실습을 진행하기 위해 간단하게 구성에 대한 그림을 그려봤다. \n\n![istio](/img/istio_o11y.png)\n\nIstio 환경에서 sidecar 형태로 envoy proxy가 각 Observability 구성요소로 데이터가 전달되도록 구성한다. 이때 사용되는 오픈소스 스택은 다음과 같다.\n\n- Tracing \n  - Collect : [OpenTelemetry Collector](https://github.com/open-telemetry/opentelemetry-collector)\n  - Backend : [Tempo](https://github.com/grafana/tempo)\n\n- Logging\n  - Collect : [Fluentbit](https://github.com/fluent/fluent-bit)\n  - Backend : [Loki](https://github.com/grafana/loki)\n\n- Metrics\n  - Collect : [Exporters](https://istio.io/latest/docs/ops/integrations/prometheus/)\n  - Backend : [Prometheus](https://github.com/prometheus)\n\n- Visualization\n  - [Grafana](https://github.com/grafana/grafana)\n\n### Istio 설치\n\nIstioOperator를 통해 Istio를 설치한다.\n\n```sh\n> istioctl operator init\n```\n\nIstioOperator가 설치되면 리소스를 배포하여 istio 서비스를 생성할 수 있다. envoy proxy에는 다양한 헤더들을 통해 trace 정보를 propagate 할 수 있다. 다음과 같이 `x-b3-traceid`를 traceid로 로그 포맷을 구성하도록 하고 trace 활성화를 위해 `enableTracing: true` 를 설정하고 Prometheus에서 스크랩을 할 수 있도록 `enablePrometheusMerge: true`로 설정한다.\n\n마지막으로 zipkin 프로토콜로 OpenTelemetry Collector로 전달할것이기 엔드포인드 정보도 미리 작성했다.\n\n```yaml\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  name: istio-operator\n  namespace: istio-system\nspec:\n  profile: default\n  meshConfig:\n    accessLogFile: /dev/stdout\n    accessLogFormat: |\n      [%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %RESPONSE_CODE_DETAILS% %CONNECTION_TERMINATION_DETAILS% \"%UPSTREAM_TRANSPORT_FAILURE_REASON%\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME% traceID=%REQ(x-b3-traceid)%\n    enableTracing: true\n    enablePrometheusMerge: true\n    defaultConfig:\n      tracing:\n        sampling: 100\n        max_path_tag_length: 99999\n        zipkin:\n          address: otel-collector.tracing.svc:9411\n```\n\nIstio의 대표적인 샘플앱인 [bookinfo](https://istio.io/latest/docs/examples/bookinfo/)를 `bookinfo` 네임스페이스에 설치한다.\n\n```\n> kubectl create ns bookinfo\n> kubectl label namespace bookinfo istio-injection=enabled --overwrite\n> kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/bookinfo/platform/kube/bookinfo.yaml\n```\n\n서비스 접근을 위한 `Gateway`와 `VirtualService` 도 구성한다. \n\n```yaml\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\n  namespace: bookinfo\nspec:\n  selector:\n    istio: ingressgateway # use istio default controller\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\" # Mind the hosts. This matches all\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\n  namespace: bookinfo\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - tracing/bookinfo-gateway\n  - bookinfo-gateway\n  http:\n  - match:\n    - uri:\n        prefix: \"/\"\n    route:\n    - destination:\n        host: productpage.bookinfo.svc.cluster.local\n        port:\n          number: 9080\n```\n\n서비스 확인을 위해 `LoadBalancer`로 구성된 istio-ingressgateway EXTERNAL-IP인 [http://192.168.31.10/productpage](http://192.168.31.10/productpage)로 접속을 확인할 수 있다. \n\n```\n❯ kubectl get svc -n istio-system\nNAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.97.92.31      192.168.31.10   15021:30184/TCP,80:32738/TCP,443:31058/TCP   2d23h\nistiod                 ClusterIP      10.107.186.201   <none>          15010/TCP,15012/TCP,443/TCP,15014/TCP        2d23h\n```\n\n## metric-server 설치\n\n[https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)\n\nHPA 나 Kubernetes 대시보드를 활용하기 위해서 metric-server를 설치한다. kubelet tls 인증서 에러가 날 경우 설치 파일을 받아서 ARG에 `--kubelet-insecure-tls` 플래그를 추가해서 설치하면 된다.\n\n```\n> kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nserviceaccount/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\nservice/metrics-server created\ndeployment.apps/metrics-server created\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\n\n> kubectl top nodes\nNAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   \n192.168.31.186   175m         9%     2228Mi          29%       \n192.168.31.187   82m          4%     893Mi           11%       \n192.168.31.188   64m          3%     795Mi           10%       \n192.168.31.189   134m         6%     1817Mi          23%       \n192.168.31.190   43m          2%     636Mi           8% \n```\n\n## Observability Stack 설치\n\n차트로 일괄배포를 위해 [Loki-stack](https://github.com/grafana/helm-charts/tree/main/charts/loki-stack)를 사용했고, 테스트를 위한 구성으로 구성 변경을 한 chart values는 다음과 같다. \n\n- [Prometheus](https://github.com/prometheus) - 시계열 데이터(메트릭)을 수집하고 저장하는 프로젝트이며, 키-밸류 형태로 쿼리와 연산이 가능한 모니터링 시스템이다. node-exporter, alertmanager, pushgateway 도 차트로 같이 설치한다. \n- [Grafana](https://github.com/grafana/grafana) - 데이터를 시각화하는 플랫폼으로 다양한 로그, 메트릭, 트레이스 데이터를 시작화하고 알림 관리를 할 수 있다.\n- [Loki](https://github.com/grafana/loki) - 다른 로깅 시스템과 달리 로그를 제외한 메타데이터 인덱싱만 하기 때문에 Prometheus처럼 레이블을 염두에 두고 설계된 로깅 데이터스토어 시스템이다.\n- [Fluent Bit](https://github.com/fluent/fluent-bit) - 로깅 컬렉터(Collector)로 fluentd 보다 경량화되고 빠른 로그 프로세스이자 전달자(Forwarder)로 다른 로깅 컬렉터인 promtail, filebeat, logstash는 false로 설정한다. \n- [Tempo](https://github.com/grafana/tempo) - Tempo는 분산 추적(Tracing)을 위한 비용 효율적인 백엔드 데이터스토어 프로젝트로 위에서 구성한 Prometheus, Grafana, Loki 등과 같은 다른 Observability 프로젝트와 통합이 쉬운것이 특징이다.\n\n### Prometheus, Loki 설치\n\n`Prometheus`와 `Loki`는 Loki-Stack으로 구성한다. fluent-bit는 별도 구성 파일을 필요로 하기 때문에 나중에 설치할 예정이다. Prometheus는 Persistent Volume은 별도로 구성하지 않은 상태로 `persistentVolume`을 `false`로 설정하여 배포한다.\n\n```sh\n> kubectl create ns tracing\n> helm repo add grafana https://grafana.github.io/helm-charts\n> helm repo update\n> helm install loki grafana/loki-stack --version 2.4.1 -n tracing -f - << 'EOF'\nfluent-bit:\n  enabled: false\npromtail:\n  enabled: false\nprometheus:\n  enabled: true\n  server:\n    persistentVolume:\n      enabled: false\n  alertmanager:\n    persistentVolume:\n      enabled: false\nEOF\n```\n\n`Tempo`는 로그 정보와 tracing을 수신할 리시버 프로토콜 들(zipkin, otlp)을 정의한 채로 Chart로 배포한다. \n\n```sh\nhelm install tempo grafana/tempo --version 0.7.4 -n tracing -f - << 'EOF'\ntempo:\n  extraArgs:\n    \"distributor.log-received-traces\": true\n  receivers:\n    zipkin:\n    otlp:\n      protocols:\n        http:\n        grpc:\nEOF\n```\n\n### OpenTelemetry Collector 구성\n\n이제까지 observability 관련 데이터 저장소들을 구성했으니 마지막으로 트레이싱을 위한 도구인 [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/)를 설치한다. \n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n name: otel-collector-conf\n labels:\n   app: opentelemetry\n   component: otel-collector-conf\ndata:\n otel-collector-config: |\n   receivers:   \n     zipkin:\n        endpoint: 0.0.0.0:9411\n   exporters:\n     otlp:\n       endpoint: tempo.tracing.svc.cluster.local:55680\n       insecure: true\n   service:\n     pipelines:\n       traces:\n         receivers: [zipkin]\n         exporters: [otlp]\n```\n\n초기 Collect 구성 컨피그는 다름과 같이 설정했다. 자세한 설정 방법은 [https://opentelemetry.io/docs/collector/configuration/](https://opentelemetry.io/docs/collector/configuration/)에서 확인가능하다.\n\n- `spec.config.receivers.zipkin.endpoint`는 trace정보를 받을 엔드포인트 구성으로 zipkin 포맷으로 수신한다. \n- `spec.config.exporters[]`에서는 Tempo로 보내기 위한 endpoint 설정을 한다.\n- `spec.config.service`에서는 이후 collector에서 사용할 protocol이나 backend datasource 에 대한 설정이며 반드시 활성화가 되어야 사용이 가능하다.  \n\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: otel-collector\n  labels:\n    app: opentelemetry\n    component: otel-collector\nspec:\n  ports:\n  - name: otlp # Default endpoint for OpenTelemetry receiver.\n    port: 55680\n    protocol: TCP\n    targetPort: 55680\n  - name: jaeger-grpc # Default endpoint for Jaeger gRPC receiver\n    port: 14250\n  - name: jaeger-thrift-http # Default endpoint for Jaeger HTTP receiver.\n    port: 14268\n  - name: zipkin # Default endpoint for Zipkin receiver.\n    port: 9411\n  - name: metrics # Default endpoint for querying metrics.\n    port: 8888\n  - name: prometheus # prometheus exporter\n    port: 8889\n  selector:\n    component: otel-collector\n```\n\nService에서 주료 사용하게될 포트는 `zipkin`의 `9411`이다.\n\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otel-collector\n  labels:\n    app: opentelemetry\n    component: otel-collector\nspec:\n  selector:\n    matchLabels:\n      app: opentelemetry\n      component: otel-collector\n  template:\n    metadata:\n      labels:\n        app: opentelemetry\n        component: otel-collector\n    spec:\n      containers:\n      - command:\n          - \"/otelcontribcol\"\n          - \"--config=/conf/otel-collector-config.yaml\"\n          - \"--mem-ballast-size-mib=683\"\n          - \"--log-level=DEBUG\"\n        image: otel/opentelemetry-collector-contrib:0.29.0\n        name: otel-collector\n        ports:\n        - containerPort: 55679 # Default endpoint for ZPages.\n        - containerPort: 55680 # Default endpoint for OpenTelemetry receiver.\n        - containerPort: 14250 # Default endpoint for Jaeger HTTP receiver.\n        - containerPort: 14268 # Default endpoint for Jaeger HTTP receiver.\n        - containerPort: 9411 # Default endpoint for Zipkin receiver.\n        - containerPort: 8888  # Default endpoint for querying metrics.\n        - containerPort: 8889  # prometheus exporter       \n        volumeMounts:\n        - name: otel-collector-config-vol\n          mountPath: /conf\n      volumes:\n        - configMap:\n            name: otel-collector-conf\n            items:\n              - key: otel-collector-config\n                path: otel-collector-config.yaml\n          name: otel-collector-config-vol\n```\n\n\n### Fluentbit 구성\n\n클러스터 dataplane과 controlplane의 logging 데이터를 `Loki` 엔드포인트로 전달하기 위한 config로 확인하고 Chart를 배포한다. \n\n```sh\n> helm repo add fluent https://fluent.github.io/helm-charts\n> helm repo update\n> helm install fluent-bit fluent/fluent-bit --version 0.16.1 -n tracing -f - << 'EOF'\nlogLevel: trace\nconfig:\n  service: |\n    [SERVICE]\n        Flush 1\n        Daemon Off\n        Log_Level trace\n        Parsers_File custom_parsers.conf\n        HTTP_Server On\n        HTTP_Listen 0.0.0.0\n        HTTP_Port {{ .Values.service.port }}\n  inputs: |\n    [INPUT]\n        Name tail\n        Path /var/log/containers/*.log\n        Parser cri\n        Tag kube.*\n        Mem_Buf_Limit 5MB\n  outputs: |\n    [OUTPUT]\n        name loki\n        match *\n        host loki.tracing.svc\n        port 3100\n        tenant_id \"\"\n        labels job=fluentbit\n        label_keys $trace_id\n        auto_kubernetes_labels on\n  customParsers: |\n    [PARSER]\n        Name cri\n        Format regex\n        Regex ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<message>.*)$\n        Time_Key    time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L%z\nEOF\n```\n\n마지막으로 위 3가지 데이터소스(Prometheus, Loki, Tempo)가 등록된 상태의 `Grafana` Chart를 배포한다. \n\n```sh\n> helm install grafana grafana/grafana -n tracing --version 6.13.5  -f - << 'EOF'\ndatasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n      - name: Tempo\n        type: tempo\n        access: browser\n        orgId: 1\n        uid: tempo\n        url: http://tempo.tracing.svc:3100\n        isDefault: true\n        editable: true\n      - name: Prometehus\n        type: prometheus\n        access: browser\n        orgId: 1\n        uid: prometheus\n        url: http://loki-prometheus-server.tracing.svc\n        isDefault: false\n        editable: true\n      - name: Loki\n        type: loki\n        access: browser\n        orgId: 1\n        uid: loki\n        url: http://loki.tracing.svc:3100\n        isDefault: false\n        editable: true\n        jsonData:\n          derivedFields:\n            - datasourceName: Tempo\n              matcherRegex: \"traceID=(\\\\w+)\"\n              name: TraceID\n              url: \"$${__value.raw}\"\n              datasourceUid: tempo\nenv:\n  JAEGER_AGENT_PORT: 6831\nadminUser: admin\nadminPassword: password\nservice:\n  type: LoadBalancer\nEOF\n```\n\n### bookinfo productpage 접속 및 trace 확인\n\n`LoadBalancer`로 구성된 istio-ingressgateway EXTERNAL-IP인 [http://192.168.31.10/productpage](http://192.168.31.10/productpage)로 접속하는 상태에서 trace를 확인할 수 있다.\n\n```sh\n> ISTIOINGRESSGW=$(kubectl get pod -n istio-system -l app=istio-ingressgateway -o jsonpath='{.items[0].status.hostIP}')\necho -e \"PRODUCTPAGE UI URL = http://$ISTIOINGRESSGW/productpage\"\nPRODUCTPAGE UI URL = http://192.168.31.187/productpage\n```\n\n```sh\n> for i in {1..100};  do curl -s $ISTIOINGRESSGW/productpage | grep -o \"<title>.*</title>\" ; done\n```\n\nGrafana Explorer 메뉴에서 `Loki` 데이터소스를 선택하고 Log browser에 `{app=\"productpage\"}`를 입력하게 되면 하나의 트랜잭션에 대한 트레이스 정보를 로그내에서 확인하고 해당 traceid 버튼을 클릭하거나 `Tempo` 메뉴에서 탐색을 하면 관련된 trace 정보를 확인할 수 있다.\n\n![tempo](/img/tempo-grafana.png)\n\n## 정리 \n이번 포스팅에서는 eks-anywhere 클러스터에 여러가지 3rd party 구성요소들을 설치하고 실제 운영환경과 유사하게 테스트를 진행해봤다. EKS를 사용하는 관점에서 유사한 경험을 바탕으로 개발환경을 가져갈수 있기에 vSphere를 기반으로 사내 서비스를 개발하는 팀에게는 좋은 하나의 선택지가 추가되었다고 볼 수 있다. Direct Connect나 VPN과 같이 네트워크 연결성만 확보된다면 실제 기업환경에서는 AWS 네이티브 서비스와의 연계가 더 용이해지기 때문에 좋은 개발, 테스트 환경으로 다가갈 수 있을거라 생각한다.  \n\n재미있는 경험이었고 당분간은 개인프로젝트를 진행하는 홈랩 환경은 EKS Anywhere로 정착해서 테스트를 계속 진행하려고 한다. \n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/eks-anywhere/",
      "metadata": {
        "permalink": "/kubernetes/eks-anywhere/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2022-03-06-eks-anywhere.md",
        "source": "@site/blog/2022-03-06-eks-anywhere.md",
        "title": "EKS Anywhere on vSphere Homelab",
        "description": "vSphere homelab 환경에서 EKS Anywhere 구성하기",
        "date": "2022-03-06T00:00:00.000Z",
        "formattedDate": "March 6, 2022",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "vsphere",
            "permalink": "/tags/vsphere"
          },
          {
            "label": "eks anywhere",
            "permalink": "/tags/eks-anywhere"
          },
          {
            "label": "eks",
            "permalink": "/tags/eks"
          },
          {
            "label": "flux",
            "permalink": "/tags/flux"
          },
          {
            "label": "gitops",
            "permalink": "/tags/gitops"
          }
        ],
        "readingTime": 30.095,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "EKS Anywhere on vSphere Homelab",
          "comments": true,
          "classes": "wide",
          "description": "vSphere homelab 환경에서 EKS Anywhere 구성하기",
          "toc": true,
          "toc_label": "Table of Contents",
          "slug": "kubernetes/eks-anywhere/",
          "date": "2022-03-06T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "vsphere",
            "eks anywhere",
            "eks",
            "flux",
            "gitops",
            "eks-anywhere"
          ]
        },
        "prevItem": {
          "title": "EKS Anywhere Advanced Usages",
          "permalink": "/kubernetes/eks-anywhere-adv/"
        },
        "nextItem": {
          "title": "What is Tailscale?",
          "permalink": "/kubernetes/what-is-tailscale/"
        }
      },
      "content": "## EKS Anywhere\n\n`EKS Anywhere`는 eksctl를 사용하는 환경에서 [Kubernetes Cluster API Provider vSphere](https://github.com/kubernetes-sigs/cluster-api-provider-vsphere) 인 CAPV를 통해 vSphere 기반으로 쿠버네티스 클러스터를 구성해주는 오픈소스이다. CAPV Spec은 [Kubernetes Cluster API](https://cluster-api.sigs.k8s.io/) 기반으로 쿠버네티스 스타일 API형태의 선언적(Declarative) 방식으로 쿠버네티스 클러스터의 Lifecycle을 관리하는 프로젝트이다. \n\nUbuntu 또는 Mac admin 환경에서 eksctl 명령어로 로컬 클러스터들을 관리하고 클러스터를 생성, 삭제할 수 있다. 일단 기본적으로 클러스터를 생성하고 관리하는 방식이 eksctl과 거의 유사하다. 또한 EKS 서비스의 배포 버전과 동일한 버전을 사용함으로써 (포스팅 당시 버전 1.21) vSphere 기반 하이브리드 환경에서의 쿠버네티스 클러스터 및 워크로드 관리를 편하게 할 수 있다.\n\n<!--truncate-->\n\n크게 EKS native 서비스와 크게 다른점 몇가지는 다음과 같다. \n\n- 컴퓨팅 환경 : vSphere, Docker(개발환경 구성시)\n- 노드 지원 OS :  BottleRocket, Ubuntu\n- CNI : Cilium\n\n자세한 내용은 [비교표](https://anywhere.eks.amazonaws.com/docs/concepts/eksafeatures/)를 참고하자.\n\n## 사전 준비사항 \n### 로컬 관리 환경\n- Docker 20.x 이상 버전  \n- Mac OS(10.15) / Ubuntu(20.04.2 LTS)\n- CPU 4코어\n- 메모리 16GB\n- 디스크 30GB\n\n### vSphere\n[https://anywhere.eks.amazonaws.com/docs/reference/vsphere/vsphere-prereq/](https://anywhere.eks.amazonaws.com/docs/reference/vsphere/vsphere-prereq/)\n\n- vCenter 환경의 vSphere 7.0 이상\n- 최소 VM 4대 (각 2 vCPU, 8GB 메모리, 25GB 디스크 이상 권장)\n  - Control Plane 1대\n  - etcd 노드 1대\n  - worker 노드 2대\n- DHCP 환경\n- API 서버 용 Static IP 1개\n- 외부 바이너리, 이미지 다운로드를 위한 네트워크 환경\n  - 클러스터 자체에서 방화벽 등으로 인한 인터넷이 안되는 경우 [프록시 구성](https://anywhere.eks.amazonaws.com/docs/reference/clusterspec/proxy/)을 통해 가능함\n\n## Homelab 구성환경\n\n기존에 운영하던 ESXi 환경을 그대로 활용하도록 했다. 추가적으로 하드웨어를 구비하기 위해 비용이 발생한부분은 없다.   \n\n### Hardware\n- Dell Precision Tower 3620\n  - Intel(R) Xeon(R) CPU E3-1240L v5\n  - TeamGroup DDR4-2666 16GB * 4ea\n  - (VM 용) SAMSUNG NVMe SSD PM981 - 1TB\n  - (ESXi 용) Sandisk SSD Z410- 240GB\n\n### Software\n- vSphere 7.0 Update 3\n- vCenter 7.0.3 (VSCA)\n- Docker Desktop [4.2.0](https://docs.docker.com/desktop/mac/release-notes/#docker-desktop-420)버전을 사용 \n  - 4.3.0 버전 하위 호환 문제가있어 금번 테스트 환경에서는 4.2.0 사용\n  - 도커 리소스를 최소 `8vCPU`, `16GB`로 설정\n- [kubectl](https://kubernetes.io/ko/docs/tasks/tools/) v1.23.4\n- [eksctl](https://eksctl.io/introduction/#installation) v0.85.0\n- [eksctl-anywhere](https://anywhere.eks.amazonaws.com/docs/getting-started/install/) v0.7.0\n- [aws-iam-authenticator](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/install-aws-iam-authenticator.html) v0.5.5\n- [bottlerocket OVA](https://anywhere.eks.amazonaws.com/docs/reference/artifacts/) v1.21 (자동 생성되는 OVA로 별도 구성이 필요없다)  \n\n## 클러스터 환경구성\n\n[공식 홈페이지의 클러스터 생성, 삭제 워크플로우](https://anywhere.eks.amazonaws.com/docs/concepts/clusterworkflow/)를 참고하여 사전 구성된 Homelab에 적용한다. \n\n![workflow](https://anywhere.eks.amazonaws.com/images/eks-a_create_cluster.png)\n\n처음에는 eksctl 명령으로 클러스터를 생성하는 과정으로 사전에 생성한 cluster config yaml을 통해 로컬 도커 엔진에 bootstrap을 하는 명령을 내린다. 이때 로컬 도커 엔진에서는 Docker in Docker 엔진인 [kind](https://kind.sigs.k8s.io/)를 통해 클러스터 번들 컨피그를 생성한다. \n\n[clusterctl](https://github.com/kubernetes-sigs/cluster-api/tree/main/cmd/clusterctl)를 통해 [Cluster API Provider Docker](https://github.com/kubernetes-sigs/cluster-api/tree/main/test/infrastructure/docker)형태로 부모(bootstrap) 클러스터를 만드는 과정이 `eksctl anywhere` 명령으로 추상화 되는것이라 볼 수 있다. \n\n해당 단계에서 먼저 Bastion Host나 관리 머신에 `kubectl`, `eksctl`, `eksctl-anywhere`, `aws-iam-authenticator` 등이 설치되어 있는지 확인한다.\n\nhttps://anywhere.eks.amazonaws.com/docs/getting-started/install/\n\n`homelab` 이름으로 클러스터 컨피그를 생성한다.\n\n```sh\nCLUSTER_NAME=homelab\neksctl anywhere generate clusterconfig $CLUSTER_NAME \\\n   --provider vsphere > $CLUSTER_NAME.yaml\n```\n\n모든 컨피그에 대한 상세 설정은 [vSphere configuration](https://anywhere.eks.amazonaws.com/docs/reference/clusterspec/vsphere/) 에서 확인할 수 있다. \n\n생성된 컨피그를 하나씩 살펴보자. \n\n```yaml\napiVersion: anywhere.eks.amazonaws.com/v1alpha1\nkind: Cluster\nmetadata:\n  name: homelab\n  namespace: default\nspec:\n  clusterNetwork:\n    cni: cilium\n    pods:\n      cidrBlocks:\n      - 192.168.0.0/20\n    services:\n      cidrBlocks:\n      - 10.96.0.0/12\n  controlPlaneConfiguration:\n    count: 2\n    endpoint:\n      host: 192.168.31.3\n    machineGroupRef:\n      kind: VSphereMachineConfig\n      name: homelab-cp\n  datacenterRef:\n    kind: VSphereDatacenterConfig\n    name: homelab\n  externalEtcdConfiguration:\n    count: 1\n    machineGroupRef:\n      kind: VSphereMachineConfig\n      name: homelab-etcd\n  gitOpsRef:\n    kind: GitOpsConfig\n    name: cluster-gitops\n  kubernetesVersion: \"1.21\"\n  managementCluster:\n    name: homelab\n  workerNodeGroupConfigurations:\n  - count: 2\n    machineGroupRef:\n      kind: VSphereMachineConfig\n      name: homelab\n    name: md-0\n```\n\n첫번째는 `Cluster` 리소스로 Cluster의 이름과 클러스터 전반 설정과 Control Plane, Data Plane, etcd VM의 버전, 개수 등을 정할 수 있다. \n\n클러스터 이름은 `homelab`으로 설정한다. 기본적으로 CNI는 Cilium을 사용하는 구조이기 때문에 필수적으로 `cilium`로 설정해야 한다. 그리고 kubeadm 구성시 필요한 `pods.cidrBlocks`과 `services.cidrBlocks`에서 파드와 서비스가 사용할 네트워크 대역을 설정한다. 둘다 CNI에서 통신되는 가상 네트워크 대역이기 때문에 현재 사용중인 샤오미 공유기 대역(192.168.31.x)과 충돌하지 않으면서 라우팅에 문제되지 않고 현재 네트워크에 사용되지 않는 대역(192.168.0.0/20, 10.96.0.0/12) 으로 설정했다.  \n\n> DHCP 유의사항: 본인의 경우 Xiaomi 공유기를 사용중인데 별도의 DHCP서버를 구성하지 않고 사용을 하고 있어서인지, 신규 노드를 위한 VM이 생성되고 `bottlerocket` OS가 기동될때 호스트명이 `MiWifi-R3600-SRV` 이런식으로 대소문자를 포함한 형태로 자동 생성되다 보니 부팅이 되지 않는 현상이 발생했다. 이에 [별도의 DHCP 구성](https://anywhere.eks.amazonaws.com/docs/reference/vsphere/vsphere-dhcp/)을 통해 진행을 하면 해당 이슈를 피할수 있다. \n\n`controlPlaneConfiguration` 필드에서는 controlPlane 노드의 `count`와 `endpoint`를 설정해야 한다. `count` 초기 값은 `2`로 되어 있지만 홈랩의 자원이 부족하기 때문에 `1`로 설정했다. 실제 프로덕션에서는 2이상을 권고한다. 그리고 `endpoint.host`는 쿠버네티스 API서버 엔드포인트로 실제 kubectl 명령 및 사용자와 인터페이스를 위한 접근 가능한 DHCP 할당으로 충돌되지 않는 네트워크 대역의 고정 IP로 설정한다. `machineGroupRef`는 이후 생성할 VM spec에 매칭시킬 label 값으로 자동 생성된다. \n\n`externalEtcdConfiguration`는 etcd 구성을 위한 필드로 `count`는 기본설정 값이 `3`이지만 홈랩 환경에서는 `1`로 구성하였다. 실제 프로덕션에서는 Raft 알고리즘 기반 Redundancy 구성을 위해 `3`이상을 권고한다.\n\n`gitOpsRef`는 GitOps 형태로 클러스터를 관리하기 위해 참조하는 값으로 아래 `GitOpsConfig` 에서 사용할 이름 `cluster-gitops` 를 명시한다. \n\n`kubernetesVersion`은 EKS 배포 버전와 동일한 `1.21`로 설정한다. \n\n`workerNodeGroupConfigurations`은 워커노드 구성으로 `count`를 `2`로 설정하였다. \n\n다음은 배포되는 vSphere spec을 정의하는 리소스로 `VSphereDatacenterConfig`이다.\n\n```yaml\napiVersion: anywhere.eks.amazonaws.com/v1alpha1\nkind: VSphereDatacenterConfig\nmetadata:\n  name: homelab\nspec:\n  datacenter: \"Datacenter\"\n  insecure: false\n  network: \"VM Network\"\n  server: \"192.168.31.2\"\n  thumbprint: \"**:89:91:5B:02:50:F6:41:D0:DE:6B:A0:B8:43:41:A4:81:03:**\"\n```\n\n`spec`에서는 실제 vCenter의 엔드포인트, datacenter, network 정보 등을 입력한다. \n> 유의사항 : `insecure` 옵션의 경우 기본값이 false로 되어 있다. 해당 값이 `false`의 경우 아래 그림에 있는 것 처럼 보통 vCenter 메뉴에서 `thumbprint` 값을 확인할 수 있고 해당 값을 `**:89:91:5B:02:50:F6:41:D0:DE:6B:A0:B8:43:41:A4:81:03:**` 형태로 변경하여 사용하거나 [govc](https://github.com/vmware/govmomi/tree/master/govc)를 사용하여 `govc about.cert -thumbprint -k` 명령어로 확인이 가능하다.\n\n![cert](/img/vsphere_cert.png)\n\n마지막은 각 노드 역할(control plane, etcd, worker)별로 사용할 OS 및 리소스 spec을 정의하는 컨피그이다. 위 `Cluster` 리소스에서 설정된 `machineGroupRef.name`으로 매칭되는 값을 metadata로 자동 생성 해주고 모두 유사하기 때문에 이중 `VSphereMachineConfig`만 살펴본다. \n\n```yaml\napiVersion: anywhere.eks.amazonaws.com/v1alpha1\nkind: VSphereMachineConfig\nmetadata:\n  name: homelab-cp\nspec:\n  datastore: \"datastore2\"\n  diskGiB: 25\n  folder: \"cp\"\n  memoryMiB: 8192\n  numCPUs: 2\n  osFamily: bottlerocket\n  resourcePool: \"*/Resources\"\n  users:\n  - name: ec2-user\n    sshAuthorizedKeys:\n    - ssh-rsa AAAAB3******U= vsphere\n\n```\n\nDatacenter에 구성한 `datastore`중 설치되는 VM 영역이 사용할 대상 datastore를 입력한다. VM디스크 `diskGiB`, Datacenter `folder`, VM메모리 `memoryMiB`, vCPU `numCPUs`를 입력하고, `resourcePool`은 지정 리소스 그룹을 별도로 설정하지 않았기 때문에 `\"*/Resources\"`로 설정한다. 그리고 `osFamily`는 ubuntu, bottlerocket두가지로 제공이 되는데 기본값은 bottlerocket이고, 해당 이미지의 기본 user는 `ec2-user`로 설정하고 해당 VM에 SSH접근을 위해서 별도로 생성한 `sshAuthorizedKeys`를 입력한다. \n\n```yaml\napiVersion: anywhere.eks.amazonaws.com/v1alpha1\nkind: GitOpsConfig\nmetadata:\n  name: cluster-gitops\n  namespace: default\nspec:\n  flux:\n    github:\n      branch: main\n      clusterConfigPath: clusters/homelab\n      fluxSystemNamespace: flux-system\n      owner: ddiiwoong\n      personal: true\n      repository: eks-anywhere-homelab\n```\n\n마지막은 클러스터를 GitOps 스타일로 관리하기 위해 `GitOpsConfig`를 추가한다. spec 에서 사용할 github의 `branch`, github 레포 아래 컨피그가 저장될 경로인 `clusterConfigPath`, gitops 오픈소스인 [Flux](https://fluxcd.io/docs/get-started/)가 구성될 네임스페이스인 `fluxSystemNamespace`, github account인 `owner`, repo 이름이 되는 `repository` 까지 설정한다. 해당 구성을 포함한 채로 클러스터를 배포하게 되면 클러스터가 생성되는 중에 선언한 `repository`로 이후 관리될 클러스터 컨피그 및 kustomize 파일 등을 `commit`하게 된다. \n\n## 클러스터 생성\n\n나머지 etcd, worker node 도 `VSphereMachineConfig`를 동일한 형식으로 작성하고, vSphere 관련 인증정보와 gitops로 활용할 레포의 github token을 환경변수로 입력하여 클러스터 생성을 진행한다. 홈랩으로 구성한 샘플 컨피그는 [링크](https://raw.githubusercontent.com/ddiiwoong/eks-anywhere-homelab/main/clusters/homelab/homelab/eksa-system/eksa-cluster.yaml)에서 확인할 수 있다. \n\n```sh\n> export EKSA_VSPHERE_USERNAME='administrator@ddii.local'\n> export EKSA_VSPHERE_PASSWORD='***********'\n> export EKSA_GITHUB_TOKEN=ghp_************************\n> eksctl anywhere create cluster -f homelab.yaml\nWarning: The recommended size of an external etcd cluster is 3 or 5\nChecking Github Access Token permissions\n✅ Github personal access token has the required repo permissions\nPerforming setup and validations\n✅ Connected to server\n✅ Authenticated to vSphere\n✅ Datacenter validated\n✅ Network validated\n✅ Datastore validated\n✅ Folder validated\n✅ Resource pool validated\n✅ Datastore validated\n✅ Folder validated\n✅ Resource pool validated\n✅ Datastore validated\n✅ Folder validated\n✅ Resource pool validated\n✅ Control plane and Workload templates validated\n✅ Vsphere Provider setup is valid\n✅ Flux path\n✅ Create preflight validations pass\nCreating new bootstrap cluster\nInstalling cluster-api providers on bootstrap cluster\nProvider specific setup\nCreating new workload cluster\nInstalling networking on workload cluster\nInstalling storage class on workload cluster\nInstalling cluster-api providers on workload cluster\nInstalling EKS-A secrets on workload cluster\nMoving cluster management from bootstrap to workload cluster\nInstalling EKS-A custom components (CRD and controller) on workload cluster\nCreating EKS-A CRDs instances on workload cluster\nInstalling AddonManager and GitOps Toolkit on workload cluster\nAdding cluster configuration files to Git\nEnumerating objects: 20, done.\nCounting objects: 100% (20/20), done.\nCompressing objects: 100% (12/12), done.\nTotal 20 (delta 1), reused 19 (delta 0), pack-reused 0\nFinalized commit and committed to local repository\t{\"hash\": \"ea4293c849725e593704c2a8ff71a708ad8cf8f2\"}\nWriting cluster config file\nDeleting bootstrap cluster\n🎉 Cluster created!\n```\n\n각 단계를 순서대로 간단히 살펴보면 처음 bootstrap cluster 구성 단계에서는 아래와 같은 일을 수행한다.\n- Github Access Token 권한 확인\n- vSphere 인증\n- vSphere 리소스(Datacenter, Network, Datastore, Folder 등) 검증\n- Control Plane, Workload Plane 템플릿 검증\n- kind로 bootstrap cluster 생성\n- cluster-api provider 역할을 하는 CAPI(Cluster API) 설치\n\n다음은 생성된 bootstrap cluster의 CAPI 관리도구가 vSphere 클러스터에 연결후에 진행한다.\n\n- [govc](https://github.com/vmware/govmomi/tree/master/govc)를 사용해서 연결된 vSphere 클러스터에 새로운 etcd를 포함한 Control, Workload 노드 생성\n- [Cilium](https://cilium.io/) CNI 추가\n- Storage Class 추가\n- 구성된 Workload Cluster에 CAPI 추가\n  - provider 셋업\n  - 추가적인 CRD 및 GitOps 컴포넌트 추가\n- GitOps를 위해 최초 생성된 cluster config를 지정한 repo에 Commit \n- 마지막으로 로컬 관리 머신에 cluster config를 저장\n\n\n각각의 단계에 대한 설명은 [다음 링크](https://anywhere.eks.amazonaws.com/docs/concepts/clusterworkflow/#4-authenticate-and-create-bootstrap-cluster)에서 확인할 수 있다. \n\n처음 몇번 시도를 하다보면 여러가지 에러사항을 만나게 된다. 해당 단계의 디버그 레벨 출력을 보기 위해서는 `-v 9` 플래그를 추가해서 설치를 진행하면 상세 내역을 확인할 수 있는데 debug 레벨로 마지막 몇줄 로그를  살펴보면 도커에서 kind로 도커내에 클러스터를 구성한 이후에 docker내 kind 클러스터를 삭제하는 것을 확인할 수 있다. \n\n```sh\n2022-03-04T15:23:33.638+0900    V4      Deleting kind cluster   {\"name\": \"homelab-eks-a-cluster\"}\n2022-03-04T15:23:33.638+0900    V6      Executing command       {\"cmd\": \"/usr/local/bin/docker exec -i eksa_1646374542969789000 kind delete cluster --name homelab-eks-a-cluster\"}\nDeleting cluster \"homelab-eks-a-cluster\" ...\n2022-03-04T15:23:36.244+0900    V0      🎉 Cluster upgraded!\n2022-03-04T15:23:36.244+0900    V4      Task finished   {\"task_name\": \"delete-kind-cluster\", \"duration\": \"4.560798797s\"}\n2022-03-04T15:23:36.244+0900    V4      ----------------------------------\n2022-03-04T15:23:36.244+0900    V4      Tasks completed {\"duration\": \"7m50.030420843s\"}\n2022-03-04T15:23:37.227+0900    V3      Cleaning up long running container      {\"name\": \"eksa_1646374542969789000\"}\n2022-03-04T15:23:37.228+0900    V6      Executing command       {\"cmd\": \"/usr/local/bin/docker rm -f -v eksa_1646374542969789000\"}\n```\n\n몇가지 경험한 에러 사항을 나열해 보면 위에서도 언급한 내용과 같이 Cluster 컨피그 내 `vSphere thumbprint`를 반드시 넣어주는것이 좋다. 공식 가이드에는 `insecure` 필드를 true로 설정하고 `thumbprint`를 null로 진행할 경우 아래와 같이 kubeconfg secret을 구성단계에서 실패하거나 Workload 노드가 join할 때 에러가 발생하게 된다.  \n\n```sh\n$ eksctl anywhere create cluster -f eksa-mgmt-cluster.yaml\n...\ncollecting management cluster diagnostics\n⏳ Collecting support bundle from cluster, this can take a while\t{\"cluster\": \"bootstrap-cluster\", \"bundle\": \"mgmt/generated/bootstrap-cluster-2022-03-02T23:25:32+09:00-bundle.yaml\", \"since\": 1646227532274217000, \"kubeconfig\": \"mgmt/generated/mgmt.kind.kubeconfig\"}\nSupport bundle archive created\t{\"path\": \"support-bundle-2022-03-02T14_25_34.tar.gz\"}\nAnalyzing support bundle\t{\"bundle\": \"mgmt/generated/bootstrap-cluster-2022-03-02T23:25:32+09:00-bundle.yaml\", \"archive\": \"support-bundle-2022-03-02T14_25_34.tar.gz\"}\nAnalysis output generated\t{\"path\": \"mgmt/generated/bootstrap-cluster-2022-03-02T23:27:23+09:00-analysis.yaml\"}\ncollecting workload cluster diagnostics\nError: failed to create cluster: error checking availability of kubeconfig secret: kubeconfig secret does not exist\n```\n\n정상적으로 클러스터 배포가 되었으면 eksctl config yaml이 포함된 디렉토리 하위에 클러스터 이름으로 새로운 디렉토리와 `kubeconfig`와 실제 배포된 cluster config를 확인 할 수 있다. \n\n```\n> tree\n.\n├── homelab\n│   ├── homelab-eks-a-cluster.kubeconfig\n│   └── homelab-eks-a-cluster.yaml\n├── homelab.yaml\n```\n\n기존 kubeconfig와 merge를 위해 [krew](https://github.com/kubernetes-sigs/krew)를 통해 설치한  [konfig](https://github.com/corneliusweig/konfig) 도구를 사용해서 기존 컨피그와 병합한다. \n\n```sh\n> kubectl konfig merge ~/.kube/config ./homelab/homelab-eks-a-cluster.kubeconfig > merged-and-flattened-config\n> mv merged-and-flattened-config ~/.kube/config\n```\n\n생성된 리소스를 확인해보면 `Bottlerocket` OS 이미지와 `containerd://1.5.8+bottlerocket` CRI를 확인할 수 있다. 노드 이름이 IP로 보이는 이유는 DHCP서버를 별도로 두지 않아 Prefix 할당이 되지 않는 가정용 공유기 환경에서 발생하는 현상이다. \n\n```sh\n> kubectl get node -o wide\nNAME             STATUS   ROLES                  AGE     VERSION   INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                                  KERNEL-VERSION   CONTAINER-RUNTIME\n192.168.31.186   Ready    control-plane,master   9m     v1.21.6   192.168.31.186   192.168.31.186   Bottlerocket OS 1.5.3 (vmware-k8s-1.21)   5.10.93          containerd://1.5.8+bottlerocket\n192.168.31.187   Ready    <none>                 8m     v1.21.6   192.168.31.187   192.168.31.187   Bottlerocket OS 1.5.3 (vmware-k8s-1.21)   5.10.93          containerd://1.5.8+bottlerocket\n192.168.31.188   Ready    <none>                 7m     v1.21.6   192.168.31.188   192.168.31.188   Bottlerocket OS 1.5.3 (vmware-k8s-1.21)   5.10.93          containerd://1.5.8+bottlerocket\n192.168.31.189   Ready    control-plane,master   7m     v1.21.6   192.168.31.189   192.168.31.189   Bottlerocket OS 1.5.3 (vmware-k8s-1.21)   5.10.93          containerd://1.5.8+bottlerocket\n```\n\nCRD 리스트를 확인해보자. 많은 CRD중에 `x-k8s.io`로 필터를 하면 `machine`, `vsphere`, `cluster` 관련된 정보를 확인할수 있다.\n\n```sh\n> kubectl get crd | grep \"x-k8s.io\"\nclusterclasses.cluster.x-k8s.io                              2022-03-04T14:30:28Z\nclusterresourcesetbindings.addons.cluster.x-k8s.io           2022-03-04T14:30:28Z\nclusterresourcesets.addons.cluster.x-k8s.io                  2022-03-04T14:30:28Z\nclusters.cluster.x-k8s.io                                    2022-03-04T14:30:28Z\netcdadmclusters.etcdcluster.cluster.x-k8s.io                 2022-03-04T14:30:41Z\netcdadmconfigs.bootstrap.cluster.x-k8s.io                    2022-03-04T14:30:38Z\nkubeadmconfigs.bootstrap.cluster.x-k8s.io                    2022-03-04T14:30:34Z\nkubeadmconfigtemplates.bootstrap.cluster.x-k8s.io            2022-03-04T14:30:34Z\nkubeadmcontrolplanes.controlplane.cluster.x-k8s.io           2022-03-04T14:30:45Z\nkubeadmcontrolplanetemplates.controlplane.cluster.x-k8s.io   2022-03-04T14:30:47Z\nmachinedeployments.cluster.x-k8s.io                          2022-03-04T14:30:29Z\nmachinehealthchecks.cluster.x-k8s.io                         2022-03-04T14:30:29Z\nmachinepools.cluster.x-k8s.io                                2022-03-04T14:30:30Z\nmachines.cluster.x-k8s.io                                    2022-03-04T14:30:30Z\nmachinesets.cluster.x-k8s.io                                 2022-03-04T14:30:30Z\nproviders.clusterctl.cluster.x-k8s.io                        2022-03-04T14:27:56Z\nvsphereclusteridentities.infrastructure.cluster.x-k8s.io     2022-03-04T14:30:51Z\nvsphereclusters.infrastructure.cluster.x-k8s.io              2022-03-04T14:30:52Z\nvsphereclustertemplates.infrastructure.cluster.x-k8s.io      2022-03-04T14:30:52Z\nvspheredeploymentzones.infrastructure.cluster.x-k8s.io       2022-03-04T14:30:52Z\nvspherefailuredomains.infrastructure.cluster.x-k8s.io        2022-03-04T14:30:53Z\nvspheremachines.infrastructure.cluster.x-k8s.io              2022-03-04T14:30:53Z\nvspheremachinetemplates.infrastructure.cluster.x-k8s.io      2022-03-04T14:30:53Z\nvspherevms.infrastructure.cluster.x-k8s.io                   2022-03-04T14:30:54Z\n```\n\n이중 `machine` CRD를 확인해보면 etcd를 볼 수 있는데 이는 VM인스턴스 형태의 외부 리소스 형태로 배포되기 때문에 쿠버네티스 노드 정보에서는 확인할수 없던 `etcd` 머신을 확인을 할 수 있다. \n\n```\n> kubectl get machine -A\nNAMESPACE     NAME                            CLUSTER   NODENAME         PROVIDERID                                       PHASE     AGE     VERSION\neksa-system   homelab-etcd-jrhvt              homelab                    vsphere://421b44b1-6017-8469-734b-01bcf68cb459   Running   11m     \neksa-system   homelab-j5mql                   homelab   192.168.31.186   vsphere://421b4382-0727-3afc-5c47-b73455010d35   Running   11m     v1.21.5-eks-1-21-8\neksa-system   homelab-md-0-76478bb486-jszj4   homelab   192.168.31.187   vsphere://421b4dc9-b7c6-f0a2-0b50-e5379748b9a9   Running   11m     v1.21.5-eks-1-21-8\neksa-system   homelab-md-0-76478bb486-rpxlm   homelab   192.168.31.188   vsphere://421b20a4-9870-cd20-4cff-67bb4a9d8372   Running   11m     v1.21.5-eks-1-21-8\neksa-system   homelab-w6bq8                   homelab   192.168.31.189   vsphere://421befe9-e26a-3f18-9ccf-3fe32dd57fd2   Running   11m     v1.21.5-eks-1-21-8\n```\n\n초기 구성 파드들을 살펴보면 CAPI 관련 리소스, cert-manager, eks-anywhere, Cilium CNI, vSphere 관련 컨트롤러 등을 확인할 수 있다. \n\n```\n> kubectl get pod -A\nNAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE\ncapi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-694cc79bb7-2h29c       1/1     Running   0          3h42m\ncapi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5b6b48dd8c-g6md4   1/1     Running   0          3h42m\ncapi-system                         capi-controller-manager-689cd9b4fd-vxpc8                         1/1     Running   0          3h42m\ncapv-system                         capv-controller-manager-6b467446b9-n865b                         1/1     Running   0          3h42m\ncert-manager                        cert-manager-7988d4fb6c-72fw7                                    1/1     Running   0          3h44m\ncert-manager                        cert-manager-cainjector-6bc8dcdb64-7d7qg                         1/1     Running   0          3h44m\ncert-manager                        cert-manager-webhook-68979bfb95-hjq8m                            1/1     Running   0          3h44m\neksa-system                         eksa-controller-manager-5c74596687-45v8t                         2/2     Running   0          3h41m\netcdadm-bootstrap-provider-system   etcdadm-bootstrap-provider-controller-manager-74c86ffb56-cpw9k   1/1     Running   0          3h42m\netcdadm-controller-system           etcdadm-controller-controller-manager-7894945688-js7z4           1/1     Running   0          3h42m\nkube-system                         cilium-bq5gq                                                     1/1     Running   0          3h44m\nkube-system                         cilium-fn2f7                                                     1/1     Running   0          3h44m\nkube-system                         cilium-lgptt                                                     1/1     Running   0          3h44m\nkube-system                         cilium-operator-86d59d5c88-76z2t                                 1/1     Running   1          3h44m\nkube-system                         cilium-operator-86d59d5c88-9p4s9                                 1/1     Running   0          3h44m\nkube-system                         cilium-w55nc                                                     1/1     Running   0          68m\nkube-system                         cilium-wk89j                                                     1/1     Running   0          68m\nkube-system                         coredns-745c7986c7-k729b                                         1/1     Running   0          3h46m\nkube-system                         coredns-745c7986c7-thtbm                                         1/1     Running   0          3h46m\nkube-system                         kube-apiserver-192.168.31.189                                    1/1     Running   0          3h46m\nkube-system                         kube-controller-manager-192.168.31.189                           1/1     Running   0          3h46m\nkube-system                         kube-proxy-6xcdk                                                 1/1     Running   0          3h44m\nkube-system                         kube-proxy-d4qbv                                                 1/1     Running   0          68m\nkube-system                         kube-proxy-mhzb4                                                 1/1     Running   0          3h46m\nkube-system                         kube-proxy-vrw2p                                                 1/1     Running   0          68m\nkube-system                         kube-proxy-xkfmh                                                 1/1     Running   0          3h44m\nkube-system                         kube-scheduler-192.168.31.189                                    1/1     Running   0          3h46m\nkube-system                         kube-vip-192.168.31.189                                          1/1     Running   0          3h46m\nkube-system                         vsphere-cloud-controller-manager-bxp4t                           1/1     Running   2          68m\nkube-system                         vsphere-cloud-controller-manager-jdncv                           1/1     Running   3          3h44m\nkube-system                         vsphere-cloud-controller-manager-jhbbp                           1/1     Running   2          3h44m\nkube-system                         vsphere-cloud-controller-manager-jpn89                           1/1     Running   2          68m\nkube-system                         vsphere-cloud-controller-manager-qblld                           1/1     Running   1          3h46m\nkube-system                         vsphere-csi-controller-576c9c8dc8-9k2lm                          5/5     Running   0          3h46m\nkube-system                         vsphere-csi-node-9fbs2                                           3/3     Running   0          68m\nkube-system                         vsphere-csi-node-ffjlv                                           3/3     Running   0          3h44m\nkube-system                         vsphere-csi-node-lrkqk                                           3/3     Running   0          68m\nkube-system                         vsphere-csi-node-s49x6                                           3/3     Running   0          3h46m\nkube-system                         vsphere-csi-node-z5hdz                                           3/3     Running   0          3h44m\n```\n\n\n## 로드밸런서 구성\n\n쿠버네티스 v1.14.2 이후 버전부터는 `IPVS`모드에서 kube-proxy 사용을 위해 MetalLB 구성전에 [strict ARP 모드를 활성화](https://metallb.universe.tf/installation/#preparation)해야한다. \n\n```sh\n> kubectl get configmap kube-proxy -n kube-system -o yaml | \\\nsed -e \"s/strictARP: false/strictARP: true/\" | \\\nkubectl apply -f - -n kube-system\n\n> kubectl describe configmap -n kube-system kube-proxy | grep ARP\n  strictARP: true\n```\n\n미리 서비스에서 사용할 IP 대역을 지정하기 위해 helm chart configmap value를 미리 생성한다. \n\n```\ncat << 'EOF' >> values.yaml\nconfigInline:\n  address-pools:\n    - name: default\n      protocol: layer2\n      addresses:\n      - 192.168.31.10-192.168.31.19\nEOF\n```\n\nmetallb가 사용할 네임스페이스 `metallb-system`를 만들고 위 configmap value파일로 metalLB를 설치한다.\n\n```\n> helm repo add metallb https://metallb.github.io/metallb\n> kubectl create ns metallb-system\n> helm install metallb metallb/metallb -n metallb-system -f values.yaml\n> kubectl get all -n metallb-system\nNAME                                      READY   STATUS    RESTARTS   AGE\npod/metallb-controller-69bbb4669c-2pdnk   1/1     Running   0          115s\npod/metallb-speaker-2km46                 1/1     Running   0          115s\npod/metallb-speaker-cm9xw                 1/1     Running   0          115s\npod/metallb-speaker-dptbx                 1/1     Running   0          115s\npod/metallb-speaker-jk7hn                 1/1     Running   0          115s\npod/metallb-speaker-kgfkl                 1/1     Running   0          115s\npod/metallb-speaker-t7qkw                 1/1     Running   0          115s\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/metallb-speaker   6         6         6       6            6           kubernetes.io/os=linux   115s\n\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/metallb-controller   1/1     1            1           115s\n\nNAME                                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/metallb-controller-69bbb4669c   1         1         1       115s\n```\n\nmetallb 리소스가 모두 정상으로 올라오면 서비스 하나를 LoadBalancer 타입으로 배포하고 해당 서비스로 접속해본다.  \n\n```\n> kubectl apply -f https://anywhere.eks.amazonaws.com/manifests/hello-eks-a.yaml\n> kubectl expose deployment hello-eks-a --port=80 --type=LoadBalancer --name=hello-eks-a-lb\n> SVC1EXIP=$(kubectl get svc hello-eks-a-lb -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n> curl $SVC1EXIP\n⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢\n\nThank you for using\n\n███████╗██╗  ██╗███████╗                                             \n██╔════╝██║ ██╔╝██╔════╝                                             \n█████╗  █████╔╝ ███████╗                                             \n██╔══╝  ██╔═██╗ ╚════██║                                             \n███████╗██║  ██╗███████║                                             \n╚══════╝╚═╝  ╚═╝╚══════╝                                             \n                                                                     \n █████╗ ███╗   ██╗██╗   ██╗██╗    ██╗██╗  ██╗███████╗██████╗ ███████╗\n██╔══██╗████╗  ██║╚██╗ ██╔╝██║    ██║██║  ██║██╔════╝██╔══██╗██╔════╝\n███████║██╔██╗ ██║ ╚████╔╝ ██║ █╗ ██║███████║█████╗  ██████╔╝█████╗  \n██╔══██║██║╚██╗██║  ╚██╔╝  ██║███╗██║██╔══██║██╔══╝  ██╔══██╗██╔══╝  \n██║  ██║██║ ╚████║   ██║   ╚███╔███╔╝██║  ██║███████╗██║  ██║███████╗\n╚═╝  ╚═╝╚═╝  ╚═══╝   ╚═╝    ╚══╝╚══╝ ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝\n                                                                     \nYou have successfully deployed the hello-eks-a pod hello-eks-a-9644dd8dc-647bq\n\nFor more information check out\nhttps://anywhere.eks.amazonaws.com\n\n⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢⬡⬢\n```\n## 인그레스 컨트롤러 설치\n\nAPI Gateway 오픈소스인 [Gloo Edge](https://docs.solo.io/gloo-edge/latest/)를 인그레스 컨트롤러로 사용할 수 있다. `Envoy` 기반의 경량화된 오픈소스로 쿠버네티스 네이티브한 옵션들을 제공하기 때문에 개인적으로 선호하는편이다. 공식 설치 문서는 다음 링크에서 확인할 수 있다.\n\n[https://docs.solo.io/gloo-edge/latest/installation/ingress/](https://docs.solo.io/gloo-edge/latest/installation/ingress/)\n\n공식 CLI인 [glooctl](https://docs.solo.io/gloo-edge/latest/installation/ingress/#installing-on-kubernetes-with-glooctl)나 [Helm Chart](https://docs.solo.io/gloo-edge/latest/installation/ingress/#installing-on-kubernetes-with-helm)로 설치를 진행할 수 있다. ingress를 구성하기 위해 `gateway.enabled` value값을 `false`로 `ingress.enabled` value를 `true`로 변경하여 inline 구성으로 배포를 진행한다.\n\n```sh\nhelm repo add gloo https://storage.googleapis.com/solo-public-helm\nhelm repo update\nkubectl create namespace gloo-system\nhelm install gloo gloo/gloo --namespace gloo-system \\\n  --set gateway.enabled=false,ingress.enabled=true\n```\n\n```sh\n> kubectl get all -n gloo-system\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/discovery-9694c78f6-zrf4l        1/1     Running   0          88s\npod/gloo-7cd566cb69-bfghh            1/1     Running   0          88s\npod/ingress-7fb5c9687d-qq98j         1/1     Running   0          88s\npod/ingress-proxy-5cc555c45b-9mj9n   1/1     Running   0          88s\n\nNAME                    TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                               AGE\nservice/gloo            ClusterIP      10.100.162.70   <none>          9977/TCP,9976/TCP,9988/TCP,9979/TCP   88s\nservice/ingress-proxy   LoadBalancer   10.107.107.38   192.168.31.11   80:32207/TCP,443:32228/TCP            88s\n\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/discovery       1/1     1            1           88s\ndeployment.apps/gloo            1/1     1            1           88s\ndeployment.apps/ingress         1/1     1            1           88s\ndeployment.apps/ingress-proxy   1/1     1            1           88s\n\nNAME                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/discovery-9694c78f6        1         1         1       88s\nreplicaset.apps/gloo-7cd566cb69            1         1         1       88s\nreplicaset.apps/ingress-7fb5c9687d         1         1         1       88s\nreplicaset.apps/ingress-proxy-5cc555c45b   1         1         1       88s\n```\n\n샘플 `petstore` 서비스를 배포하고, 인그레스 클래스 어노테이션을 `kubernetes.io/ingress.class: gloo`로 선언하고 `ddiiwoong.com` 호스트 이름으로 배포한다.\n\n```sh\nkubectl apply -f \\\n  https://raw.githubusercontent.com/solo-io/gloo/v1.2.9/example/petstore/petstore.yaml\n```\n\n```\ncat <<EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n name: petstore-ingress\n annotations:\n    # note: this annotation is only required if you've set \n    # REQUIRE_INGRESS_CLASS=true in the environment for \n    # the ingress deployment\n    kubernetes.io/ingress.class: gloo\nspec:\n  rules:\n  - host: ddiiwoong.com\n    http:\n      paths:\n      - path: /.*\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: petstore\n            port:\n              number: 8080\nEOF\n```\n\n```\n> kubectl get ingress\nNAME               CLASS    HOSTS           ADDRESS         PORTS   AGE\npetstore-ingress   <none>   ddiiwoong.com   192.168.31.11   80      7m41s\n```\n\n해당 호스트로 정상 라우팅되어 접속되는 것을 확인할 수 있다. \n```\n> curl -H \"Host: ddiiwoong.com\" 192.168.31.11/api/pets\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}]\n```\n\n## GitOps 컨트롤러로 클러스터 관리\n\ngitops 오픈소스인 [Flux](https://fluxcd.io/docs/get-started/)를 사용해서 여러가지 클러스터 관리를 수행할 수 있다. 위에서 생성한 `desired state`인 클러스터 컨피그를 github repo에서 Flux 컨트롤러를 통해 클러스터 운영을 수행할 수 있다. \n\n[https://anywhere.eks.amazonaws.com/docs/tasks/cluster/cluster-flux/](https://anywhere.eks.amazonaws.com/docs/tasks/cluster/cluster-flux/)\n\nWorker 노드와 Control Plan, Etcd등의 vSphere에서 관리하는 datastore, 디스크, 메모리, CPU, resourcepool 등을 관리할 수 있다. 단, EKS의 관리형 노드그룹과 유사한 방식으로 노드그룹의 VM의 숫자를 늘리거나 줄일수 있지만 아직까지 신규로 worker 노드그룹을 생성하거나 삭제하는 기능은 제공하지 않는다.\n\n초기 클러스터 컨피그에 아래와 같이 기존 노드그룹 `md-0`에 신규 VM 1대를 추가해보자. (2대->3대)\n\n- `clusters/$CLUSTER_NAME/eksa-system/eksa-cluster.yaml`\n\n```yaml\napiVersion: anywhere.eks.amazonaws.com/v1alpha1\nkind: Cluster\nmetadata:\n  name: homelab\n  namespace: default\nspec:\n  ...\n  workerNodeGroupConfigurations:\n  - count: 3\n    machineGroupRef:\n      kind: VSphereMachineConfig\n      name: homelab\n    name: md-0\n```\n\n초기 환경에서 구성된 gitops repo에 수정된 파일을 커밋한다.\n\n```sh\ngit add clusters/homelab/eksa-system/eksa-cluster.yaml\ngit commit -m 'Scale from 2 to 3 at WorkerNodeGroup md-0 '\ngit push origin main\n```\n\n`md-0` 노드그룹에 VM이 새롭게 추가된 것을 확인할 수 있다. \n\n```sh\n> kubectl get machine -n eksa-system\nNAME                            CLUSTER   NODENAME         PROVIDERID                                       PHASE     AGE   VERSION\nhomelab-etcd-jrhvt              homelab                    vsphere://421b44b1-6017-8469-734b-01bcf68cb459   Running   75m   \nhomelab-j5mql                   homelab   192.168.31.186   vsphere://421b4382-0727-3afc-5c47-b73455010d35   Running   75m   v1.21.5-eks-1-21-8\nhomelab-md-0-76478bb486-j68fd   homelab   192.168.31.190   vsphere://421bc1ee-eac7-f1db-b351-5fac25b45fc0   Running   11m   v1.21.5-eks-1-21-8\nhomelab-md-0-76478bb486-jszj4   homelab   192.168.31.187   vsphere://421b4dc9-b7c6-f0a2-0b50-e5379748b9a9   Running   75m   v1.21.5-eks-1-21-8\nhomelab-md-0-76478bb486-rpxlm   homelab   192.168.31.188   vsphere://421b20a4-9870-cd20-4cff-67bb4a9d8372   Running   75m   v1.21.5-eks-1-21-8\nhomelab-w6bq8                   homelab   192.168.31.189   vsphere://421befe9-e26a-3f18-9ccf-3fe32dd57fd2   Running   75m   v1.21.5-eks-1-21-8\n```\n\n## 정리 \n이번 포스팅에서는 eks-anywhere 오픈소스를 vSphere 홈서버에서 테스트를 진행한 경험을 정리했다. \n\n설치를 진행하면서 에러가 나면 하나하나 공식 문서를 확인하고, 회사 슬랙 찬스도 사용하면서 힘들게 설치를 완료했다. 생각보다 디버깅을 하는데 많은 시간을 허비했는데 실제로 2일 이상 고생을 했다. 대부분의 경우 디버그 레벨의 로그를 보면서 트러블슈팅을 통해 문제 해결이 가능했고 그 과정에서 EKS Anywhere의 내부 구성요소들과 실제 Cluster API 워크플로우를 이해할 수 있었다. \n\n다만 아직까지 문서화나 커뮤니티에 많은 유스케이스가 없다는 점과 Cilium을 메인 CNI로 활용함에도 불구하고 `CiliumNetworkPolicy` API를 사용하지 못하는 점이나 Cilium 유저 인터페이스인 [Hubble UI](https://github.com/cilium/hubble-ui)를 활성화해서 사용할 수가 없는 [제약사항](https://anywhere.eks.amazonaws.com/docs/tasks/workload/networking-and-security/#additional-cilium-features)등이 있었고, 쿠버네티스 관련해서는 `Persistent Volume` 관련하여 vSAN을 사용하지 않은 환경에서는 VMDK를 수동으로 생성하고 해당 볼륨을 매핑시켜야 활용이 가능한 제약사항이 존재하는 것이 조금 아쉬운 부분이다.\n\n실제 프로덕션 환경에서 사용할 정도라기 보단 EKS를 사용하는 관점에서 유사한 경험을 바탕으로 개발환경을 가져갈수 있고 tanzu나 다른 cluster-api를 활용한 도구들 보다 좀더 유연하고 오픈소스에 가까운 경험성을 제공한다고 생각한다. 게다가 Flux의 `Server-side reconciliation` 기반 [쿠버네티스 리소스 관리 방식](https://kubernetes.io/docs/reference/using-api/server-side-apply/)으로 클러스터를 매우 유연하게 관리할 수 있다는 것도 주목해볼만한 내용이라 생각한다. 클러스터 자체도 인프라 리소스라 가정하고 최초에 원하는 형태로 선언하고 구성이후에도 쿠버네티스 CRD 형식으로 노드그룹과 인스턴스 등의 여러 리소스를 GitOps 방식으로 관리하는 방식을 채택하고 있기 때문에 실제 인프라 관리를 하는 운영자를 위한 가장 효과적인 방법을 채택했다고 볼 수 있다.\n\n그리고 표준규격의 Managed 하드웨어를 사용하는 Outpost와 비교하는 자료들이 최근 많이 올라오고 있는데 EKS Anywhere의 가장 큰 장점은 내가 가진 하드웨어를 EKS Distro기반의 오픈소스를 통해 사용할 수 있고 AWS 환경과의 직접적인 통신이 없이도 자체적인 워크로드 운영이 가능한게 가장 큰 장점이라고 생각한다. \n\n현재 재직중인 회사의 제품과 관련해서 로드맵을 자세히 언급하지는 못하지만 엔터프라이즈 vSphere 환경과 동시에 EKS를 사용하는 조직에서는 중요한 하나의 선택지가 될 수 있다는 점에서도 지켜볼만하다고 생각한다. \n\n다음 포스팅에서는 해당 클러스터에 OpenTelemetry collector, Loki, Prometheus 구성을 통한 observability 환경 구성을 진행해볼 예정이다.\n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "kubernetes/what-is-tailscale/",
      "metadata": {
        "permalink": "/kubernetes/what-is-tailscale/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2022-02-05-what-is-tailscale.md",
        "source": "@site/blog/2022-02-05-what-is-tailscale.md",
        "title": "What is Tailscale?",
        "description": "wireguard와 tailscale을 비교하고 kubernetes 환경에서 활용방안 검토",
        "date": "2022-02-05T00:00:00.000Z",
        "formattedDate": "February 5, 2022",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "wireguard",
            "permalink": "/tags/wireguard"
          },
          {
            "label": "tailscale",
            "permalink": "/tags/tailscale"
          },
          {
            "label": "vpn",
            "permalink": "/tags/vpn"
          },
          {
            "label": "network",
            "permalink": "/tags/network"
          }
        ],
        "readingTime": 28.07,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "What is Tailscale?",
          "comments": true,
          "classes": "wide",
          "description": "wireguard와 tailscale을 비교하고 kubernetes 환경에서 활용방안 검토",
          "slug": "kubernetes/what-is-tailscale/",
          "date": "2022-02-05T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "wireguard",
            "tailscale",
            "vpn",
            "network"
          ]
        },
        "prevItem": {
          "title": "EKS Anywhere on vSphere Homelab",
          "permalink": "/kubernetes/eks-anywhere/"
        },
        "nextItem": {
          "title": "Synology monitoring with Prometheus and snmp exporter",
          "permalink": "/prometheus/synology-prometheus/"
        }
      },
      "content": "> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다.\n\n이직을 하고 블로그에 손을 놓고 있다가 페이스북 [Kubernetes Korea Group](https://ko-kr.facebook.com/groups/)에 올라온 [쿠버네티스 네트워킹 스터디 모집 공고](https://gasidaseo.notion.site/c9cd413265ea4ea1b1ae38eb36dfda94)를 보게되었다. 이직후에 대부분의 프로젝트가 쿠버네티스 기반으로 진행되다 보니 네트워크 전문가 이신 [가시다](https://www.facebook.com/jongho.seo.5811)님의 [팀 블로그](https://cloudneta.github.io/)나 이나 여러 양질의 게시글들을 참고만 하다가 조금더 깊이 학습을 하고 싶어 스터디를 신청했고, 간단한 면접(??)후에 정식으로 스터디에 참여하게 되었다.  \n\n본론으로 돌아가서 calico 스터디를 진행하다가 [wireguard](https://www.wireguard.com/)라는 opensource vpn을 알게 되었고 해당 패키지를 이것저것 테스트하다가 기존에 사용중인 [tailscale](https://tailscale.com/) 과 유사하다는 것을 인지하고 내부 구성을 조금 찾아보기로 했다.  \n\n확인해보니 tailscale은 wireguard 기반으로 하는 동일 기술의 상용 솔루션이였고, 이에 몇가지 공부차 정리를 위해 해당 포스트를 작성하게 되었다. \n\n<!--truncate-->\n\n## Wireguard\n\n- [https://www.wireguard.com/](https://www.wireguard.com/)\n\n먼저 WireGuard를 간단하게 정의하면 보안에 초점을 두고 단순함과 쉬운 사용을 대표적인 특징으로 내세우는 오픈소스 기반 VPN 소프트웨어로 IPsec과 OpenVPN보다 사용하기 용이하고 가벼운 것을 강점으로 [Linux 5.6 커널 이후부터 기본 패키지](https://lore.kernel.org/wireguard/CAHmME9qOpDeraWo5rM31EWQW574KEduRBTL-+0A2ZyqBNDeYkg@mail.gmail.com/T/#u)로 탑재가 되었다. 제이슨 도넨필드(Jason Donenfeld, [@zx2c4](https://twitter.com/zx2c4))가 설계한 WireGuard는 암호화 민첩성, 즉 다양한 암호화, 키 교환, 해싱 알고리즘에 대한 선택권을 제공한다는 개념을 버리고 아주 간결한 코드 구조를 통해 커널에서 직접 동작하고 주요 암호 알고리즘에 대해서 병렬처리하므로 빠른 속도를 자랑한다.  \n\n홈페이지에 게시된 몇가지 특징을 정리하면 \n- Simple & Easy-to-use : SSH 처럼 키교환 방식으로 진행되고 WireGuard에 의해 관리되기 때문에 쉽게 사용이 가능하고, IP가 변경되는 모바일 로밍중에서 연결이 보장되는 것이 특징이다. \n- Cryptographically Sound : [Noise protocol framework](http://www.noiseprotocol.org/), [Curve25519](http://cr.yp.to/ecdh.html), [ChaCha20](http://cr.yp.to/chacha.html), [Poly1305](http://cr.yp.to/mac.html), [BLAKE2](https://blake2.net/), [SipHash24](https://131002.net/siphash/), [HKDF](https://eprint.iacr.org/2010/264) 와 같은 가장 빠르고 최신의 암호, 해시 알고리즘을 사용한다.  \n- Minimal Attack Surface : WireGuard는 구현 용이성과 단순성을 염두에 두고 설계되었다. 매우 적은 코드 라인으로 구현되서 보안 취약점에 대해 쉽게 감사할 수 있다. 그만큼 철저히 검증될 가능성이 높고, 예상치 못한 곳에서의 버그로 인한 취약점이 발생할 가능성이 상대적으로 적다는 얘기가 된다.  \n- High Performance : 모든 것이 kernel에서 동작하고 안전한 네트워킹이 매우 빠른 속도 로 동작하기 때문에 스마트폰과 백본 라우터와 같은 작은 임베디드 장치 등 모두에 적합하다.  \n- Well Defined & Thoroughly Considered : Wireguard는 [백서](https://www.wireguard.com/papers/wireguard.pdf)가 발행될 정도로 기술적으로 여러가지 고려사항들을 명확하게 정의해놨다.\n\n관련한 자세한 내용은 [백서](https://www.wireguard.com/papers/wireguard.pdf) 를 참고한다.\n\n### Wireguard in Calico CNI\n\n스터디를 진행할때는 쿠버네티스 파드와 파드 사이의 패킷을 네트워크 레벨로 암호화 하는 도구로 Calico에 내장된 WireGuard 플러그인을 사용해서 파드간 터널을 설정하고 트래픽을 암호화 하는 테스트를 진행하게 되었다. Calico CNI에서 구성하는 방법은 `calicoctl`를 사용해서 설정하기 때문에 간단하다. \n\n```\n$ calicoctl patch felixconfiguration default --type='merge' -p '{\"spec\":{\"wireguardEnabled\":true}}'\n\n$ calicoctl get felixconfiguration default -o yaml | grep wireguardEnabled\n  wireguardEnabled: true\n```\n\n파드간의 ping test를 진행하고 난 이후 파드 레벨에서 패킷 덤프한 내용을 wireshark로 확인한 내용이다. 그림에서처럼 icmp 패킷은 확인이 안되고 udp와 wireguard 프로토콜을 통해 데이터가 암호화 된것을 확인할 수 있다.\n\n![wg](/img/wg.png)\n\n이번 포스팅에서는 tailscale 활용방안을 소개하는 것으로 간단하게 calico를 통해 wireguard 설정하는 것은 넘어간다. 다른 좋은 내용들은 다음 링크들에서 확인할 수 있다.  \n\n### 참고자료\n- [WireGuard로 멋진 VPN 서버 구축하기 - devsisters](https://tech.devsisters.com/posts/wireguard-vpn-1/)\n- [WireGuard VPN 해부 - Slowboot](https://slowbootkernelhacks.blogspot.com/2020/09/wireguard-vpn.html)\n- [Kilo - multi-cloud k8s network overlay built on WireGuard](https://github.com/squat/kilo)\n- [Go implementation of WireGuard](https://git.zx2c4.com/wireguard-go/about/)\n\n## Tailscale\n\n- [https://tailscale.com/](https://tailscale.com/)\n\nTailScale은 서버, 컴퓨터 및 클라우드 인스턴스간에 보안 네트워크를 만드는 VPN으로 Wireguard 프로토콜을 사용해서 만든 클라우드 서비스이다. Tailscale이 어떻게 동작하는지는 [공식 블로그](https://tailscale.com/blog/how-tailscale-works/)에서 간단히 확인할 수 있다. \n\n간단하게 블로그 내용을 정리해보면 기존의 VPN 방식은 Hub-and-spoke 방식으로 아래 그림과 같이 Gateway 방식으로 모든 클라이언트와 서버를 연결하는게 기존의 방식이다. 새로운 클라이언트나 서버가 추가될 경우 새로운 키를 모든 사용자에게 배포하게 되고 이는 장비가 증가함에 따라 많은 작업이 필요로 하게 된다.\n\n![hub](https://tailscale.com/blog/how-tailscale-works/hub-and-spoke-single.svg)\n\nWireguard 터널을 이용하면 노드간의 연결이 직접 이뤄지게 되고 이는 아래 그림과 같이 메쉬 형태로 구성되게 된다. \n\n![mesh](https://tailscale.com/blog/how-tailscale-works/mesh-network.svg)\n\n이런 메쉬방식의 가장 큰 단점은 피어끼리 연결하는 노드 네트워크의 경우 연결되는 경우의 수가 `n(n-1)`으로 그림의 예시에서도 10*9 = 90개의 연결 구성이 필요하게 된다. 모든 노드가 해당 사항으로 Wireguard의 key를 로테이트하거나 사용자를 추가, 제거할때마다 각 노드에 업데이트가 되어야 한다. 만약에라도 각 노드에 static ip가 없을 경우 노드끼리 찾는일이 결고 쉽지는 않을 것이다. 특히 방화벽을 운영하는 엔터프라이즈 조직의 경우에 더욱더 까다로운 일이 된다고 말한다.\n\n그래서 Tailscale에서 이야기하는 가장 핵심은 노드 간의 트래픽을 감사, 통제할 수 있는 컨트롤 플레인을 두는것이다. 방화벽이나 static ip가 없는 환경에서도 노드, 피어간의 통제가 가능한 환경을 제공하는 것이 기본 사상이다. \n\n![controlplane](https://tailscale.com/blog/how-tailscale-works/mesh-coordination-server.svg)\n\n### 기본 작동방식\n\n- 각 노드(클라이언트)는 랜덤 공개키와 개인키를 생성하고 공개키를 ID(tailscale account)와 연결한다.\n- 노드는 중앙의 tailscale서버에 공개키와 해당 노드가 현재 연결가능한지와 도메인 정보(client name) 정보를 전달한다. \n- 노드는 tailscale서버 업데이트된 도메인의 공개키 및 주소 목록을 다운로드 한다.\n- 노드는 내려받은 공개 키 세트로 Wireguard 인스턴스를 구성한다.\n\n이는 다시 hub-and-spoke 모델로 돌아간것 같지만 실제 컨트롤 플레인과 데이터 플레인을 분리해서 처리하기 때문에 실제 중앙 서버가 관리하는 것은 공개키 세트와 연결가능한 노드 리스트 정보뿐이다.\n\n## Tailscale 테스트\n\n### talescale 설치 (macOS 기준)\n\n- [https://tailscale.com/download](https://tailscale.com/download)\n\nmacOS용 CLI 설치 정보는 아래 링크에서 확인할 수 있다. \n- [https://tailscale.com/kb/1065/macos-variants/](https://tailscale.com/kb/1065/macos-variants/)\n\nmacOS용 GUI 클라이언트에 CLI도 내장되어 있기 때문에 `.zshrc`에 alias를 추가했다. \n\n```\nalias tailscale=\"/Applications/Tailscale.app/Contents/MacOS/Tailscale\"\n```\n\n2022년 2월 5일자 기준으로 1.20.2 버전을 사용한다. \n\n```\n$ tailscale version\n1.20.2\n  tailscale commit: 312750ddd288cf4073cfaef56a45102b9c1e8421\n  other commit: 2c164d9c7443e2f3014fa54ea45e946b35152680\n  go version: go1.17.6-tse44d304e54\n```\n\nwindows에서도 쉽게 설치가 가능하고, 물론 애정하는 Synology에도 패키지를 설치할 수 있다. \n\n[https://github.com/tailscale/tailscale-synology](https://github.com/tailscale/tailscale-synology)\n\n설치된 클라이언트 모두를 tailscale 계정으로 로그인하면 VPN연결이 끝난다. \n\n연결된 머신리스트는 아래와 같이 [Machines](https://login.tailscale.com/admin/machines) 메뉴 에서 확인할 수 있고 각 노드나 클라이언트로 할당된 `100.*` 대역 IP로 접속이 가능하다. \n\n![machine](/img/tailscale-web.png)\n\n여기서 machine name으로 직접 접속이 가능하기 위해서는 MagicDNS 기능을 사용해야 하는데 `beta` 기능으로 [활성화](https://login.tailscale.com/admin/dns)를 할 수 있고 네트워크에 있는 장치의 DNS 이름을 자동으로 등록하는 기능이다.\n\n- DNS 설정가이드 : [https://tailscale.com/kb/1081/magicdns/](https://tailscale.com/kb/1081/magicdns/)\n\nCLI로 현재 연결된 client 정보나 publickey 등을 확인할 수 있다. \n\n```\n$ tailscale status\n100.89.226.113  88665a4a51c4         ddiiwoong@   macOS   -\n100.76.233.116  jinwoonuimbp141      ddiiwoong@   macOS   offline\n100.95.209.95   proxy                ddiiwoong@   linux   -\n100.106.65.5    win11                ddiiwoong@   windows -\n\n$ tailscale status --json | jq '.Self.PublicKey'\n\"nodekey:97c4dc44ecdf56******************adb3246d87fd9e270\"\n```\n\nDNS 등록 방식은 아래와 같이 구성되고\n\n![magicdns](https://tailscale.com/kb/1081/magicdns/magic-dns-naming.png)\n\n내가 등록한 머신리스트에서 win11 은 다음 정보로 도메인이 등록된다.\n\n```\nwin11.ddiiwoong.gmail.com.beta.tailscale.net\n```\n\n두가지 방식으로 해당 머신에 ping check를 할 수 있다.\n\n```sh\n$ ping win11\nPING win11.ddiiwoong.gmail.com.beta.tailscale.net (100.106.65.5): 56 data bytes\n64 bytes from 100.106.65.5: icmp_seq=0 ttl=128 time=124.031 ms\n64 bytes from 100.106.65.5: icmp_seq=1 ttl=128 time=2.180 ms\n64 bytes from 100.106.65.5: icmp_seq=2 ttl=128 time=2.776 ms\n\n$ ping win11.ddiiwoong.gmail.com.beta.tailscale.net\nPING win11.ddiiwoong.gmail.com.beta.tailscale.net (100.106.65.5): 56 data bytes\n64 bytes from 100.106.65.5: icmp_seq=0 ttl=128 time=5.086 ms\n64 bytes from 100.106.65.5: icmp_seq=1 ttl=128 time=2.060 ms\n64 bytes from 100.106.65.5: icmp_seq=2 ttl=128 time=2.213 ms\n```\n\n### kubernetes 환경구성\n\nkubernetes 리소스로 tailscale을 구성을 해보자. 먼저 `1.16` 버전 이상의 kubernetes 클러스터를 준비한다. 아래 구성환경은 `1.21.5` 버전으로 eksctl로 설치한 기본 EKS 클러스터이다. \n\n```sh\n$ kubectl get node\nNAME                                                 STATUS   ROLES    AGE   VERSION\nip-192-168-101-195.ap-northeast-2.compute.internal   Ready    <none>   19h   v1.21.5-eks-9017834\nip-192-168-134-35.ap-northeast-2.compute.internal    Ready    <none>   19h   v1.21.5-eks-9017834\nip-192-168-186-235.ap-northeast-2.compute.internal   Ready    <none>   19h   v1.21.5-eks-9017834\n```\n\n### authKey 발급 및 Secret 등록\n\nauthKey는 tailscale 네트워크에 등록하기 위한 Key로 아래와 같이 관리를 Web에서 할 수 있다.\n\n![key](/img/tailscale-key.png)\n\n[Keys](https://login.tailscale.com/admin/settings/keys) 메뉴로 이동해서 auth Key를 발급받는다. 발급을 할때 여러머신에서 사용할 경우는 `Reusable`, 임시로 키를 사용할때는 `Ephemeral` 옵션을 사용할 수도 있다. 아래와 같은 형식으로 발급되고, 해당 key는 kubernetes `Secret` 리소스에 등록을 해서 사용할 것이다.\n\n```\ntskey-kZAinb5CNTRL-********************\n```\n\n해당 auth Key를 Secret으로 생성한다.\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: tailscale-auth\nstringData:\n  AUTH_KEY: tskey-kZAinb5CNTRL-********************\n```\n```sh\n$ kubectl get secret -n tailscale\nNAME                    TYPE                                  DATA   AGE\ndefault-token-fww2j     kubernetes.io/service-account-token   3      18h\ntailscale-auth          Opaque                                3      18h\ntailscale-token-7cgsc   kubernetes.io/service-account-token   3      17h\n```\n\n### image build & push\n\ntailscale image를 빌드한다. 자세한 내용은 아래 링크를 참고한다.  \n- [https://github.com/tailscale/tailscale/tree/main/docs/k8s](https://github.com/tailscale/tailscale/tree/main/docs/k8s)\n\nDockerfile은 다음과 같이 공식 이미지를 사용한다. \n```dockerfile\nFROM ghcr.io/tailscale/tailscale:latest\nCOPY run.sh /run.sh\nCMD \"/run.sh\"\n```\n\n[run.sh](https://github.com/tailscale/tailscale/blob/main/docs/k8s/run.sh)을 살펴보면 `tailscaled` 데몬을 실행할때 몇가지 환경변수를 입력받아 처리하는 것을 알 수 있다. \n\n`AUTH_KEY`와 `KUBE_SECRET`은 위 Secret으로 처리를 하고 여러 사용 모드(router, userspace)도 컨테이너 실행시 입력받아 처리한다. 또한, 마지막 구문을 보면 목적지 IP DNAT 처리를 위한 iptables 등록 옵션도 있다.\n\n```sh\n#! /bin/sh\n\nexport PATH=$PATH:/tailscale/bin\n\nAUTH_KEY=\"${AUTH_KEY:-}\"\nROUTES=\"${ROUTES:-}\"\nDEST_IP=\"${DEST_IP:-}\"\nEXTRA_ARGS=\"${EXTRA_ARGS:-}\"\nUSERSPACE=\"${USERSPACE:-true}\"\nKUBE_SECRET=\"${KUBE_SECRET:-tailscale}\"\n\nset -e\n\nTAILSCALED_ARGS=\"--state=kube:${KUBE_SECRET} --socket=/tmp/tailscaled.sock\"\n\nif [[ \"${USERSPACE}\" == \"true\" ]]; then\n  if [[ ! -z \"${DEST_IP}\" ]]; then\n    echo \"IP forwarding is not supported in userspace mode\"\n    exit 1\n  fi\n  TAILSCALED_ARGS=\"${TAILSCALED_ARGS} --tun=userspace-networking\"\nelse\n  if [[ ! -d /dev/net ]]; then\n    mkdir -p /dev/net\n  fi\n\n  if [[ ! -c /dev/net/tun ]]; then\n    mknod /dev/net/tun c 10 200\n  fi\nfi\n\necho \"Starting tailscaled\"\ntailscaled ${TAILSCALED_ARGS} &\nPID=$!\n\nUP_ARGS=\"--accept-dns=false\"\nif [[ ! -z \"${ROUTES}\" ]]; then\n  UP_ARGS=\"--advertise-routes=${ROUTES} ${UP_ARGS}\"\nfi\nif [[ ! -z \"${AUTH_KEY}\" ]]; then\n  UP_ARGS=\"--authkey=${AUTH_KEY} ${UP_ARGS}\"\nfi\nif [[ ! -z \"${EXTRA_ARGS}\" ]]; then\n  UP_ARGS=\"${UP_ARGS} ${EXTRA_ARGS:-}\"\nfi\n\necho \"Running tailscale up\"\ntailscale --socket=/tmp/tailscaled.sock up ${UP_ARGS}\n\nif [[ ! -z \"${DEST_IP}\" ]]; then\n  echo \"Adding iptables rule for DNAT\"\n  iptables -t nat -I PREROUTING -d \"$(tailscale --socket=/tmp/tailscaled.sock ip -4)\" -j DNAT --to-destination \"${DEST_IP}\"\nfi\n\nwait ${PID}\n```\n\nbuild하고 레지스트리로 push 하자.\n```sh\n$ export IMAGE_TAG=ddiiwoong/tailscale-k8s:latest\n$ docker build . -t $(IMAGE_TAG)\n$ docker push $(IMAGE_TAG)\n$ docker images ddiiwoong/tailscale-k8s:latest\nREPOSITORY            TAG       IMAGE ID       CREATED        SIZE\nddiiwoong/tailscale-k8s   latest    9dc489da64c4   18 hours ago   44.1MB\n```\n\n### RBAC 리소스 생성\n\nrbac 설정을 위해서 `role`, `rolebinding`, `serviceaccount`를 생성한다.\n\n```sh\nexport SA_NAME=tailscale\nexport KUBE_SECRET=tailscale-auth\n```\n\n위 ENV값을 치환해서 생성한다.\n\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tailscale\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: tailscale\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"secrets\"]\n  # Create can not be restricted to a resource name.\n  verbs: [\"create\"]\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resourceNames: [\"tailscale-auth\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"update\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: tailscale\nsubjects:\n- kind: ServiceAccount\n  name: tailscale\nroleRef:\n  kind: Role\n  name: tailscale\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### sidecar 배포\n\nsidecar 생성을 위해 manifest를 확인한다. 자세히 살펴보면 `ts-sidecar` 컨테이너 securityContext에 `NET_ADMIN` 권한이 추가된것을 볼수 있다. 이는 sidecar 파드가 터널 인터페이스를 구성하기 위해 상위 노드 `CAP_NET_ADMIN` 권한을 취득하기 위함이다. 자세한 내용은 [Set capabilities for a Container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container) 문서를 확인하자. \n\n그리고 추가 sidecar로 `nicolaka/netshoot`를 추가했는데 이는 [네트워크 장애 처리에 유용한 프로젝트](https://github.com/nicolaka/netshoot)로 여러가지 네트워크 정보를 확인할 수 있다.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  serviceAccountName: tailscale\n  containers:\n  - name: nginx\n    image: nginx\n  - name: netshoot\n    image: nicolaka/netshoot \n    command: [\"tail\"]\n    args: [\"-f\", \"/dev/null\"]\n  - name: ts-sidecar\n    imagePullPolicy: Always\n    image: ddiiwoong/tailscale-k8s:latest\n    env:\n      # Store the state in a k8s secret\n    - name: KUBE_SECRET\n      value: tailscale-auth\n    - name: USERSPACE\n      value: \"false\"\n    - name: AUTH_KEY\n      valueFrom:\n        secretKeyRef:\n          name: tailscale-auth\n          key: AUTH_KEY\n          optional: true\n    securityContext:\n      capabilities:\n        add:\n        - NET_ADMIN\n```\n\n생성을 하고 살펴보면 sidecar nginx 파드가 생성된 것을 확인할 수 있다.\n\n```sh\n$ kubectl get pod nginx -n tailscale\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   3/3     Running   0          25s\n```\n\ntailscale sidecar 로그를 잠깐 살펴보자. 새로운 `tailscale0` tun 인터페이스가 추가되고 원래 pod eth0 인터페이스 ip도 확인이 된다. 또한 `wireguard` device도 올라오는 것을 확인할 수 있다. \n\n```sh\n$ kubectl logs nginx ts-sidecar -n tailscale\nStarting tailscaled\nRunning tailscale up\n2022/02/05 12:20:26 logtail started\n2022/02/05 12:20:26 Program starting: v1.20.2-t8e643357d, Go 1.17.6-tse44d304e54: []string{\"tailscaled\", \"--state=kube:tailscale-auth\", \"--socket=/tmp/tailscaled.sock\"}\n2022/02/05 12:20:26 LogID: 31416c9a454e3667661bf0f21ccde8ebf72604d7434660b0db55e838be372bc4\n2022/02/05 12:20:26 logpolicy: using system state directory \"/var/lib/tailscale\"\nlogpolicy.Read /var/lib/tailscale/tailscaled.log.conf: open /var/lib/tailscale/tailscaled.log.conf: no such file or directory\n2022/02/05 12:20:26 wgengine.NewUserspaceEngine(tun \"tailscale0\") ...\n2022/02/05 12:20:26 router: disabling tunneled IPv6 due to system IPv6 config: disable_ipv6 is set\n2022/02/05 12:20:26 dns: [rc=unknown ret=direct]\n2022/02/05 12:20:26 dns: using *dns.directManager\n2022/02/05 12:20:26 link state: interfaces.State{defaultRoute=eth0 ifs={eth0:[192.168.118.89/32]} v4=true v6=false}\n2022/02/05 12:20:26 magicsock: disco key = d:1b04c57ebf000fe0\n2022/02/05 12:20:26 Creating wireguard device...\n2022/02/05 12:20:26 Bringing wireguard device up...\n2022/02/05 12:20:26 Bringing router up...\n2022/02/05 12:20:26 external route: up\n2022/02/05 12:20:26 Clearing router settings...\n2022/02/05 12:20:26 Starting link monitor...\n2022/02/05 12:20:26 Engine created.\n2022/02/05 12:20:26 netmap packet filter: (not ready yet)\n2022/02/05 12:20:26 Start\n2022/02/05 12:20:26 using backend prefs\n...\n2022/02/05 12:20:28 active login: ddiiwoong@gmail.com\n2022/02/05 12:20:28 netmap packet filter: 1 filters\n...\n2022/02/05 12:20:28 dns: Set: {DefaultResolvers:[] Routes:{} SearchDomains:[] Hosts:4}\n2022/02/05 12:20:28 dns: Resolvercfg: {Routes:{} Hosts:4 LocalDomains:[]}\n2022/02/05 12:20:28 dns: OScfg: {Nameservers:[] SearchDomains:[] MatchDomains:[]}\n2022/02/05 12:20:28 Taildrop disabled; no state directory\n2022/02/05 12:20:28 peerapi starting without Taildrop directory configured\n2022/02/05 12:20:28 peerapi: serving on http://100.95.209.95:34980\n...\n```\n\nnetshoot 파드내에서도 `tailscale0` 인터페이스가 추가된 것을 확인할 수 있다.\n```sh\n$ kubectl exec -it nginx -c netshoot -- ip -c addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n3: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default\n    link/ether 6e:f8:f5:f3:47:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 192.168.118.89/32 scope global eth0\n       valid_lft forever preferred_lft forever\n4: tailscale0: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1280 qdisc pfifo_fast state UNKNOWN group default qlen 500\n    link/none\n    inet 100.95.209.95/32 scope global tailscale0\n       valid_lft forever preferred_lft forever\n```\n\n로컬에서 `tailscale` CLI를 통해 IP정보도 동일한 것을 확인 할 수 있다.\n\n```sh\n$ tailscale status\n100.89.226.113  88665a4a51c4         ddiiwoong@   macOS   -\n100.76.233.116  jinwoonuimbp141      ddiiwoong@   macOS   offline\n100.95.209.95   nginx                ddiiwoong@   linux   -\n100.106.65.5    win11                ddiiwoong@   windows -\n```\n\n먼저 어떠한 kubernetes 서비스도 없는 것을 확인하고, 미리 magicdns를 구성해놨기 때문에 `nginx` 주소로 직접 통신을 시도해보자. \n\n```\n$ kubectl get svc -n A\nNo resources found in A namespace.\n\n$ ping nginx\nPING nginx.ddiiwoong.gmail.com.beta.tailscale.net (100.95.209.95): 56 data bytes\n64 bytes from 100.95.209.95: icmp_seq=0 ttl=255 time=193.606 ms\n64 bytes from 100.95.209.95: icmp_seq=1 ttl=255 time=95.747 ms\n64 bytes from 100.95.209.95: icmp_seq=2 ttl=255 time=94.441 ms\n\n$ curl http://nginx\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n```\n\n### Userspace Sidecar 테스트\n\nuserspace 모드로 실행해보자. 위와 차이점은 `NET_ADMIN` 권한을 빼고, `runAsUser: 1000`와 `runAsGroup: 1000`을 추가했다. 해당 클라이언트나 노드가 외부로 통신을 위해서는 [SOCKS5 이나 HTTP proxy 모드](https://tailscale.com/kb/1112/userspace-networking/)로 실행되어야 한다. 이번 데모에서는 구성하지 않는다.\n\nuserspace 모드는 주로 서버리스 워크로드(Heroku, Google Cloud Run, AWS Lambda, Github Action)에서 활용을 할 때 좋다. 단 보안을 위해 authKey를 `ephemeral` 형태로 구성하는게 좋다.\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  serviceAccountName: tailscale\n  containers:\n  - name: nginx\n    image: nginx\n  - name: netshoot\n    image: nicolaka/netshoot \n    command: [\"tail\"]\n    args: [\"-f\", \"/dev/null\"]\n  - name: ts-sidecar\n    imagePullPolicy: Always\n    image: ddiiwoong/tailscale-k8s:latest\n    securityContext:\n      runAsUser: 1000\n      runAsGroup: 1000\n    env:\n    - name: KUBE_SECRET\n      value: tailscale-auth\n    - name: USERSPACE\n      value: \"true\"\n    - name: AUTH_KEY\n      valueFrom:\n        secretKeyRef:\n          name: tailscale-auth\n          key: AUTH_KEY\n          optional: true\n```\n\n동일하게 접속이 가능한데, userspace 모드 이기 때문에 netshoot 컨테이너로 인터페이스를 확인하면 권한이 없기 때문에 eth0 만 확인 가능하다. \n\n```sh\n$ kubectl exec -it nginx -c netshoot -- ip -c addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n3: eth0@if10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default\n    link/ether 3a:c5:b6:cf:d1:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 192.168.117.251/32 scope global eth0\n       valid_lft forever preferred_lft forever\n```\n\n### Proxy 모드 테스트\n\n기존의 배포된 Service 리소스에 연결하는 방식이다. 미리 nginx를 띄워 놓고 ClusterIP를 확인한다.\n\n```\n$ kubectl create deployment nginx --image nginx\n$ kubectl expose deployment nginx --port 80\n$ kubectl get svc nginx -o=jsonpath='{.spec.clusterIP}'\ndeployment.apps/nginx created\nservice/nginx exposed\n10.100.213.92%\n```\n\nProxy 파드를 배포한다. 참고할 사항은 `sysctler` 컨테이너인데, 기본적으로 `net.ipv4.ip_forward` 플래그는 `Kubelet`에서 화이트리스트 처리가 되어 있지 않으므로 내부에서 IP Forwarding을 위해서 추가 설정이 필요하다. \n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: proxy\nspec:\n  serviceAccountName: tailscale\n  initContainers:\n  - name: sysctler\n    image: busybox\n    securityContext:\n      privileged: true\n    command: [\"/bin/sh\"]\n    args:\n      - -c\n      - sysctl -w net.ipv4.ip_forward=1\n    resources:\n      requests:\n        cpu: 1m\n        memory: 1Mi\n  containers:\n  - name: tailscale\n    imagePullPolicy: Always\n    image: ddiiwoong/tailscale-k8s:latest\n    env:\n    - name: KUBE_SECRET\n      value: tailscale-auth\n    - name: USERSPACE\n      value: \"false\"\n    - name: AUTH_KEY\n      valueFrom:\n        secretKeyRef:\n          name: tailscale-auth\n          key: AUTH_KEY\n          optional: true\n    - name: DEST_IP\n      value: 10.100.213.92\n    securityContext:\n      capabilities:\n        add:\n        - NET_ADMIN\n```\n\n```\nkubectl get pod -n tailscale\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-5g49v   1/1     Running   0          5m59s\nproxy                    1/1     Running   0          22s\n```\n\n정상적으로 배포된 proxy 컨테이너로 접속해보면 동일하게 접근이 되는 것을 확인할 수 있다.  \n\n\n```sh\n$ tailscale status\n100.89.226.113  88665a4a51c4         ddiiwoong@   macOS   -\n100.76.233.116  jinwoonuimbp141      ddiiwoong@   macOS   offline\n100.95.209.95   proxy                ddiiwoong@   linux   idle, tx 788 rx 1452\n100.106.65.5    win11                ddiiwoong@   windows -\n\n$ curl https://proxy\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n```\n\n### Subnet Router 모드 테스트\n\nTailscale에서 Subnet Router 모드를 사용하면 전체 클러스터의 subnet 대역으로 접근이 가능하다. \n\n기본적으로 EKS API는 서비스 subnet으로 `10.100.0.0/16` 또는 `172.20.0.0/16` 을 사용한다. 배포한 eksctl로 구성한 EKS 클러스터 subnet은 3개 AZ에 19bit로 나뉘어 배포되었고 `192.168.96.0/19`, `192.168.128.0/19`, `192.168.160.0/19`으로 구성되어 있다. \n\n```\n$ kubectl get node -o wide\nNAME                                                 STATUS   ROLES    AGE   VERSION               INTERNAL-IP       EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                CONTAINER-RUNTIME\nip-192-168-101-195.ap-northeast-2.compute.internal   Ready    <none>   23h   v1.21.5-eks-9017834   192.168.101.195   <none>        Amazon Linux 2   5.4.172-90.336.amzn2.x86_64   docker://20.10.7\nip-192-168-134-35.ap-northeast-2.compute.internal    Ready    <none>   23h   v1.21.5-eks-9017834   192.168.134.35    <none>        Amazon Linux 2   5.4.172-90.336.amzn2.x86_64   docker://20.10.7\nip-192-168-186-235.ap-northeast-2.compute.internal   Ready    <none>   23h   v1.21.5-eks-9017834   192.168.186.235   <none>        Amazon Linux 2   5.4.172-90.336.amzn2.x86_64   docker://20.10.7\n\n$ kubectl get pod -o wide\nNAME                     READY   STATUS    RESTARTS   AGE     IP                NODE                                                 NOMINATED NODE   READINESS GATES\nnginx-6799fc88d8-5g49v   1/1     Running   0          11m     192.168.112.183   ip-192-168-101-195.ap-northeast-2.compute.internal   <none>           <none>\nproxy                    1/1     Running   0          5m39s   192.168.155.196   ip-192-168-134-35.ap-northeast-2.compute.internal    <none>           <none>\n\n$ kubectl get svc -o wide\nNAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nnginx   ClusterIP   10.100.213.92   <none>        80/TCP    11m   app=nginx\n```\n\n해당 subnet 대역을 접속하기 위해서 서비스 대역인 `10.100.0.0/16`와 파드 대역인 `192.168.96.0/19`, `192.168.128.0/19`, `192.168.160.0/19`를 설정해서 router 모드로 tailscale을 실행하자.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: subnet-router\n  labels:\n    app: tailscale\nspec:\n  serviceAccountName: tailscale\n  containers:\n  - name: tailscale\n    imagePullPolicy: Always\n    image: ddiiwoong/tailscale-k8s:latest\n    env:\n    - name: KUBE_SECRET\n      value: tailscale-auth\n    - name: USERSPACE\n      value: \"true\"\n    - name: AUTH_KEY\n      valueFrom:\n        secretKeyRef:\n          name: tailscale-auth\n          key: AUTH_KEY\n          optional: true\n    - name: ROUTES\n      value: 10.100.0.0/16,192.168.96.0/19,192.168.128.0/19,192.168.160.0/19\n    securityContext:\n      runAsUser: 1000\n      runAsGroup: 1000\n```\n\n배포된 router 디바이스와, 내부의 서비스에 접근하기 위해 파드와 서비스 IP를 확인한다.\n\n```sh\n$ tailscale status\n100.89.226.113  88665a4a51c4         ddiiwoong@   macOS   -\n100.76.233.116  jinwoonuimbp141      ddiiwoong@   macOS   offline\n100.95.209.95   subnet-router        ddiiwoong@   linux   idle, tx 1556 rx 2908\n100.106.65.5    win11                ddiiwoong@   windows -\n\n$ kubectl get pod -n tailscale\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-6799fc88d8-5g49v   1/1     Running   0          39m\nsubnet-router            1/1     Running   0          9m47s\n\n$ kubectl get pod -n tailscale -o wide\nNAME                     READY   STATUS    RESTARTS   AGE   IP                NODE                                                 NOMINATED NODE   READINESS GATES\nnginx-6799fc88d8-5g49v   1/1     Running   0          39m   192.168.112.183   ip-192-168-101-195.ap-northeast-2.compute.internal   <none>           <none>\nsubnet-router            1/1     Running   0          10m   192.168.137.188   ip-192-168-134-35.ap-northeast-2.compute.internal    <none>           <none>\n\n$ kubectl get svc -n tailscale\nNAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nnginx   ClusterIP   10.100.213.92   <none>        80/TCP    40m\n```\n\n바로 접속을 해보면 접속이 안된다는걸 알수 있다. [tailscale 서비스](https://login.tailscale.com/admin/machines)로 이동해 route 설정에서 접근할 대역을 아래와 같이 활성화하고 curl로 접속해보면 정상적으로 접근이 된다는걸 알수 있다. 이걸로 생각해봤을 때 `NetworkPolicy` 등과 함께 설정을 하면 내부 리소스 접근 통제도 가능할 것으로 생각된다. \n\n![router](/img/subnetmode.png)\n\n```\n$ curl 192.168.112.183\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n\n$ curl 10.100.213.92\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n...\n```\n\n## 활용방안\n\n기존에는 집 내부에 있는 자원 NAS나 macOS 등을 원격으로 관리하기 위한 용도였다면, 이번 테스트를 통해 기본 Personal 무료 Plan으로도 활용가능한 몇가지 유스케이스를 생각해봤다.  \n\n- 추가적인 VPN 구성없이 원격지 쿠버네티스 클러스터에 존재하는 애플리케이션 디버깅 및 관리 가능 (Telepresence 대체)\n- sidecar 모드로 활용시 Private 리소스에 대한 서비스 구성 및 IP관리 불필요 \n- Prometheus Service Discovry를 tailscale로 진행하여 개인 관리 디바이스 전체 모니터링\n- 모바일 웹앱 테스트시 본인 휴대폰으로 직접 액세스 가능\n- 현재 개인 프로젝트로 활용중인 Plex(미디어서버), 노트 앱 등을 서비스 구성없이 ACL 및 MFA 적용을 통한 보안 강화\n- wireguard 프로토콜을 활용하기 때문에 어디서든 끊김없이 클러스터 리소스 접근\n- 현재 구성중인 site-to-site vpn 대체 (OCI-OCI간)\n\n더 많은 활용방안이 있을텐데 일단 생각나는 것들만 적어보았다.\n\n## 정리\n\n이번 포스팅은 네트워크 스터디를 진행하면서 과제로 작성된 부분이 없지 않다. 정말 이번 스터디는 역대급으로 퀄리티나 여러가지면에서 좋은 점이 많다. 항상 대단하다고 느끼는 분들과 함께하고 있어서 마지막까지 완주할 수 있도록 조금 더 공부해야 겠다는 생각이 든다. 다시한번 [가시다](https://www.facebook.com/jongho.seo.5811)님을 `shout-out` 하고 2022년 첫 포스팅은 이것으로 마무리한다.\n\n> 해당 포스팅은 현재 재직중인 회사에 관련이 없고, 개인 역량 개발을 위한 자료로 활용할 예정입니다."
    },
    {
      "id": "/prometheus/synology-prometheus/",
      "metadata": {
        "permalink": "/prometheus/synology-prometheus/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2021-06-11-synology-prometheus.md",
        "source": "@site/blog/2021-06-11-synology-prometheus.md",
        "title": "Synology monitoring with Prometheus and snmp exporter",
        "description": "Prometheus로 시놀로지 NAS Monitoring하는 방법 정리",
        "date": "2021-06-11T00:00:00.000Z",
        "formattedDate": "June 11, 2021",
        "tags": [
          {
            "label": "Prometheus, Synology, 시놀로지",
            "permalink": "/tags/prometheus-synology-시놀로지"
          }
        ],
        "readingTime": 11.165,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Synology monitoring with Prometheus and snmp exporter",
          "comments": true,
          "classes": "wide",
          "description": "Prometheus로 시놀로지 NAS Monitoring하는 방법 정리",
          "slug": "/prometheus/synology-prometheus/",
          "date": "2021-06-11T00:00:00.000Z",
          "categories": [
            "prometheus"
          ],
          "tags": [
            "Prometheus, Synology, 시놀로지"
          ]
        },
        "prevItem": {
          "title": "What is Tailscale?",
          "permalink": "/kubernetes/what-is-tailscale/"
        },
        "nextItem": {
          "title": "vSphere with Tanzu homelab running on ASUS PN50 (2/2)",
          "permalink": "/kubernetes/vSphere-with-Tanzu-homelab-2/"
        }
      },
      "content": "이번 포스팅에서는 아재라면 다들 하나씩 구비하고 있을 시놀로지(Synology) NAS를 Prometheus와 snmp_exporter를 활용해서 모니터링 하는 방법을 정리한다.\n\n<!--truncate-->\n\n## 구성방안\n\n총 5개의 구성으로 모니터링 스택을 구성하려고 한다.  \n\n구성방법은 아래와 같다. 모든 구성요소는 시놀로지내 도커 컨테이너로 구성될 예정이다.\n\n- Prometheus : 데이터 수집 및 저장을 담당하는 시계열 데이터베이스\n- Alertmanager : Prometheus에서 발생된 알림을 수신하여 메시지를 전달\n- Node Exporter : Linux Node의 모니터링 데이터를 Expose\n- SNMP Exporter : SNMP 기반 디바이스의 모니터링 데이터를 Expose\n- Grafana : 시각화 도구\n\n![arch](/img/synology/arch.png)  \n\n\n## 시놀로지 모니터링\n\n기본적으로 시놀로지 UI에서도 관련된 데이터들을 확인할수 있고 디스크 오류나 전원 등 특별한 이벤트가 발생했을 경우는 별도로 이메일이나 커스텀 스크립트를 통해 알림을 받을 수 있지만 메트릭을 기반으로 하는 특정 상황에서 알림을 받거나 내가 원하는 모니터링 대시보드를 꾸미기 위한 용으로 시놀로지는 SNMP기반의 모니터링을 제공한다. 다른 NMS 또는 모니터링 도구등을 사용하기 보다는 업무와 관련있는 실제 SNMP exporter를 구성해보고 프로메테우스와 그라파나 대시보드를 통해 모니터링을 하는데 목적이 있다.\n\n## 시놀로지 설정\n\n### 사전 요구사항\n- [시놀로지 Docker](https://www.synology.com/en-global/dsm/packages/Docker)\n- SSH 허용\n- Admin 권한\n- SNMP 설정\n\nSNMP 설정을 제외한 3가지 구성은 기본적으로 되어있다 가정하고 진행한다.  \n\n시놀로지는 기본적으로 SNMP 설정이 비활성화 상태이기 때문에 변경이 필요하다. 아래 그림과 같이 시놀로지의 제어판 - 터미널 및 SNMP - SNMP탭 으로 이동해서 SNMP 서비스를 활성화를 체크하고 이후 snmp exporter 설정에서 사용될 community값을 원하는 값으로 변경한 후 저장한다.  \n\n![snmp](/img/synology/snmp.png)\n\n그럼 이제 시놀로지는 외부 통신을 위한 Port인 UDP 161 port로 snmpd을 실행하게 된다. \n\n```sh\nroot@DSM2:~# netstat -unlp | grep 161\nudp        0      0 0.0.0.0:161             0.0.0.0:*                           9437/snmpd\nudp6       0      0 :::161                  :::*                                9437/snmpd\n```\n\n## SNMP Exporter\n\n[https://github.com/prometheus/snmp_exporter](https://github.com/prometheus/snmp_exporter)\n\nSNMP Exporter는 프로메테우스가 수집할 수 있는 형식으로 SNMP 메트릭 데이터를 Expose 할때 사용하는 방법으로 전통적인 모니터링 도구와 마찬가지로 MIB를 사용하게 된다. 레포지토리 컨셉에도 설명이 적혀있듯이 SNMP 데이터는 계층형 데이터 구조를 가지고 있고 프로메테우스는 다차원 매트릭스를 사용하고 있기 때문에 두 시스템은 잘 맞는다고 볼 수 있다.\n\nSNMP 데이터 구조를 여기서 자세히 설명하지는 않겠지만  SNMP index와 label을 매핑하는 방식으로 데이터를 처리한다. 다른 익스포터와 동일하게 daemon 형태로 실행이 되고 `http://localhost:9116/snmp?module=if_mib&target=1.2.3.4` 와 같이 module과 target을 설정하는 방식으로 데이터 수집을 할 수 있다.\n\n기본 config 파일을 `snmp.yml`을 참조하게 되는데 수동으로 작성하는 것이 아니라 generator를 통해 생성하게 된다.  \n[https://github.com/prometheus/snmp_exporter/tree/main/generator](https://github.com/prometheus/snmp_exporter/tree/main/generator)\n\n해당 링크에서 확인할 수 있듯이 generator에서 벤더별 MIB 파일과 generator.yml를 참조해서 빌드, 실행하고 결과값으로 snmp.yml이 생성되게 된다. 따로 만들어도 되지만 시놀로지에만 해당 파일을 바로 사용할 수 있도록 미리 만들어 두신 분이 계셔서 [tumak](https://grafana.com/orgs/tumak)의 그라파나 대시보드를 참조하였다. 그리고 이후 시놀로지 대시보드로도 사용할 예정이다.  \n\n[https://grafana.com/grafana/dashboards/14284](https://grafana.com/grafana/dashboards/14284)\n\n만들어진 `snmp.yml`에서 변경해야 할 부분은 현재 시놀로지에서 설정된 SNMP community 설정이다. snmp.yml 맨 아래 community 값을 시놀로지에 설정한 값과 일치하게 변경한다.  \n\n```yaml\n  auth:\n    community: synology\n```\n\n## Prometheus, Alertmanager, Exporter, Grafana 구성\n\n간단한 아키텍처 구성도로 시놀로지 자체 또는 OS기반에서 발생하는 모니터링 데이터를 수집하는 각각의 Exporter를 구성하고 해당 데이터를 수집하는 Prometheus 서버와 관련 데이터를 시각화하는 Grafana까지 한번에 구성하기 위해 Docker-compose를 사용할 예정이다.  \n\nDocker Compose 파일을 맨 아래 snmp exporter 부터 하나씩 쪼개서 살펴보자.  \n\nSNMP Exporter는 위에서 만든 snmp.yml 파일을 마운트해서 실행되도록 한다. 그러면 SNMP exporter가 실행되면서 snmp파일을 참조해서 실행이 되고 prometheus.yaml에서 정의한 target과 SNMP 프로토콜을 통해 관련된 메트릭 값을 수집해서 Expose하게 될 것이다.  \n\n```yaml\n  snmp-exporter:\n    image: prom/snmp-exporter\n    container_name: snmp_exporter\n    volumes:\n      - ./snmp_exporter/:/etc/snmp_exporter/\n    ports:\n      - 9116:9116\n    volumes:\n      - ./snmp-synology/snmp.yml:/etc/snmp_exporter/snmp.yml\n    command:\n      - \"--config.file=/etc/snmp_exporter/snmp.yml\"\n```\n\n다음은 Node Exporter를 실행하는 부분으로 시스템에 관련된 권한이 필요하므로 `/proc, /sys, /` 등 파일시스템의 read only 권한과 `privileged: true` 설정을 통해 실행되게 될 것이다.\n\n```yaml\n  node-exporter:\n    privileged: true\n    image: prom/node-exporter\n    container_name: node-exporter\n    restart: always\n    ports:\n      - \"9100:9100\"\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n    command:\n      - \"--path.procfs=/host/proc\"\n      - \"--path.sysfs=/host/sys\"\n      - \"--collector.filesystem.ignored-mount-points\"\n      - \"^/(rootfs/)?(dev|etc|host|proc|run|sys|volume1)($$|/)\"\n```\n\n다음은 시각화와 알림을 담당할 Grafana와 Alertmanager 부분으로 Alertmanager의 경우 기본적인 config를 마운트해서 실행을 할 예정이다.\n\n```yaml\n  grafana:\n    container_name: grafana\n    image: grafana/grafana\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - prometheus\n\n  alertmanager:\n    container_name: alertmanager\n    image: prom/alertmanager:v0.21.0\n    ports:\n      - 9093:9093\n    volumes:\n      - ./config/alertmanager.yml:/etc/alertmanager/alertmanager.yml\n    restart: always\n```\n\n마지막으로, Prometheus는 알림룰을 생성하기 위해서 별도로 rules 디렉토리와 미리 생성해둔 config를 mount 시킨다.  \n\n```yaml\nversion: \"3.8\"\nservices:\n  prometheus:\n    container_name: prometheus\n    image: quay.io/prometheus/prometheus:v2.26.0\n    volumes:\n      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml\n      - ./rules:/etc/prometheus/rules\n    command: \"--config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus\"\n    ports:\n      - 9090:9090\n```\n\nprometheus.yml config를 살펴보면 수집주기는 1분으로, 나머지 얼럿매니저와 수집할 대상인 exporter들의 target endpoint를 추가하여 실행할 예정이다. 여기서는 시놀로지 SNMP 서비스와 통신하기 위해 SNMP Exporter의 target을 실제 사용중인 시놀로지 IP를 등록하도록 한다.  \n\n```yaml\nglobal:\n  scrape_interval:     1m\n  evaluation_interval: 1m\n\nalerting:\n  alertmanagers:\n  - static_configs:\n    - targets: ['alertmanager:9093']\n\nrule_files:\n  - \"/etc/prometheus/rules/*\"\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n        labels:\n          group: 'prometheus'\n  - job_name: node\n    static_configs:\n    - targets: ['node-exporter:9100']\n  - job_name: 'snmp-exporter'\n    static_configs:\n    - targets: ['192.168.0.100']\n    metrics_path: /snmp\n    params:\n      module: [synology]\n    relabel_configs:\n      - source_labels: [__address__]\n        target_label: __param_target\n      - source_labels: [__param_target]\n        target_label: instance\n      - target_label: __address__\n        replacement: 192.168.0.100:9116  # The SNMP exporter's real synology hostname:port. \n```\n\n## Stack 실행\n\n모든 설정이 완료되면 일단 기본적인 실행이 가능한 상태로 시놀로지 CLI를 실행한다. 기본적으로 시놀로지는 admin계정으로 로그인해서 sudo 권한을 통해 Docker Compose 실행을 해야 한다.  \n\n```sh\nssh admin@192.168.31.7\nsudo -i\n```\n\n[https://github.com/ddiiwoong/synology-prometheus.git](https://github.com/ddiiwoong/synology-prometheus.git)을 clone해서 /snmp-synology/snmp.yml 파일의 community 값을 본인의 synology 값으로 변경한 후 Docker Compose를 데몬형태로 실행한다.  \n\n```sh\ndocker-compose up -d\n```\n\n본인의 `http://<시놀로지 IP>:9090/targets` 으로 접속해서 3개의 target이 정상인지 확인한다. 그리고 표현식 브라우저로 돌아와서 `node_cpu_seconds_total` 과 `diskModel`을 통해 두개의 exporter에서 데이터가 수집되고 있는지 확인할 수 있다. `diskModel`은 시놀로지에 장착된 하드디스크 모델 정보를 보여주는 메트릭이다.  \n\n![diskmodel](/img/synology/diskmodel.png)\n\n그라파나 대시보드는 `http://<시놀로지 IP>:3000` 로 접속해서 초기 계정인 `admin:admin`을 입력하면 다음과 같이 기본 화면을 볼수 있다. 여기서 좌측의 기어모양 메뉴(Configuration)의 `Data Sources`를 클릭하고 프로메테우스 데이터소스를 등록한다.  \n\nDocker Compose로 실행할 때 아래와 같이 `prometheus` 이름으로 실행했기 때문에 HTTP URL에 `prometheus:9090`을 입력하고 `Save & Test`로 데이터 소스를 저장한다.  \n\n![grafana](/img/synology/grafana.png)\n\n## 시놀로지 대시보드 구성\n\n마지막으로 [https://grafana.com/grafana/dashboards/14284](https://grafana.com/grafana/dashboards/14284)의 대시보드를 추가하는 과정을 진행한다.  \n\n좌측의 + 메뉴(Create)의 `Import`를 클릭하고 위 대시보드 ID `14284`를 입력하고 등록하면 아래와 같이 시놀로지의 모니터링 대시보드를 확인할 수 있다. \n\n![synology](/img/synology/synology.png)\n\n## 정리\n\n간단(?)하게 시놀로지 나스를 모니터링하는 방법에 대해서 정리해봤다. 누군가에게는 재미로 누군가에게는 도움이 되길 바라며, 포스팅과 관련된 모든것은 개인 취미로 작성된 내용으로 회사나 나스와 관련된 제품과 관계가 없음을 알린다."
    },
    {
      "id": "kubernetes/vSphere-with-Tanzu-homelab-2/",
      "metadata": {
        "permalink": "/kubernetes/vSphere-with-Tanzu-homelab-2/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2020-12-06-vSphere-with-Tanzu-homelab-2.md",
        "source": "@site/blog/2020-12-06-vSphere-with-Tanzu-homelab-2.md",
        "title": "vSphere with Tanzu homelab running on ASUS PN50 (2/2)",
        "description": "이번 포스트에서는 Tanzu Cluster를 단일 홈랩 vSphere 서버에 올려본다",
        "date": "2020-12-06T00:00:00.000Z",
        "formattedDate": "December 6, 2020",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Tanzu",
            "permalink": "/tags/tanzu"
          },
          {
            "label": "vSphere",
            "permalink": "/tags/v-sphere"
          },
          {
            "label": "Homelab",
            "permalink": "/tags/homelab"
          },
          {
            "label": "ASUS",
            "permalink": "/tags/asus"
          },
          {
            "label": "PN50",
            "permalink": "/tags/pn-50"
          }
        ],
        "readingTime": 17.815,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "vSphere with Tanzu homelab running on ASUS PN50 (2/2)",
          "comments": true,
          "classes": "wide",
          "description": "이번 포스트에서는 Tanzu Cluster를 단일 홈랩 vSphere 서버에 올려본다",
          "slug": "kubernetes/vSphere-with-Tanzu-homelab-2/",
          "date": "2020-12-06T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "Tanzu",
            "vSphere",
            "Homelab",
            "ASUS",
            "PN50"
          ]
        },
        "prevItem": {
          "title": "Synology monitoring with Prometheus and snmp exporter",
          "permalink": "/prometheus/synology-prometheus/"
        },
        "nextItem": {
          "title": "vSphere with Tanzu homelab running on ASUS PN50 (1/2)",
          "permalink": "/kubernetes/vSphere-with-Tanzu-homelab/"
        }
      },
      "content": "지난글에 이어 이번에는 네트워크 구성하는 방법과 vSphere Cluster에 추가적인 Kubernetes 클러스터를 배포하는 것에 대해 이야기 하고자 한다. 이번 글도 지난글과 동일하게 [virtuallyGhetto](https://www.virtuallyghetto.com/) 블로그 포스팅을 참고하여 그대로 진행한 내용이라고 보면 된다.\n\n[https://www.virtuallyghetto.com/2020/11/complete-vsphere-with-tanzu-homelab-with-just-32gb-of-memory.html](https://www.virtuallyghetto.com/2020/11/complete-vsphere-with-tanzu-homelab-with-just-32gb-of-memory.html)\n\n지난 포스팅은 다음 링크에서 확인할 수 있다.\n> 이전글 - [vSphere with Tanzu homelab running on ASUS PN50 (1/2)](https://ddii.dev/kubernetes/vSphere-with-Tanzu-homelab/)\n\n<!--truncate-->\n\n\n![vcsa](/img/vcsa.png)\n\n## VDS(vSphere Distributed Switch) 구성\n\n이번 구성의 경우에는 usb nic를 하나만을 사용해서 진행하기 때문에 위에서 설정했던 3개의 네트워크(Management, Frontend, Workload)로 분리하기 위해 VDS를 신규로 생성하고 기존 네트워크 어댑터를 변경하는 작업을 수행해야 한다.  \n\n먼저 신규 구성한 vCenter UI([https://vcsa.tanzu.local/ui](https://vcsa.tanzu.local/ui))로 접속해 `Networking` 메뉴로 이동해서 `VDS` 메뉴 우클릭 후 `Add and Manage Hosts`를 선택한다. \n\n![network](/img/vcsa_network_menu.png)\n\nVDS 설정하는 6단계를 진행한다. \n\n* 1단계 -  `Add hosts`를 선택한다.\n* 2단계 - ESXi host를 선택한다. \n* 3단계 - pyhysical adapter를 생성한 후에 선택하게 되는데 구성한 랩에서는 USB TYPE NIC를 사용하여 `vusb0` 어댑터를 선택하고 `Assign uplink`에서 `dvUplink1`를 Assign한다.\n* 4단계 - VMkernel adapter에서 vmk0을 선택하고 `Assign port group`에서 Network를 Management를 Assign한다.\n* 5단계 - VM Migration은 생략한다.\n* 6단계 - 설정되는 내용을 확인하고 Finish를 하게 되면 기존 VCSA의 기본 네트워크가 `vSwitch`에서 `VDS`로 변경되었기 때문에 VSCA Client 접속이 끊어지게 된다. \n\n다시 ESXi Client로 접속해서 VCSA Port Group(네트워크)을 아래와 같이 기존 `VM Network`에서 `Management`로 변경하면 다시 VSCA Client로 접속이 가능하게 된다.\n\n![network_adapter](/img/vcsa_network.png)\n\n## RouterVM의 Network Port Group 추가(Frontend, Workload)\n\n이전 포스팅에서 생성한 Router VM은 구성할 내부 네트워크(10.10.0.0/24 & 10.20.0.0/24)로의 라우팅과 외부 네트워크로 NAT 및 포워딩을 위해서 필요하기 때문에 vSphere 또는 VCSA Client에서 `router.tanzu.local`의 네트워크 어댑터를 `Frontend`, `Workload`로 추가하는 작업이 필요하다.\n\n![routervm](/img/routervm.png)\n\n구성을 완료한 후에 간단한 네트워크 구성을 보면 다음과 같다.  \n\n![vswitch](/img/vswitch.png)\n\n물리 인터페이스 `vusb0` 아래 가상 스위치가 3개 구성되어 있고 외부에서도 RouterVM을 통해 접속이 가능하도록 접속하는 로컬 머신에서 라우팅 경로 추가가 필요하다.\n\n* Windows  \n  `route print` 명령으로 인터페이스 ID를 먼저 확인하고 아래와 같이 설정한다.\n\n  ```powershell\n  route ADD 10.10.0.0 MASK 255.255.255.0 192.168.0.201 METRIC 3 IF 5\n  route ADD 10.20.0.0 MASK 255.255.255.0 192.168.0.201 METRIC 3 IF 5\n  ```\n\n* MacOS\n  ```sh\n  sudo route -n add -net 10.10.0.0/24 192.168.0.201\n  sudo route -n add -net 10.20.0.0/24 192.168.0.201\n  ```\n\n* WSL\n  ```sh\n  sudo route -n add -net 10.10.0.0/24 gw <Gateway IP>\n  sudo route -n add -net 10.20.0.0/24 gw <Gateway IP>\n  ```\n\n## HAProxyVM 설정\n\n이전 포스팅에서 생성한 HAProxy도 추가된 네트워크 환경으로 설정을 하기 위해 다음 스크립트를 사용한다.   \n\n* [https://github.com/lamw/vsphere-with-tanzu-homelab-scripts/blob/master/deploy_3nic_haproxy.ps1](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts/blob/master/deploy_3nic_haproxy.ps1)\n\n구성한 환경에 맞게 스크립트내 변수들을 수정한 후 실행한다.\n\n```powershell\n$HAProxyOVA = \"C:\\esxi\\vmware-haproxy-v0.1.8.ova\"\n\n$Cluster = \"Tanzu-Cluster\"\n$VMHost = \"esxi-01.tanzu.local\"\n$Datastore = \"datastore1\"\n\n$HAProxyDisplayName = \"haproxy.tanzu.local\"\n$HAProxyHostname = \"haproxy.tanzu.local\"\n$HAProxyDNS = \"192.168.0.201\"\n$HAProxyManagementNetwork = \"Management\"\n$HAProxyManagementIPAddress = \"192.168.0.203/24\" # Format is IP Address/CIDR Prefix\n$HAProxyManagementGateway = \"192.168.0.1\"\n$HAProxyFrontendNetwork = \"Frontend\"\n$HAProxyFrontendIPAddress = \"10.10.0.2/24\" # Format is IP Address/CIDR Prefix\n$HAProxyFrontendGateway = \"10.10.0.1\"\n$HAProxyWorkloadNetwork = \"Workload\"\n$HAProxyWorkloadIPAddress = \"10.20.0.2/24\" # Format is IP Address/CIDR Prefix\n$HAProxyWorkloadGateway = \"10.20.0.1\"\n$HAProxyLoadBalanceIPRange = \"10.10.0.64/26\" # Format is Network CIDR Notation\n$HAProxyOSPassword = \"VMware1!\"\n$HAProxyPort = \"5556\"\n$HAProxyUsername = \"wcp\"\n$HAProxyPassword = \"VMware1!\"\n...\n```\n\n```powershell\n./deploy_3nic_haproxy.ps1\n```\n\n이후 HAProxy VM내에서 여러 인터페이스에서 `Reverse Path Filtering`을 `Disable`하기 위해 다음 스크립트를 실행한다. \n\n*[setup_haproxy.sh](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts/blob/master/setup_haproxy.sh)  \n\n```sh\n#!/bin/bash\n\ntouch /etc/sysctl.d/999-tanzu.conf\nchmod +x /etc/sysctl.d/999-tanzu.conf\n\nIFS=$'\\n'\nfor i in $(sysctl -a | grep rp_filter | grep 1);\ndo\n    SYSCTL_SETTING=$(echo ${i} | awk '{print $1}')\n    # Update live system\n    sysctl -w ${SYSCTL_SETTING}=0\n    # Persist settings upon reboot\n    echo \"${SYSCTL_SETTING}=0\" >> /etc/sysctl.d/999-tanzu.conf\ndone\n```\n\n> `Reverse Path Filtering`는 기본적으로 하나의 인스턴스에서 여러개의 인터페이스로 들어오는 패킷의 소스IP에 대해서 라우팅 테이블을 확인한 후 패킷이 들어온 동일한 인터페이스로 다시 나가는지 확인하는 것을 말한다. `rp_filter`가 1로 설정이 되어있으면, 특정 인터페이스로 들어온 패킷 정보와 OS의 FIB(Forward Information Base)에 정의된 정보와 일치하지 않을경우, 들어온 패킷을 버리는 기능을 하기 때문에 위 스크립트를 통해 rp_filter 값을 모두 0으로 변경하는 것이다. 자세한 내용은 [https://access.redhat.com/solutions/53031](https://access.redhat.com/solutions/53031)를 참고하자.\n\n## Content Library 구성\n\n`Content Library`는 레포지토리의 개념으로 여러 vApp 템플릿과 ISO 이미지 등과 같은 파일들과 컨테이너 개체를 저장하고 관리할 수 있는 도구이다. 레지스트리(Registry)와 유사하지만 `Content Library`에는 VM templates, OVA, OVF 파일 등이 저장된다.  \n\n`Menu` -> `Content Libraries`로 이동해서 새로운 라이브러리를 추가를 한다. 이름과 vCenter Server를 선택하고 2번째 단계에서 Subscribed content library를 선택하고 `Subscription URL`에 https://wp-content.vmware.com/v2/latest/lib.json 를 입력하고 3번째 단계에서 저장될 데이터스토어를 선택하면 된다.  \n\n실제 위 URL은 `OVF Template` 여러개를 다운받는데 Tanzu Cluster내에서 Master와 Worker Node들로 쓰일 Kubernetes 1.16 ~ 1.18까지 여러개의 VM이미지 (photon OS)들이 추가가 되는것을 알 수 있다. 아래 그림을 보면 현재 Tanzu에서 사용되는 Kubernetes 클러스터 버전이 1.18.5라는 것을 알 수 있다.\n\n![tkg](/img/content_tkg.png)\n\n## Workload Management 구성\n\nWorkload Management란 쉽게 생각하면 vSphere 기반에서 Kubernetes 클러스트를 프로비저닝하는 역할을 하는 도구라 보면 된다.  \n\nWorkload Management는 다음과 같은 역할을 한다.  \n\n* Kubernetes 클러스터의 컴퓨팅, 스토리지 리소스에 대한 사이징 및 네임스페이스 관리\n* Kubernetes 클러스터의 네트워크 및 DNS, NTP 및 VDS같은 네트워크 관리\n* Kubernetes 클러스터 상세 정보 (Node 개수 및 상태 정보)\n\nWorkload Management에서 클러스터 생성을 스크립트로 진행하기 전에 PowerCLI를 통해 접속 테스트를 한다. \n\n```powershell\nConnect-VIServer -Server vcsa.tanzu.local -User administrator@vsphere.local -Password VMware1!\nConnect-CisServer -Server vcsa.tanzu.local -User administrator@vsphere.local -Password VMware1!\n```\n\n그리고 PowerCLI Workload Management 모듈을 설치하고 여러 환경변수들을 설정한다. 기존에 사용한 정보들과 위에서 만든 Content Library 이름으로 변경하고 실행한다.  \n\n```powershell\nImport-Module VMware.WorkloadManagement\n\n$vSphereWithTanzuParams = @{\n    ClusterName = \"Tanzu-Cluster\";\n    TanzuvCenterServer = \"vcsa.tanzu.local\";\n    TanzuvCenterServerUsername = \"administrator@vsphere.local\";\n    TanzuvCenterServerPassword = \"VMware1!\";\n    TanzuContentLibrary = \"TKG\"; # Content Library 이름\n    ControlPlaneSize = \"TINY\";\n    MgmtNetwork = \"Management\";\n    MgmtNetworkStartIP = \"192.168.0.220\";\n    MgmtNetworkSubnet = \"255.255.255.0\";\n    MgmtNetworkGateway = \"192.168.0.1\";\n    MgmtNetworkDNS = @(\"192.168.0.201\");\n    MgmtNetworkDNSDomain = \"tanzu.local\";\n    MgmtNetworkNTP = @(\"162.159.200.123\");\n    WorkloadNetwork = \"Workload\";\n    WorkloadNetworkStartIP = \"10.20.0.10\";\n    WorkloadNetworkIPCount = 20;\n    WorkloadNetworkSubnet = \"255.255.255.0\";\n    WorkloadNetworkGateway = \"10.20.0.1\";\n    WorkloadNetworkDNS = @(\"192.168.0.201\");\n    WorkloadNetworkServiceCIDR = \"10.96.0.0/24\";\n    StoragePolicyName = \"Tanzu-Storage-Policy\";\n    HAProxyVMvCenterServer = \"vcsa.tanzu.local\";\n    HAProxyVMvCenterUsername = \"administrator@vsphere.local\";\n    HAProxyVMvCenterPassword = \"VMware1!\";\n    HAProxyVMName = \"haproxy.tanzu.local\";\n    HAProxyIPAddress = \"192.168.0.203\";\n    HAProxyRootPassword = \"VMware1!\";\n    HAProxyPassword = \"VMware1!\";\n    LoadBalancerStartIP = \"10.10.0.64\";\n    LoadBalancerIPCount = 64\n}\nNew-WorkloadManagement2 @vSphereWithTanzuParams\n```\n\n클러스터를 생성하는 시간은 약 40분 이상 소요되는데 Tanzu-Cluster의 Control Plane Node (Master Node) 생성이 완료되면 IP가 10.10.0.64(HAProxy Load Balancer에서 할당된 첫번째 주소)로 할당이 되는 것을 확인할 수 있다.  \n\n다음은 Namespace를 생성하는 과정을 진행한다. `Workload Management`에서 `NEW NAMESPACE`메뉴를 클릭하고 위에서 생성한 `Tanzu-Cluster`를 선택하고 이름(test)을 입력하고 Namespace를 생성한다. \n\n![tkg_permission](/img/tkg_permission.png)\n\n## Cluster 접속  (Kubernetes CLI)\n\n생성된걸 확인한 후 브라우저에 해당 클러스터의 Control Plane IP (https://10.10.0.64)로 접속하면 OS별(Linux, MacOS, Windows)로 CLI 플러그인을 설치할 수 있다.  \n\n설치 및 PATH 설정을 진행하고 아래와 같이 Control Plane으로 CLI 로그인을 하면 현재 설정된 Context 확인이 가능하다. \n\n```sh\n> kubectl vsphere login --server=10.10.0.64 -u administrator@vsphere.local --insecure-skip-tls-verify\n\n        🥳vSphere with Tanzu Basic Cluster enabled by William Lam's Script 🥳\n\nPassword:\nLogged in successfully.\n\nYou have access to the following contexts:\n   10.10.0.64\n   docker-desktop\n   nexclipper\n   test\n\nIf the context you wish to use is not in this list, you may need to try\nlogging in again later, or contact your cluster administrator.\n\nTo change context, use `kubectl config use-context <workload name>`\n```\n\n```sh\n> kubectl config use-context nexclipper\nSwitched to context \"nexclipper\".\n\n> kubectl get nodes\nNAME                               STATUS   ROLES    AGE     VERSION\n422f46864113e3de3361943d14073728   Ready    master   4d21h   v1.18.2-6+38ac483e736488\n422faddc15554b09d82907c0dfb57752   Ready    master   4d21h   v1.18.2-6+38ac483e736488\n```\n\nnexclipper namespace로 context를 전환하고 node를 확인하면 master node를 2개 확인할 수 있는데 이는 `nexclipper`라는 네임스페이스를 총괄하는 Super Master(Control Plane) 같은 개념이라고 볼 수 있다. 처음에 클러스터 배포전에 Namespace 리소스를 만들길래 다소 헷갈리긴 했지만 Tanzu에서의 Namespace는 기존 쿠버네티스의 리소스를 논리적으로 나누는 컨셉이 아닌, 정해진 리소스와 유저와 그룹의 권한을 먼저 Namespace로 할당하고 그담에 다시 조직이나 어플리케이션 별로 Kubernetes Workload를 다시 제공하는 컨셉이라고 보면된다.  \n\n최근에 `SKT`나 `Kakao Enterprise` 처럼 쿠버네티스 클러스터의 라이프사이클을 쿠버네티스로 관리하는 컨셉으로 이해하면 좋다. 아래 그림을 참고하면 쉽게 이해할 수 있을 것이다.  \n\n![tanzu](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/img/GUID-DF8905E1-C098-4882-BFC1-B0BAC668B424-high.png)  \n참조 : https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/img/GUID-DF8905E1-C098-4882-BFC1-B0BAC668B424-high.png\n\nTanzuKubernetesCluster CRD를 통해 클러스터를 신규로 생성한다.  \n`namespace: nexclipper`로 설정하고 원하는 배포 버전과 이름, CNI, Control Plane, Worker Node 개수 등을 기재하게 되는데 특이한 것은 CNI는 [Antrea](https://github.com/vmware-tanzu/antrea)를 사용하는것을 확인할 수 있다.  \n\n```yaml\napiVersion: run.tanzu.vmware.com/v1alpha1\nkind: TanzuKubernetesCluster\nmetadata:\n  name: nexclipper-cluster\n  namespace: nexclipper\nspec:\n  distribution:\n    version: v1.17.8+vmware.1-tkg.1.5417466\n  settings:\n    network:\n      cni:\n        name: antrea\n      pods:\n        cidrBlocks:\n        - 193.0.2.0/16\n      serviceDomain: managedcluster.local\n      services:\n        cidrBlocks:\n        - 195.51.100.0/12\n  topology:\n    controlPlane:\n      class: best-effort-xsmall\n      count: 1\n      storageClass: tanzu-storage-policy\n    workers:\n      class: best-effort-xsmall\n      count: 1\n      storageClass: tanzu-storage-policy\n```\n\n신규 TKG 클러스터를 배포한다. \n\n```sh\n> kubectl apply -f tkc.yaml  \n\ntanzukubernetescluster.run.tanzu.vmware.com/nexclipper-cluster created\n```\n\n배포하고 tanzukubernetescluster 를 조회해보면 Control Plane 1개, Worker Node 1개의  v1.17.8 버전의 클러스터가 배포된것을 알 수 있다. \n\n```\n> kubectl get tanzukubernetescluster\nNAME                 CONTROL PLANE   WORKER   DISTRIBUTION                     AGE   PHASE\nnexclipper-cluster   1               1        v1.17.8+vmware.1-tkg.1.5417466   33m   running\n```\n\n생성한 tanzukubernetescluster로 접속하여 클러스터 정보를 확인한다.\n\n```\n> kubectl-vsphere login --server=10.10.0.64 -u administrator@vsphere.local --insecure-skip-tls-verify --tanzu-kubernetes-cluster-name nexclipper-cluster --tanzu-kubernetes-cluster-namespace nexclipper\n\n> kubectl config use-context nexclipper-cluster\nSwitched to context \"nexclipper-cluster\".\n\n> kubectl get nodes\nNAME                                                STATUS   ROLES    AGE     VERSION\nnexclipper-cluster-control-plane-95nhv              Ready    master   3h10m   v1.17.8+vmware.1\nnexclipper-cluster-workers-kp68q-78df45db5c-jzmmv   Ready    <none>   3h6m    v1.17.8+vmware.1\n```\n\n생성한 클러스터에 간단한 앱을 배포한다.  \n[Kubedoom](https://github.com/storax/kubedoom)은 Doom 게임내에서 애플리케이션(괴물)을 일부러 종료시켜 복원성을 보게하는 재미있는 앱이다.\n\n```\n> git clone https://github.com/storax/kubedoom.git\n> cd kubedoom\n> kubectl apply -f manifest/\n> kubectl -n kubedoom port-forward deployment/kubedoom 5900:5900\nForwarding from 127.0.0.1:5900 -> 5900\nForwarding from [::1]:5900 -> 5900\n```\n\n배포한후에 포트포워딩을 한 앱으로 VNC viewer로 접속하면 재미있는 화면을 볼 수 있을 것이다. VNC 패스워드는 `idbehold`이다.\n\n![doom](/img/doom.png)\n\n## Tanzu 장단점 (개인의견)\n\n하드웨서 고민, 구매, ESXi 설치, vCenter, TKG을 설치하고 아키텍처들을 찾아보면서 몇가지 장단점이 눈에 보였는데 가장 큰 장점은 하나의 네임스페이스를 만들어 Super Master(Control Plane)를 먼저 구성하고 자원(컴퓨팅, 네트워크, 스토리지 등)과 기존에 보유하고 있는 권한 체계로 할당을 한 뒤 필요할때 마다 특정 팀이나 조직에게 별도의 클러스터를 할당하는 아키텍처를 추구하고 있어 VMware 환경에서 Kubernetes를 신규 도입하고 엔터프라이즈에 적용하기 용이하다는 점이다.  \n\n또한 기본적으로 갖춰야 하는 Security Policy나 리소스 할당이 클러스터 구축이후에 되는 것이 아니라 이미 정의된 규칙으로 클러스터 구성이 이후에 진행되기 때문에 관리적인 측면에서도 장점이 있다고 볼 수 있다. \n\n하지만 개인적으로 생각하는 단점들도 몇가지 있는데 첫번째는 용어의 중복으로 인한 혼동이 있었다는점이다. 네임스페이스나 기존 쿠버네티스에서 많이 사용되는 용어보다는 VMware에서 만든 CRD등을 미리 학습하거나 알고 있지 않으면 기존 경험이 있는 유저도 접근성이 떨어질 것 같다는 생각이 들었다. \n\n비용 측면에서는 일단 `vCenter` 이외에 `Workload Management` 라이센스가 별도로 필요하기 때문에 자세히 알아보진 않았지만 적지 않은 비용이 들 것으로 예상된다. 게다가 많은 구성요소로 인한 자원이 소요가 있어 하나의 작은 클러스터만 띄웠음에도 64GB 머신의 메모리 사용량이 거의 60GB에 육박했기 때문에 기존 인프라 비용에 추가적인 비용에 대한 고려가 반드시 필요할 것 같다. \n\n위에도 언급했지만 러닝커브에 대한 부분도 문제점이 될 수 있을것이다. NSX기반의 새로운 CNI뿐만 아니라 기존 VMware 운영을 하던 DevOps팀이나 인프라 운영조직의 러닝커브, 추상화 레벨이 높아져서 내가 원하는 클러스터의 이벤트나 로그를 확인할 때 탐색적으로 메뉴를 찾기가 쉽지 않은 부분들이 있었다. 그래서 트러블슈팅 때에도 로그의 원천을 찾는데 시간이 다소 걸리기도 했다. \n\n\n## 정리\n\n한 일주일동안 스토리지 오류로 씨름하다가 겨우 앱을 배포했지만 거의 2주동안 재미있게 구성해볼수 있었다. 솔직히 이야기하자면 개인 클러스터로 구성하는데는 추천하지 않지만 Tanzu를 공부하고 운영해야 하는 경우나 자격증을 취득하는 부분이 필요하다면 한번쯤은 해볼 가치가 있다고 생각한다.  \n\n거의 1년만에 포스팅을 작성했는데 다음 포스팅에는 Security Policy 관련한 테스트를 진행하면서 연말을 정리해볼 생각이다."
    },
    {
      "id": "kubernetes/vSphere-with-Tanzu-homelab/",
      "metadata": {
        "permalink": "/kubernetes/vSphere-with-Tanzu-homelab/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2020-11-30-vSphere-with-Tanzu-homelab.md",
        "source": "@site/blog/2020-11-30-vSphere-with-Tanzu-homelab.md",
        "title": "vSphere with Tanzu homelab running on ASUS PN50 (1/2)",
        "description": "이번 포스트에서는 Tanzu Cluster를 단일 홈랩 vSphere 서버에 올려본다",
        "date": "2020-11-30T00:00:00.000Z",
        "formattedDate": "November 30, 2020",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Tanzu",
            "permalink": "/tags/tanzu"
          },
          {
            "label": "vSphere",
            "permalink": "/tags/v-sphere"
          },
          {
            "label": "Homelab",
            "permalink": "/tags/homelab"
          },
          {
            "label": "ASUS",
            "permalink": "/tags/asus"
          },
          {
            "label": "PN50",
            "permalink": "/tags/pn-50"
          }
        ],
        "readingTime": 18.245,
        "truncated": true,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "vSphere with Tanzu homelab running on ASUS PN50 (1/2)",
          "comments": true,
          "classes": "wide",
          "description": "이번 포스트에서는 Tanzu Cluster를 단일 홈랩 vSphere 서버에 올려본다",
          "slug": "kubernetes/vSphere-with-Tanzu-homelab/",
          "date": "2020-11-30T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "Tanzu",
            "vSphere",
            "Homelab",
            "ASUS",
            "PN50"
          ]
        },
        "prevItem": {
          "title": "vSphere with Tanzu homelab running on ASUS PN50 (2/2)",
          "permalink": "/kubernetes/vSphere-with-Tanzu-homelab-2/"
        },
        "nextItem": {
          "title": "2019 Retrospective",
          "permalink": "/retrospective/2020plan/"
        }
      },
      "content": "스타트업으로 이직후 정신없는 6개월을 보냈고 집에서 하는 사이드 프로젝트나 공부하는 것들이 소홀해지는것 같아 마음을 다잡고자 작성하는 포스팅이다. 최근 몇개월 간은 회사 블로그 글만 작성하다보니 개인 블로그를 거의 못하고 있어서 회사 제품과는 아직까지 거리가 있는 플랫폼 공부차 작성한다. 순수하게 정보공유차 작성하는 것이며, 특정회사의 회사의 제품이나 플랫폼을 홍보하고자 함은 아니다.\n\n<!--truncate-->\n\n## Tanzu\n\n[https://tanzu.vmware.com/content/blog/simplify-your-approach-to-application-modernization-with-4-simple-editions-for-the-tanzu-portfolio](https://tanzu.vmware.com/content/blog/simplify-your-approach-to-application-modernization-with-4-simple-editions-for-the-tanzu-portfolio)  \n\nTanzu는 애플리케이션을 개발, 구동, 운영하는 모든 컨테이너 환경을 통합 관리하는 쿠버네티스를 상용화 한 제품으로 이해하는 것이 가장 쉽다. \n\n![tanzu](/img/tanzu.png)  \n(출처 : VM웨어 코리아 발표자료 갈무리)\n\n포트폴리오를 간단히 살펴보면 기존 인프라를 구성하는 레이어인 vSphere 위에 `VMware Tanzu Kubernetes Grid`를 볼 수 있다.  \n\n오늘은 기존 Pivotal 이나 Spring Boot를 제외한 플랫폼 레이어의 TKG(Tanzu Kubernetes Grid)를 홈랩으로 구성하는 과정을 정리했다. \n\n모든 과정은 virtuallyGhetto 블로그를 참고하여 작성했고 가장 따라한 부분은 아래 포스팅이니 NUC으로 구성하려면 원문을 참고하면 된다.  \n[https://www.virtuallyghetto.com/2020/11/complete-vsphere-with-tanzu-homelab-with-just-32gb-of-memory.html#comments](https://www.virtuallyghetto.com/2020/11/complete-vsphere-with-tanzu-homelab-with-just-32gb-of-memory.html#comments)\n\n## 준비사항(Hardware)\n\n* ASUS PN50  \n  [https://www.asus.com/kr/Mini-PCs/Mini-PC-PN50/](https://www.asus.com/kr/Mini-PCs/Mini-PC-PN50/)\n\n  PN50 베어본을 선택한 이유는 일단 AMD Ryzen 4800u CPU가 저전력인데도 불구하고 8코어 16스레드, 64GB sodimm이 지원되며, 2.5인치와 M.2 SSD가 동시 장착이 가능하다는 점이다. 온보드 NIC 드라이버가 vSphere 7.0에서 지원되지 않지만 USB Network Adapter를 통해 구성이 가능했기 때문에 망설임 없이 구매를 결정했다. 구입은 네이버 검색(asus pn50 4800u)을 통해 71만원 정도에 구매를 했다. 이도 스마일 캐시를 직접 구매해서 실제 구매금액은 65만원 정도가 소요되었다.\n\n* SODIMM DDR4 32G PC4-25600 (TeamGroup)  \n  해당 제품은 가성비로 가장 저렴하게 살 수 있는 국내 워런티가 가능한 SODIMM 메모리로 32G 2개를 구매했으며 네이버 검색으로 2개 25만원 정도로 구매했다.\n\n* 2.5 SSD (ADATA SU655 240GB)  \n  ESXi 서버 설치용으로 사용하기 위해 기존에 갖고 있던 ADATA SU655 240GB를 사용했고 현재 3.5만원 정도에 구매가능하다. \n\n* M.2 SSD (WD BLACK SN750 M.2 NVMe 500GB)  \n  아마존에서 직구한 녀석으로 69.99달러 소요되었다. 배대지 가격을 합치면 8.5만원 정도 소요되었다. \n\n* Gigabit USB Lancard (tplink UE300C)  \n  ESXi 7.0버전에서 온보드 NIC 인식이 불가능한 이유로 별도로 기기비트 유선 랜카드를 구매했다. 1.5만원 정도에 구매가능 하고 3년 보증이 되는 TPLINK제품으로 선택하였다.  \n  [https://coupa.ng/bM0lN8](https://coupa.ng/bM0lN8) -> `해당 링크를 통해 구매하면 저에게 쿠팡 파트너스를 통해 적립금이 지급되니 필요하신 분은 아래를 통해 구매를 부탁드립니다`\n\n실제 위 스펙으로 대충 구매를 진행하게 되면 `104만원` 정도 소요가 될것으로 예상된다. 환율이나 할인을 전혀 받지 않고 1개의 SSD로 구성한다고 가정해도 약 8코어 16스레드 64GB 머신을 `110만원` 정도로 구성이 가능하다. 물론 서버나 워크스테이션이나 Intel NUC기반으로도 구성이 가능하지만 전력소모와 집안을 차지하는 부피를 고려하면 금액적으로 메리트가 있다고 생각한다.\n\n## 준비사항(Software)\n\n* Windows PC (with Powershell, WSL) or MAC\nWindow를 사용을 하지 않고 MAC이나 리눅스로 구성하려 했으나 온보드 NIC 인식을 위한 드라이버 통합 커스텀 이미지 작성 편의를 위해 Window 환경을 사용하였다.\n\n* [ventoy](https://www.ventoy.net/en/index.html)  \n  ESXi 부팅 이미지 제작을 위한 툴이며 [rufus](https://rufus.ie/)에 비해 활용도가 높다.\n* [vSphere 7.0 Update 1](https://my.vmware.com/en/group/vmware/downloads/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/7_0)  \n  평가판은 [https://my.vmware.com/en/group/vmware/evalcenter?p=vsphere-eval-7](https://my.vmware.com/en/group/vmware/evalcenter?p=vsphere-eval-7) 에서 다운로드가 가능하다.\n* [PowerShell 7.10](https://github.com/PowerShell/PowerShell/releases/tag/v7.1.0)  \n  기본적으로 Window에는 6.x 버전의 PowerShell이 설치되어 있기 때문에 7.1.0 버전을 신규로 설치해야 한다.\n* [PowerCLI](https://www.powershellgallery.com/packages?q=VMware.PowerCLI)  \n  https://www.powershellgallery.com/packages/VMware.PowerCLI/12.1.0.17009493\n  커스텀 ESXi 7.0 이미지 제작을 위해 필요하다.\n* [USB Network Native Driver for ESXi](https://flings.vmware.com/usb-network-native-driver-for-esxi?download_url=https%3A%2F%2Fdownload3.vmware.com%2Fsoftware%2Fvmw-tools%2FUSBNND%2FESXi701-VMKUSB-NIC-FLING-40599856-component-17078334.zip)  \n  PN50 USB NIC(RTL8153) 인식을 위한 네이티브 드라이버.\n* [OVFTool](https://my.vmware.com/group/vmware/downloads/details?downloadGroup=OVFTOOL440P01&productId=974)  \n  OVA를 배포하기 위한 도구로 Linux 64 bundle로 WSL에 설치한다. \n* [vcsa-deploy](https://my.vmware.com/group/vmware/evalcenter?p=vsphere-eval-7)  \n  vCenter를 배포하는 도구로 이미지내에 포함되어 있기 때문에 vCenter Server Appliance ISO를 다운받아 압축을 풀면 확인할 수 있다. \n\n## 최종 예상 구성요소\n* vCenter Server Appliance\n* VMFS Storage\n* vSphere with Tanzu Cluster\n* HAProxy\n* Router (Forwarding, DNS)\n* Supervisor Control Plane VMs (Master 2ea, Worker 3ea)\n\n## ESXi 설치\n\n초반에도 이야기 했듯이 ESXi 7.0이상에서는 ASUS PN50의 온보드 인터페이스를 사용하지 못한다. 그래서 별도로 구매한 USB 인터페이스의 RTL8153 칩셋을 이미지에 통합시키는 작업이 필요하다.  \n\n드라이버 이미지 통합을 위한 내용은 아래 포스팅을 참고했다.  \n* [https://www.virten.net/2020/09/esxi-on-amd-ryzen-based-asus-pn50/](https://www.virten.net/2020/09/esxi-on-amd-ryzen-based-asus-pn50/)\n* [https://www.virten.net/2020/04/how-to-add-the-usb-nic-fling-to-esxi-7-0-base-image/](https://www.virten.net/2020/04/how-to-add-the-usb-nic-fling-to-esxi-7-0-base-image/)\n\n### ESXi 다운로드\n\n다음 링크에서 `VMware vSphere 7.0 Update 1` 를 다운로드 한다.  \n* [https://my.vmware.com/en/group/vmware/downloads/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/7_0](https://my.vmware.com/en/group/vmware/downloads/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/7_0)\n\n기본적으로 VMware 회원가입이 필요하며 라이센스가 없는 경우 60일 trial로 가능하다.  \n* [https://my.vmware.com/en/group/vmware/evalcenter?p=vsphere-eval-7](https://my.vmware.com/en/group/vmware/evalcenter?p=vsphere-eval-7)\n\n### USB Network Native Driver for ESXi\n\nUSB NIC Fling 드라이버를 준비한다. \n\n* [https://flings.vmware.com/usb-network-native-driver-for-esxi?download_url=https%3A%2F%2Fdownload3.vmware.com%2Fsoftware%2Fvmw-tools%2FUSBNND%2FESXi701-VMKUSB-NIC-FLING-40599856-component-17078334.zip\n](https://flings.vmware.com/usb-network-native-driver-for-esxi?download_url=https%3A%2F%2Fdownload3.vmware.com%2Fsoftware%2Fvmw-tools%2FUSBNND%2FESXi701-VMKUSB-NIC-FLING-40599856-component-17078334.zip)\n\n### PowerShell 7.1.0 설치\n\n기본적으로 Window에는 6.x 버전의 PowerShell이 설치되어 있기 때문에 특정 스크립트를 구동하기 위해서는 7.1.0 버전을 신규로 설치해야 한다.  \n\n* [https://github.com/PowerShell/PowerShell](https://github.com/PowerShell/PowerShell)\n\n\n### PowerCLI 준비\n\nPowerCLI 다운로드 및 설치\n* [https://www.powershellgallery.com/packages/VMware.PowerCLI/12.1.0.17009493](https://www.powershellgallery.com/packages/VMware.PowerCLI/12.1.0.17009493)\n\n관리자 권한으로 PowerShell을 실행하고 PowerCLI 모듈 설치전에 실행 권한을 수정한다.  \n* 참고 : [https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_execution_policies?view=powershell-7.1](https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_execution_policies?view=powershell-7.1)\n\n```powershell\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned\n```\n\nPowerCLI를 설치한다.\n```powershell\nInstall-Module -Name VMware.PowerCLI -Scope CurrentUser\n```\n\n### Image Profile 확인\n포스팅을 하는 20년 11월 29일 최신 ESXi 이미지는 `ESXi-7.0U1b-17168206-standard`로 vSphere ESXi 7.0 이미지 프로필은 아래에서 확인이 가능하다.  \n* [https://www.virten.net/vmware/vmware-esxi-image-profiles/](https://www.virten.net/vmware/vmware-esxi-image-profiles/)\n\nstable 릴리스로 구성하기 위해 이전 릴리스인 `ESXi-7.0.1-16850804-standard` 이미지 프로필로 설치를 진행한다.  \n\n설치 프로세스를 살펴보면 `Add-EsxSoftwareDepot`로 repo설정을 하고 `Export-ESXImageProfile`를 통해 원하는 이미지 프로필의 bundle을 다운로드 받는다. 위에서 다운로드 받은 드라이버를 `Add-EsxSoftwarePackage`로 패키지를 추가하고 새로운 ISO이미지를 생성하는 과정을 알 수 있다.  \n\n이미지 프로필 변경을 통해 원하는 이미지 버전으로 구성이 가능하다.  \n\n```powershell\nAdd-EsxSoftwareDepot https://hostupdate.vmware.com/software/VUM/PRODUCTION/main/vmw-depot-index.xml\nExport-ESXImageProfile -ImageProfile \"ESXi-7.0.1-16850804-standard\" -ExportToBundle -filepath  ESXi-7.0.1-16850804-standard.zip\nRemove-EsxSoftwareDepot https://hostupdate.vmware.com/software/VUM/PRODUCTION/main/vmw-depot-index.xml\nAdd-EsxSoftwareDepot .\\ESXi-7.0.0-15843807-standard.zip\nAdd-EsxSoftwareDepot .\\ESXi701-VMKUSB-NIC-FLING-40599856-component-17078334.zip\nNew-EsxImageProfile -CloneProfile \"ESXi-7.0.1-16850804-standard\" -name \"ESXi-7.0.1-16850804-USBNIC\" -Vendor \"virten.net\"\nAdd-EsxSoftwarePackage -ImageProfile \"ESXi-7.0.1-16850804-USBNIC\" -SoftwarePackage \"vmkusb-nic-fling\"\nExport-ESXImageProfile -ImageProfile \"ESXi-7.0.1-16850804-USBNIC\" -ExportToIso -filepath ESXi-7.0.1-16850804-USBNIC.iso\nExport-ESXImageProfile -ImageProfile \"ESXi-7.0.1-16850804-USBNIC\" -ExportToBundle -filepath ESXi-7.0.1-16850804-USBNIC.zip\n```\n\n### ESXi 설치 이미지 제작\n\nESXi 부팅 이미지 제작은 [rufus](https://rufus.ie/), [ventoy](https://www.ventoy.net/en/index.html) 둘 중 편한 도구로 선택하면 되는데 ventoy가 조금 더 활용도가 높다고 판단하여 진행했고, OS설치 USB 생성은 생략한다. \n\n### 부팅 및 설치 \n\n일반적인 바이오스 설정과 동일하게 Boot 우선순위를 USB로 설정하고 다음과 같은 ventoy화면에서 앞서 생성한 `ESXi-7.0.1-16850804-USBNIC.iso` 이미지로 ESXi 설치를 진행한다.\n\n![ventoy](https://nuanceofiqlusion.files.wordpress.com/2020/07/ventoymainmenu-e1594484932538.png)\nhttps://nuanceofiqlusion.files.wordpress.com/2020/07/ventoymainmenu-e1594484932538.png  \n\n\n이후 친숙한 DCUI(Direct Console User Interface)로 접속하여 F2모드로 접속하여 Static IP 설정과 network 테스트를 진행한다.  \n\n![esxi](/img/esxi7.png)\n\n\n### vSphere Web Client 접속\n\n설정한 Static IP로 접속하여 환경을 점검한다.  \n\n![nw](/img/esxi_network.png)\n\nManagement Network에 vSwitch0이 구성된 것을 확인할 수 있다.  \n\n그리고 설치한 M.2 SSD로 datastore2를 추가하여 vCenter 설치를 진행하도록 한다.\n\n![storage](/img/esxi_storage.png)\n\n## Router 설치 및 환경 구성\n\nRouter VM은 DNS 서버의 역할뿐 아니라 앞으로 구성될 내부 네트워크(10.10.0.0/24 & 10.20.0.0/24)와 외부 네트워크의 포워딩과 NAT 역할을 수행한다.  \n\nPhoton OS 3.0 OVA로 배포되며 [OVFTool](https://my.vmware.com/group/vmware/downloads/details?downloadGroup=OVFTOOL440P01&productId=974)를 사용하여 배포를 진행하는데 이미 작성된 배포 스크립트를 활용한다. \n\n* [Photon OS 3.0 OVA](https://packages.vmware.com/photon/3.0/Rev3/ova/photon-hw13_uefi-3.0-a383732.ova)\n* [https://github.com/lamw/vsphere-with-tanzu-homelab-scripts](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts.git)\n\n\n### Router 설치\n\n스크립트 실행은 동일한 PowerShell이 동작하는 Window WSL2 환경에서 수행했기 때문에 Window `hosts` 파일과 WSL환경의 `/etc/hosts` 파일에 아래와 같이 사용하고자 하는 IP주소를 미리 등록해놓는다.  \n\n```\n192.168.0.10    esxi-01.tanzu.local\n192.168.0.201   router.tanzu.local\n192.168.0.202   vcsa.tanzu.local\n192.168.0.203   haproxy.tanzu.local\n```\n\n[스크립트 레포](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts.git)의 `deploy_photon_router_ova.sh`를 자신의 환경에 맞게 수정한다. WSL내에서 Window 시스템내에 다운로드한 OVA파일 경로는 WSL내에서 실행될 것이기에 `/mnt/c/esxi/photon-hw13_uefi-3.0-a383732.ova`로 변경하였다. Router와 ESXi hostname, password등을 확인하고 Network과 Datastore는 위에서 확인한 내용으로 수정한다.\n\n```sh\n#!/bin/bash\n# William Lam\n# www.virtuallyghetto\n\nPhotonOVA=\"/mnt/c/esxi/photon-hw13_uefi-3.0-a383732.ova\"\nPhotonRouterVMName=\"router.tanzu.local\"\nESXiHostname=\"esxi-01.tanzu.local\"\nESXiUsername=\"root\"\nESXiPassword='VMware1!'\n\nPhotonRouterNetwork=\"VM Network\"\nPhotonRouterDatastore=\"datastore1\"\n\n### DO NOT EDIT BEYOND HERE ###\n\novftool \\\n--name=${PhotonRouterVMName} \\\n--X:waitForIpv4 \\\n--powerOn \\\n--acceptAllEulas \\\n--noSSLVerify \\\n--datastore=${PhotonRouterDatastore} \\\n--net:\"None=${PhotonRouterNetwork}\" \\\n${PhotonOVA} \\\n\"vi://${ESXiUsername}:${ESXiPassword}@${ESXiHostname}\"\n```\n\n스크립드를 실행하면 OVA파일을 배포하게 된다.  \n\n```sh\n$ ./deploy_photon_router_ova.sh\nOpening OVA source: /mnt/c/esxi/photon-hw13_uefi-3.0-a383732.ova\nThe manifest validates\nThe provided certificate is in valid period\nSource is signed and the certificate validates\nCertificate information:\n  CertIssuer:/C=US/ST=California/L=Palo Alto/O=VMware, Inc.\n  CertSubject:/C=US/ST=California/L=Palo Alto/O=VMware, Inc.\n  -----BEGIN CERTIFICATE-----\n  ...\n  -----END CERTIFICATE-----\n\n\nWarning:\n - Line -1: Unsupported value 'uefi.secureBoot.enabled' for attribute 'key' on element 'ExtraConfig'.\nOpening VI target: vi://root@esxi-01.tanzu.local:443/\nDeploying to VI: vi://root@esxi-01.tanzu.local:443/\nTransfer Completed\nPowering on VM: router.tanzu.local\nTask Completed\nReceived IP address: 192.168.0.47\nCompleted successfully\n```\n\n위와 같이 DHCP를 통해 임의의 주소(192.168.0.47)를 할당 받게 되고 해당 IP로 SSH접속하여 기본 패스워드(`changeme`)를 원하는 패스워드로 변경한다.\n\n```sh\n$ ssh root@192.168.0.47\nThe authenticity of host '192.168.0.47 (192.168.0.47)' can't be established.\nECDSA key fingerprint is SHA256:ESpN1DMTIu9PKWW4QZIYs4DZ602SsdhjxEYhugkmi+g.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '192.168.0.47' (ECDSA) to the list of known hosts.\nPassword:\n\ncat > /etc/systemd/network/10-static-eth0.network << EOF\nYou are required to change your password immediately (administrator enforced)\nChanging password for root.\nCurrent password:\nNew password:\nRetype new password:\n 08:58:54 up 32 min,  0 users,  load average: 0.00, 0.00, 0.00\ntdnf update info not available yet!\nroot@photon-machine [ ~ ]#\nroot@photon-machine [ ~ ]# exit\nlogout\nConnection to 192.168.0.47 closed.\n```\n\n### Router 환경 구성\n\n위 스크립트 [레포지토리](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts)에서 DNS역할, 포워딩을 수행하도록 [`setup_photon_router.sh`](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts/blob/master/setup_photon_router.sh)를 수정한다. \n\n* `PHOTON_ROUTER_IP`=192.168.0.201 : Router 설정할 IP\n* `PHOTON_ROUTER_GW`=192.168.0.1 : Router의 게이트웨이\n* `PHOTON_ROUTER_DNS`=192.168.0.1 : Router의 Forwader DNS\n* `SETUP_DNS_SERVER`=1 : Router가 클러스터 내 DNS 역할을 수행\n\n위 설정 이외에도 추후 내부 네트워크 구성을 위한 iptables 구성이 포함되어 있다.  \n\n```sh\n#!/bin/bash\n\nPHOTON_ROUTER_IP=192.168.0.201\nPHOTON_ROUTER_GW=192.168.0.1\nPHOTON_ROUTER_DNS=192.168.0.1\nSETUP_DNS_SERVER=1\n\ntdnf -y update\nif [ ${SETUP_DNS_SERVER} -eq 1 ]; then\n    tdnf install -y unbound\n\n    cat > /etc/unbound/unbound.conf << EOF\n    server:\n        interface: 0.0.0.0\n        port: 53\n        do-ip4: yes\n        do-udp: yes\n        access-control: 192.168.0.0/24 allow\n        access-control: 10.10.0.0/24 allow\n        access-control: 10.20.0.0/24 allow\n        verbosity: 1\n\n    local-zone: \"tanzu.local.\" static\n\n    local-data: \"router.tanzu.local A 192.168.0.201\"\n    local-data-ptr: \"192.168.0.201 router.tanzu.local\"\n\n    local-data: \"vcsa.tanzu.local A 192.168.0.202\"\n    local-data-ptr: \"192.168.0.202 vcsa.tanzu.local\"\n\n    local-data: \"haproxy.tanzu.local A 192.168.0.203\"\n    local-data-ptr: \"192.168.0.203 haproxy.tanzu.local\"\n\n    local-data: \"esxi-01.tanzu.local A 192.168.0.10\"\n    local-data-ptr: \"192.168.0.10 esxi-01.tanzu.local\"\n...\n```\n\n해당 스크립트를 수행하고 나면 IP가 변경되고 외부에서도 Router DNS를 통해 쿼리가 가능한것을 확인할 수 있다.  \n\n```powershell\nPS C:\\Users\\rokak> nslookup vmware.com 192.168.0.201\n서버:    router.tanzu.local\nAddress:  192.168.0.201\n\n권한 없는 응답:\n이름:    vmware.com\nAddresses:  2a02:e980:b5::b7\n            2a02:e980:b1::b7\n            45.60.101.183\n            45.60.11.183\n```\n\n### VCSA(vCenter Server Appliance) 배포 및 구성\n\n`vcsa-deploy`를 사용하여 배포를 진행한다. `vcsa-deploy`는 VCSA 이미지내에 바이너리 형태로 존재하기 때문에 먼저 VCSA 7.0 Update 1 ISO를 다운로드한다.  \n* [https://my.vmware.com/group/vmware/evalcenter?p=vsphere-eval-7](https://my.vmware.com/group/vmware/evalcenter?p=vsphere-eval-7)\n\n위 스크립트 [레포지토리](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts)에서 [`vcsa.tanzu.local.json`](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts/blob/master/vcsa.tanzu.local.json)를 수정한다.  \n\n패스워드, `Network`와 `Datastore`를 설정하고 `dns_servers`를 Router IP로 변경한다. 나머지는 기본값으로 설정한다. \n\n```json\n{\n            ...\n            \"deployment_network\": \"VM Network\",\n            \"datastore\": \"datastore2\"\n        },\n        \"appliance\": {\n            \"__comments\": [\n            ...\n            ],\n            \"thin_disk_mode\": true,\n            \"deployment_option\": \"tiny\",\n            \"name\": \"vcsa.tanzu.local\"\n        },\n        \"network\": {\n            \"ip_family\": \"ipv4\",\n            \"mode\": \"static\",\n            \"system_name\": \"vcsa.tanzu.local\",\n            \"ip\": \"192.168.0.202\",\n            \"prefix\": \"24\",\n            \"gateway\": \"192.168.0.1\",\n            \"dns_servers\": [\n                \"192.168.0.201\"\n            ]\n        },\n    ...\n}\n```\n\n위에서 다운로드 받았던 VCSA ISO를 압축을 풀고 위 스크립트를 참조하여 배포 명령을 실행한다. \n\n```sh\n$ cd VMware-VCSA-all-7.0.1-17004997/\n$ ./vcsa-deploy install --accept-eula --acknowledge-ceip --no-ssl-certificate-verification /mnt/c/esxi/vsphere-with-tanzu-homelab-scripts/vcsa.tanzu.local.json\n```\n\nvCenter가 설치가 완료된 후 SSH로 접속한다. 기본 Shell이 Bash가 아니므로 `shell` 명령으로 접속한후 `chsh -s /bin/bash`로 기본 Shell을 Bash로 변경한다.  \n\n```sh\n$ ssh root@192.168.0.202\nThe authenticity of host '192.168.0.202 (192.168.0.202)' can't be established.\nECDSA key fingerprint is SHA256:oMr5nJ9RDeZS9E3m586kSFDoNXM76yNiLYaIyDQ6ca4.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '192.168.0.202' (ECDSA) to the list of known hosts.\n\nVMware vCenter Server 7.0.1.00100\n\nType: vCenter Server with an embedded Platform Services Controller\n\nroot@192.168.0.202's password:\nConnected to service\n\n    * List APIs: \"help api list\"\n    * List Plugins: \"help pi list\"\n    * Launch BASH: \"shell\"\n\nCommand>\nCommand> shell\nShell access is granted to root\nroot@vcsa [ ~ ]# chsh -s /bin/bash\n```\n\n### VCSA 설정 변경\n\n메모리 절약을 위해 마스터 노드 개수를 변경한다. `/etc/vmware/wcp/wcpsvc.yaml` 에서 `clusterconfig.minmasters` 와 `clusterconfig.maxmasters` 값을 2로 변경하면 된다. 자세한 내용은 다음 링크를 참조한다.\n\n* [https://www.virtuallyghetto.com/2020/04/deploying-a-minimal-vsphere-with-kubernetes-environment.html](https://www.virtuallyghetto.com/2020/04/deploying-a-minimal-vsphere-with-kubernetes-environment.html)\n\n```yaml\nlogging:\n  level: debug\n  maxsizemb: 10\n# By default, WCP Agent VM OVF URL points to the ovf bundled in vCSA, served\n# through lighttpd web server. To override it, set kubevmconfig.ovfurl param.\n#kubevm:\n#  ovfurl: 'https://this_vc_pnid:5480/wcpagent/photon-ova-%%SIZE%%.ovf'\n#  apiserverport: 6443\n#  authproxyport: 443\nrhttpproxy_port: 443\nclusterconfig:\n  minmasters: 2\n  maxmasters: 2\n  disable_ssl: false\n  namespace_graceful_disable_duration: 1800\n  upgrade_precheck_timeout: 300\n# force_upgrade_clusters:\n  force_version_for_enable: v1\\.18\\..*\n  logging_fluentbit_enabled: false\n# logging_fluentbit_rsyslog: \"\"\ncsisetting:\n   csi_enabled: true\nhdcsconfig:\n  ovf_url: 'file:///storage/lifecycle/vmware-hdcs'\n```\n\nvCenter UI사용을 위해 [`setup_vcsa.ps1`](https://github.com/lamw/vsphere-with-tanzu-homelab-scripts/blob/master/setup_vcsa.ps1) 스크립트를 수정한다.  \n\n```powershell\n.\\setup_vcsa.ps1\nConnect-VIServer: C:\\esxi\\vsphere-with-tanzu-homelab-scripts\\setup_vcsa.ps1:17\nLine |\n  17 |  $vc = Connect-VIServer $VCSAHostname -User $VCSAUsername -Password $V …\n     |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n     | 2020-11-30 오전 12:45:18 Connect-VIServer  The SSL connection could not be established,\n     | see inner exception.\n```\n\n위와 같이 SSL connection 에러가 발생하기 때문에 인증서 체크를 무시하는 설정을 먼서 한다. \n\n```powershell\nSet-PowerCLIConfiguration -InvalidCertificateAction:Ignore\n```\n\n`$VCSAPassword`, `$DatastoreName`, `$ESXiPassword` 등을 수정하고 스크립트를 실행한다.\n\n```powershell\nPS C:\\esxi\\vsphere-with-tanzu-homelab-scripts> .\\setup_vcsa.ps1\n```\n\n설치가 완료되면 신규UI로 접속이 가능해진다. 이제부터는 VCSA로 직접 접속이 가능하다.  \n기본 로그인 아이디는 `administrator@vsphere.local`로 `vcsa.tanzu.local.json` 에서 설정한 패스워드로 접속하면 vCenter 기반의 Client UI를 확인할 수 있다. 정상적으로 클러스터에 연결된 ESXi 호스트 정보를 확인할 수 있다.  \n\n![VCSA](/img/vcsa_home.png)\n\n\n\n## 정리\n\n일단 아주 작은 하드웨어로 가성비 좋은 클러스터를 구축할 수 있는 PN50으로 Tanzu 클러스터 구성을 하기 위한 기초작업이 완료되었다.  \n\n글이 길어지는 관계로 VDS(vSphere Distributed Switch)를 구성하는 이후 부분은 다음 포스팅에서 계속 이어나갈 예정이다. \n\n> 다음글 - [vSphere with Tanzu homelab running on ASUS PN50 (2/2)](https://ddii.dev/kubernetes/vSphere-with-Tanzu-homelab-2/)"
    },
    {
      "id": "retrospective/2020plan/",
      "metadata": {
        "permalink": "/retrospective/2020plan/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2020-01-03-2020plan.md",
        "source": "@site/blog/2020-01-03-2020plan.md",
        "title": "2019 Retrospective",
        "description": "19년도를 돌아보며",
        "date": "2020-01-03T00:00:00.000Z",
        "formattedDate": "January 3, 2020",
        "tags": [
          {
            "label": "Retrospective",
            "permalink": "/tags/retrospective"
          },
          {
            "label": "2019",
            "permalink": "/tags/2019"
          },
          {
            "label": "2020",
            "permalink": "/tags/2020"
          },
          {
            "label": "Planning",
            "permalink": "/tags/planning"
          }
        ],
        "readingTime": 6.255,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "2019 Retrospective",
          "comments": true,
          "classes": "wide",
          "description": "19년도를 돌아보며",
          "slug": "retrospective/2020plan/",
          "date": "2020-01-03T00:00:00.000Z",
          "categories": [
            "Retrospective"
          ],
          "tags": [
            "Retrospective",
            "2019",
            "2020",
            "Planning"
          ]
        },
        "prevItem": {
          "title": "vSphere with Tanzu homelab running on ASUS PN50 (1/2)",
          "permalink": "/kubernetes/vSphere-with-Tanzu-homelab/"
        },
        "nextItem": {
          "title": "CircleCI로 AWS BATCH(ECS) CI/CD Pipeline 구성",
          "permalink": "/devops/circleci-ecs/"
        }
      },
      "content": "또 한해가 지나갔다. 2019년 한일 정리하면서 2020년 플랜 기록용으로 끄적거려본다.\n\n- Work\n    - 18년도 까지 서비스 개발을 맡았지만 갑작스럽게 부서 이동으로 인해 FaaS 개발을 손을 떼고 신규 부서로 이전하게 되었다. 작년 초에도 큰 변화였는데 이번에도 더 큰 변화였다.\n    - 문제는 개발보다는 기획쪽에 더 가까운 업무였고 데이터 분석 관련한 플랫폼을 개발하는 TF에 포함되게 되었다. 머신러닝, 데이터 엔지니어링과는 담을 쌓았었지만 어쩔수 없이 분석 관련하여 과제 수행\n    - 1-3월에는 주로 모빌리티 관련 에코파트너들과 협력 및 사업개발을 위한 기술검토 및 검증.\n    - 4-6월에는 그룹 관계사들의 데이터레이크에 대한 정의와 설계, 플랫폼 구현을 위한 Design-Thinking, 서비스 포털 기획 등을 수행하고 그룹 관계사들의 데이터레이크에 대한 정의와 설계 진행.\n    - 7-9월에는 서비스 포털 개발을 위한 CI/CD 선정 및 devops 파이프라인 구성, 데이터 preparation 툴 검토 및 AWS Managed 서비스 검증 및 구현 업무를 수행했다. 또한 프론트엔드 서비스 처리를 위한 람다함수 개발 및 documents 레포지토리 구축 진행.\n    - 10-12월까지는 서비스 포털 오픈 및 고도화, 다음 단계 서비스 기획 수행함.\n\n- Main Tools\n    - Python, Node.js, Java(Springboot)\n    - AWS - ECS, Lambda, CloudFront, Athena, Glue, S3, SageMaker\n    - IBM Cloud - Object Storage\n    - Github Team\n    - CircleCI, Github Action\n\n- Side (Community포함)\n    - 번역 : [프로메테우스 인프라스트럭처 모니터링 [가상머신, 컨테이너 환경의 프로메테우스 모니터링 실습과 활용]](http://acornpub.co.kr/book/monitoring-prometheus)\n    - 블로그 포스팅 - 13회 (목표 25회 미달성)\n    - 사내 연구 과제 - 멀티클라우드 구현을 위한 IaC 연구 (5-10월)\n        - Terraform, Terratest, Ansible, Kubespary, CircleCI 조합으로 하나의 파이프라인에서 Bastion 없이 멀티 클라우드(AWS, IBM, GCP) 환경에 Elastic 및 K8s 클러스터 배포 및 테스트 자동화 환경 구성\n    - Istio 스터디 (9-11월)\n        - 다양한 회사분들과 스터디 진행 (카카오뱅크, 이베이, 하이퍼커넥트)\n            - 다양한 경험을 들을수 있어서 좋은 경험!!\n        - 자료 : [https://github.com/dev-chulbuji/istio-study](https://github.com/dev-chulbuji/istio-study)\n    - K8s Korea Group 모니터링 소모임 진행\n        - 1차 (4/25) SK u-tower\n            - 발표1 : OpenCensus with Prometheus and Kubernetes (김진웅)\n            - 발표2 - Service Mesh(Istio) Monitoring (나정호)\n        - 2차(6/4) 메가존\n            - 발표1 : 쿠버네티스 운영 관리 오픈소스: NexClipper 소개 (김진용)\n            - 발표2 : 난상토의 (모니터링, 아키텍처 경험 등 자유주제 공유, 토의)\n    - AWS Korea User 소모임 참여\n        - 판교, 컨테이너, 서버리스, 데이터 사이언스, 머신러닝 스터디, 데브옵스\n    - CircleCI 밋업 참여\n        - 1차 (5/14), 2차 (8/27)\n    - Open Infrastructure & Cloud Native Days Korea 2019 참여 (7/18-19)\n    - Kubernetes Forum Seoul 참여 (12/9)\n        - CFP 탈락 ㅠㅠ\n    - 개인 서버, NAS 2대 Shutdown (전기세 압박에서 해방됨)\n        - G Suite Business 로 갈아탐\n    - 미디어 환경 개편\n        - 스마트TV 직구 (75SM8670PUA)\n        - PS4 구매 (닌텐도 스위치, Xbox 기존보유)\n    - K3s Home Lab구성 및 이더패드 서비스 운영\n        - 1.16 클러스터 : 1 Master, 3 Workers\n    - 독서\n        - 20권 내외 (기술서 포함)\n    - 교육\n        - Google Study Jam - 클라우드 4회, 머신러닝 4회 수료\n        - Udemy - CKA, CKAD\n        - Elasticsearch Engineer I, II\n\n- Speak\n    - 2018년에 비해 대외 발표가 많이 적었다. 실제 K8s 관련 업무를 하지 않아 그런 듯 하다.\n    - 4월 AWS Korea User Group 판교소모임\n        - [Amazon EKS Workshop 살펴보기](https://www.slideshare.net/JinwoongKim8/eks-workshop-140043415)\n    - 4월 Kubernetes Korea Group 모니터링 소모임\n        - [Opencensus with prometheus and kubernetes](https://www.slideshare.net/JinwoongKim8/open-census-with-prometheus-and-kubernetes)\n    - 4월 Google Cloud Next ‘19 Extended Korea\n        - [Knative로 서버리스 워크로드 구현](https://www.slideshare.net/JinwoongKim8/knative)\n    - 6월 Korea DevOps MeetUP ‘19 - 4월 발표 수정\n        - [OpenCensus with Prometheus and Kubernetes](https://www.slideshare.net/JinwoongKim8/opencensus-with-prometheus-and-kubernetes)\n    - 7월 CircleCI Korea User Group 2nd Meetup\n        - [Data(?)Ops with CircleCI](https://www.slideshare.net/JinwoongKim8/dataops-with-circleci)\n    - 9월 AWS Korea User Group 판교소모임\n        - [DataOps with CircleCI](https://www.slideshare.net/JinwoongKim8/dataops-with-circleci)\n    - 11월 사내 Open Tech Lounge 발표\n        - 멀티클라우드 구현을 위한 IaC 연구\n\n- 2020 목표\n    - 커뮤니티 활동은 최대한 줄이기 (필수적인 부분만)\n        - SRE group 활성화 방안 고민\n        - 밋업, 세미나 관련 캘린더 연동 자동화 방안 고민\n    - 번역 및 책 쓰는건 고민을 더...\n    - Level Up\n        - 영어!! - 발표까지\n        - Full-Stack ??\n        - 머신러닝, 딥러닝\n        - ELK 쿼리 잘짜기\n        - NoSQL\n    - 기존업무 K8s 전환 및 고도화\n        - 서버리스, 매니지드 방향으로\n    - 사이드 프로젝트\n        - 홈 미디어 관리 고도화 (Plex)\n        - [nextcloud](https://nextcloud.com/) 서비스 오픈 (지인 대상)\n        - 모바일 웹앱 3개 만들기 (미디어 관리, 코믹 뷰어, 스트리밍)\n        - 오픈소스 기여"
    },
    {
      "id": "devops/circleci-ecs/",
      "metadata": {
        "permalink": "/devops/circleci-ecs/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-06-30-circleci-ecs.md",
        "source": "@site/blog/2019-06-30-circleci-ecs.md",
        "title": "CircleCI로 AWS BATCH(ECS) CI/CD Pipeline 구성",
        "description": "이전 포스팅에서 EKS 클러스터와 어플리케이션 배포 및 테스트를 간단하게 해봤다.  이번에는 AWS ECS 또는 AWS Batch Application을 CircleCI를 통해 배포 및 업데이트 하는 것을 알아본다.",
        "date": "2019-06-30T00:00:00.000Z",
        "formattedDate": "June 30, 2019",
        "tags": [
          {
            "label": "DevOps",
            "permalink": "/tags/dev-ops"
          },
          {
            "label": "CI/CD",
            "permalink": "/tags/ci-cd"
          },
          {
            "label": "Pipeline",
            "permalink": "/tags/pipeline"
          },
          {
            "label": "CircleCI",
            "permalink": "/tags/circle-ci"
          },
          {
            "label": "GitHub",
            "permalink": "/tags/git-hub"
          },
          {
            "label": "GitOps",
            "permalink": "/tags/git-ops"
          },
          {
            "label": "ECS",
            "permalink": "/tags/ecs"
          },
          {
            "label": "AWS Batch",
            "permalink": "/tags/aws-batch"
          }
        ],
        "readingTime": 10.595,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "CircleCI로 AWS BATCH(ECS) CI/CD Pipeline 구성",
          "comments": true,
          "classes": "wide",
          "slug": "devops/circleci-ecs/",
          "date": "2019-06-30T00:00:00.000Z",
          "categories": [
            "DevOps"
          ],
          "tags": [
            "DevOps",
            "CI/CD",
            "Pipeline",
            "CircleCI",
            "GitHub",
            "GitOps",
            "ECS",
            "AWS Batch"
          ]
        },
        "prevItem": {
          "title": "2019 Retrospective",
          "permalink": "/retrospective/2020plan/"
        },
        "nextItem": {
          "title": "CircleCI - GitHub 연동 및 EKS 구성하기",
          "permalink": "/devops/circleci/"
        }
      },
      "content": "[이전 포스팅](https://ddii.dev/devops/circleci/)에서 EKS 클러스터와 어플리케이션 배포 및 테스트를 간단하게 해봤다.  이번에는 AWS ECS 또는 AWS Batch Application을 CircleCI를 통해 배포 및 업데이트 하는 것을 알아본다.  \n\n## Application 요구사항\n기본적으로 현재 업무 중에 IBM클라우드에서 AWS로 데이터를 가져와야 하는 업무가 생겨서 시작되었고 정기적으로 매일 새벽에 크롤링(Pull)방식으로 데이터를 복제해야하는 Use-Case를 구현해야 했다.  \nCI/CD Pipeline구성이 주 목적으로 Application구현은 포스팅 내용에서 제외하였다.  \n\n### 기본 요구사항\n* Github Account\n* CircleCI Account (Github Auth)\n* AWS ECR(Elastic Container Registry)\n* AWS Batch(ECS) \n\n## CI/CD 구성안\n\n실제 프로젝트에 적용할 구성안이다.  \n기본적으로 모든 구성은 최대한 비용이 들지 않고 AWS 종속성을 제거한 방법으로 구성하였다.  \n\n* Code Repository : Github\n* CI : CircleCI\n* Registry : AWS ECR\n* CD : CircleCI\n* Target : AWS Batch(ECS)\n* Notification : Slack\n\n이번 데모에서는 다음과 같은 시나리오로 진행하려고 한다.  \n1. Terraform으로 AWS Batch 생성\n2. Github에 수정된 Batch Code(Shell Script) Commit & Triggering CircleCI\n3. Docker Build & AWS ECR로 Push\n4. S3로 myjob shell 파일 복사\n5. AWS Batch(ECS) Job Definition 변경(Change Image)\n\n### AWS Batch\n기본적으로 Batch 컴퓨팅을 목적으로 하는 서비스로 컴퓨팅 타입을 Fargate 또는 EC2 중 하나를 선택하여 동적으로 프로비저닝하는 서비스이다.  \n초기 설정이 번거롭긴 하지만 한번 구성하고 나면 별도 관리가 필요없고 Batch작업이 발생하는것과 컨테이너 이미지 보관비용 이외에는 추가 비용이 발생하지 않기 때문에 비용측면에서 장점이 있는 서비스이다.  \n위에서 이야기한 정기적으로 새벽마다 크롤링(Pull)방식으로 데이터를 복제해야하는 Use-Case를 구현하는것이 적당한 워크로드 구현방법이라 생각했기 때문에 AWS Batch 서비스를 사용하게 되었다.  \n\n간단한 Batch demo는 [https://github.com/awslabs/aws-batch-helpers/](https://github.com/awslabs/aws-batch-helpers/)에서 확인이 가능하며 CircleCI연동을 위해 Fork후  진행하였다.\n\nDemo Repository : [https://github.com/ddiiwoong/aws-batch-helpers/](https://github.com/ddiiwoong/aws-batch-helpers/)\n\n### CircleCI\n모든 Pipeline은 CircleCI기반으로 작성하였다.  혹자는 AWS Code Commit, Pipeline을 사용하지 않느냐고 문의하시는 분들도 있는데 다음과 같은 이유로 CircleCI를 선택하였다.  \n\n* Fully Managed (Serverless)\n* AWS 종속성 최소화  \n* Git, Registry, CD영역의 확장성 고려\n* AWS Console 접속 최소화\n* 쉽고 단순하게 빌드 환경구성\n* 소규모 프로젝트 빠르게 시작 가능\n\n### Amazon Elastic Container Registry (ECR)\n개인적으로 Registry는 구축형보다는 Managed서비스를 사용하는것이 좋다고 보기 때문에 Webhook을 Native하게 지원하는 DockerHub도 대체 가능하다고 생각한다.  \n* Fully Managed\n* Security (IAM) 연동\n* CircleCI Orbs 제공\n* EKS, ECS 연동 용이\n\n### Terraform\n\n[https://www.terraform.io/docs/providers/aws/index.html](https://www.terraform.io/docs/providers/aws/index.html)  \n\n선언적 인프라스트럭처 관리 도구로 많이 사용하고 있는 도구이며 Docs와 블로그 자료가 많은 관계로 따로 설명하지 않겠다.  \n이번 포스트에서는 AWS Batch를 생성하는 영역을 Terraform이 담당한다.  \n\n## CircleCI Config 작성\n```yaml\nversion: 2.1\norbs:\n  aws-ecr: circleci/aws-ecr@6.1.0\n  aws-ecs: circleci/aws-ecs@0.0.8\n  aws-s3: circleci/aws-s3@1.0.11\n  aws-cli: circleci/aws-cli@0.1.13\njobs:\n  sh-s3-upload:\n    docker:\n      - image: 'circleci/python:2.7'\n    steps:\n      - checkout\n      - aws-s3/copy:\n          from: ./myjob.sh\n          to: 's3://batch-ecr-test/myjob.sh'\n  deploy-batch:\n    executor: aws-cli/default\n    steps:\n      - checkout\n      - aws-cli/install\n      - run:\n          name: Update AWS Batch Job\n          command: |\n            aws batch register-job-definition --job-definition-name fetch_and_run --type container --container-properties '{ \"image\": \"823928750534.dkr.ecr.ap-northeast-2.amazonaws.com/fetch_and_run:v20190623\", \"vcpus\": 1, \"memory\": 512}'\nworkflows:\n  build-and-deploy:\n    jobs:\n      - aws-ecr/build-and-push-image:\n          repo: $AWS_RESOURCE_NAME_PREFIX\n          tag: v20190623\n      - sh-s3-upload:\n          name: sh-s3-upload\n          requires:\n            - aws-ecr/build-and-push-image\n      - deploy-batch:\n          name: deploy-batch\n          requires:\n            - sh-s3-upload\n```\n\n단계별 설명을 위해 부분적으로 설명하도록 하겠다.  \n1. Terraform으로 Batch 배포\n    ```json\n    resource \"aws_batch_compute_environment\" \"default\"{ \n      compute_environment_name = \"env_fetch_and_run\" \n      compute_resources { \n        instance_role = \"arn:aws:iam::823928750534:instance-profile/ecsInstanceRole\" \n        instance_type = [\n          \"optimal\",\n        ]\n        desired_vcpus = 1 \n        max_vcpus = 1 \n        min_vcpus = 0 \n        security_group_ids = [\n          \"sg-0632cf81b5c4dff17\" \n        ]\n        subnets = [\n          \"subnet-0c4f8135b536e8fab\",\n          \"subnet-0a261950d894cf27e\"\n        ] \n        type = \"EC2\" \n      } \n      service_role = \"arn:aws:iam::823928750534:role/service-role/AWSBatchServiceRole\" \n      type =\"MANAGED\"\n    }\n    ```\n    Batch Computing 환경을 구성하게 되는데 `\"aws_batch_compute_environment\"`를 사용한다.  \n    * compute_environment_name : (필수) 컴퓨팅 환경 이름\n    * compute_resources.instance_role : (필수) EC2에 적용되는 Role\n    * compute_resources.instance_type : (필수) 사용 가능한 instance 유형\n    * compute_resources.desired_vcpus : (옵션) 원하는 CPU수\n    * compute_resources.max_vcpus : (필수) 최대 CPU수  \n    * compute_resources.min_vcpus : (필수) 최소 CPU수\n    * compute_resources.security_group_ids : (필수) EC2에 적용되는 Security Group\n    * compute_resources.subnets : (필수) Subnets 목록\n    * compute_resources.type : (필수) EC2를 기반으로 생성, SPOT instance 사용가능  \n    * service_role : (필수) AWS Batch가 다른 AWS 서비스를 호출 할수있게 해주는 IAM Role(ARN)\n    * type : (필수) MANAGED나 UNMANAGED를 선택할 수 있고, MANAGED의 경우 compute_resources에 세부 사항을 설정할 수 있다.  \n\n    ```json\n    resource \"aws_batch_job_definition\" \"default\" {\n      name = \"fetch_and_run\" \n      type = \"container\"\n\n      container_properties = <<CONTAINER_PROPERTIES\n    {\n        \"command\": [],\n        \"image\": \"823928750534.dkr.ecr.ap-northeast-2.amazonaws.com/fetch_and_run:v20190623\",\n        \"vcpus\": 1,\n        \"memory\": 512,\n        \"volumes\": [],\n        \"environment\": [],\n        \"mountPoints\": [],\n        \"ulimits\": [] \n    }\n    CONTAINER_PROPERTIES\n    }\n    ```\n    Job Definition 정의(\"aws_batch_job_definition\")에서는 Resource spec 및 Command(Docker RUN), Container IMAGE 등을 선언하게 된다.  \n    * name : (필수) Job Definition 이름\n    * type : (필수) Job Definition 유형, ECS로 처리하기 위해 container로 선택\n    * container_properties : (선택) JSON 형태로 작성하는 container 속성정보 (Image, Resources, Command, MountPoint 등)\n\n2. .circleci/config.yml에 CircleCI 버전 명시 및 관련 orb(ecr,ecs,s3,cli) 추가  \n    기본적으로 2.1로 설정을 해야 ecs, eks orb 사용이 가능하다.\n    ```yaml\n    version: 2.1\n      orbs:\n        aws-ecr: circleci/aws-ecr@6.1.0\n        aws-ecs: circleci/aws-ecs@0.0.8\n        aws-s3: circleci/aws-s3@1.0.11\n        aws-cli: circleci/aws-cli@0.1.13\n    ```\n\n3. workflow 구성\n\n    `build-and-push-image` -> `sh-s3-upload` -> `deploy-batch` 순으로 순차적으로 pipeline구성을 하였다.  \n\n    ```\n    workflows:\n      build-and-deploy:\n        jobs:\n          - aws-ecr/build-and-push-image:\n              repo: $AWS_RESOURCE_NAME_PREFIX # data-crawler-test\n              tag: v20190623\n              # create-repo: true\n              # dockerfile: Dockerfile\n          - sh-s3-upload:\n              name: sh-s3-upload\n              requires:\n                - aws-ecr/build-and-push-image\n          - deploy-batch:\n              name: deploy-batch\n              requires:\n                - sh-s3-upload\n    ```\n\n    기본적으로 CircleCI에서는 orb job을 [orb name]/[predefined-job] 형식으로 선언한다.  \n\n    I. 첫번째 step에서는 container image를 build하고 push하는 Job을 수행하므로 `aws-ecr/build-and-push-image`을 선언한다.  \n    자세한 내용은 [https://circleci.com/orbs/registry/orb/circleci/aws-ecr](https://circleci.com/orbs/registry/orb/circleci/aws-ecr)를 확인하자.  \n\n    II. 두번째 step에서는 S3에 배치스크립트를 올리는 Custom Job을 수행한다.  Job은 Step의 모음으로 선행되어야할 Job은 상위에 `requires:` 에서 선언하면 된다.  \n\n    III. 세번째 step에서는 AWS Batch job definition의 container image를 교체(revision 변경)하는 Custom Job을 수행한다.  \n\n4. custom job 구성\n    Workflow에서 선언한 Custom Job의 상세 definition을 기재한다.  \n    ```yaml\n    jobs:\n      sh-s3-upload:\n        docker:\n          - image: 'circleci/python:2.7'\n        steps:\n          - checkout\n          - aws-s3/copy:\n              from: ./myjob.sh\n              to: 's3://batch-ecr-test/myjob.sh'\n      deploy-batch:\n        executor: aws-cli/default\n        steps:\n          - aws-cli/install\n          - run:\n              name: Update AWS Batch Job\n              command: |\n                aws batch register-job-definition --job-definition-name fetch_and_run --type container --container-properties '{ \"image\": \"823928750534.dkr.ecr.ap-northeast-2.amazonaws.com/fetch_and_run:v20190623\", \"vcpus\": 1, \"memory\": 512}'\n    ```\n    I. `sh-s3-upload` 에서는 `aws-s3/copy` 를 사용하여 원하는 S3버킷에 사용할 스크립트를 업로드 한다.  `checkout`에서는 현재 상태의 git repo를 clone하게 된다.  \n\n    II. `deploy-batch` 에서는 `aws-cli/install` 를 사용하여 기존에 작성한 Batch Job definition을 awscli로 원하는 새로운 image로 업데이트 하게 된다.  \n\n## CircleCI 실행\n빌드 및 배포과정을 간단하게 설명하면 코드가 업데이트되면 CircleCI는 해당 저장소의 `.circleci/config.yml` 을 기준으로 빌드 및 배포를 시작하게 된다.  \n\n## README.md에 Build Status Badge 달기\n[https://circleci.com/docs/2.0/status-badges/](https://circleci.com/docs/2.0/status-badges/)  \n\n![badge](/img/circle-badge.png)\n위 링크와 설정을 참고하여 아래 그림과 같이 Build Statud Badge를 달 수 있다.  \n\n![badge](/img/circle-badge-ss.png)\n\n## 정리\n이번에는 CircleCI를 가지고 AWS Batch job을 구성하는 데모를 진행하였다.  \n\n얼마전인가에도 [어형부형](https://github.com/leoh0)님께서도 언급하신 선언적인 구성 기반에 최대한 serverless 환경에서의 배포를 추구하게 되다 보니 아래와 같은 `CI/CD Pipeline 시나리오`를 구상하게 되었다.  \n\n![cicd](/img/circlecicd.png)\n\n새로운 기능의 브랜치(New Feature)가 Git에 push되면 빌드와 테스트가 트리거되어 자동으로 진행되고 동시에 개발자가 PR을 작성하면 동료 및 담당 상급자에게 코드 리뷰를 받게 된다.  \n\n리뷰가 통과되고 CircleCI에서 빌드가 성공했다면 승인단계를 통해 Merge를 하고 CircleCI 가 한 번 더 빌드를 시작하고 배포까지 수행하게 된다.  \n\nECS나 Batch로 배포는 CircleCI를 통해 직접 배포를 하고 EC2나 EKS로의 배포는 Terraform 및 ArgoCD를 통해 배포를 진행하는 방식이다.  \n\n다음번 포스팅에는 위 그림 기반으로 CircleCI와 ArgoCD를 활용하여 EKS기반 배포과정을 정리할 예정이다.  \n\n[![ko-fi](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/X8X1W6OZ)"
    },
    {
      "id": "devops/circleci/",
      "metadata": {
        "permalink": "/devops/circleci/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-06-18-circleci.md",
        "source": "@site/blog/2019-06-18-circleci.md",
        "title": "CircleCI - GitHub 연동 및 EKS 구성하기",
        "description": "CI/CD는 개발단계에서 지속적인 통합, 배포를 통해 효율성을 높여주는 도구라고 말할수 있다.  특히 GitOps가 중요시 되는 최근 트렌드에서 Public Git서비스와 통합은 필수적인 요소이다.",
        "date": "2019-06-18T00:00:00.000Z",
        "formattedDate": "June 18, 2019",
        "tags": [
          {
            "label": "DevOps",
            "permalink": "/tags/dev-ops"
          },
          {
            "label": "CI/CD",
            "permalink": "/tags/ci-cd"
          },
          {
            "label": "Pipeline",
            "permalink": "/tags/pipeline"
          },
          {
            "label": "CircleCI",
            "permalink": "/tags/circle-ci"
          },
          {
            "label": "GitHub",
            "permalink": "/tags/git-hub"
          },
          {
            "label": "GitOps",
            "permalink": "/tags/git-ops"
          },
          {
            "label": "EKS",
            "permalink": "/tags/eks"
          }
        ],
        "readingTime": 10.68,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "CircleCI - GitHub 연동 및 EKS 구성하기",
          "comments": true,
          "classes": "wide",
          "slug": "devops/circleci/",
          "date": "2019-06-18T00:00:00.000Z",
          "categories": [
            "DevOps"
          ],
          "tags": [
            "DevOps",
            "CI/CD",
            "Pipeline",
            "CircleCI",
            "GitHub",
            "GitOps",
            "EKS"
          ]
        },
        "prevItem": {
          "title": "CircleCI로 AWS BATCH(ECS) CI/CD Pipeline 구성",
          "permalink": "/devops/circleci-ecs/"
        },
        "nextItem": {
          "title": "Yaml 작성 기본 Tip",
          "permalink": "/kubernetes/CKAD-1/"
        }
      },
      "content": "CI/CD는 개발단계에서 지속적인 통합, 배포를 통해 효율성을 높여주는 도구라고 말할수 있다.  특히 GitOps가 중요시 되는 최근 트렌드에서 Public Git서비스와 통합은 필수적인 요소이다. \n\n[GitHub MarketPlace](https://github.com/marketplace)에서 `CI` 라고 검색하면 다음과 같은 결과를 얻을수 있다.  \n\n![github-ci](/img/github-ci.png)\n\nCircleCI, Travis CI, Google Cloud Build 등 최근 트렌드한 도구들을 확인할 수 있다.  \n\n이번 포스팅에서는 `CircleCI`를 `GitHub`과 연동해서 AWS ECR Push 및 EKS로 배포하는 간단한 Pipeline을 구성하는 방법을 적어보고자 한다.  \n\n## CircleCI - GitHub Accout 연동\n\n[circleci.com](https://circleci.com)에 접속하여 Sign Up을 진행하면 GitHub과 BitBucket 계정을 연동할 수 있는데 일반적인 GitHub 3rd Party OAuth연동 진행을 하게된다.  \n\n![init](/img/circleci-init.png)\n\n위 그림처럼 모든 Repository가 확인되고 Follow할 Repository를 선택하면 초기 구성이 완료된다.  \n\n![error](/img/init-error.png)\n\n첫 연동이 되면 Build를 진행하게 되는데 위 그림과 같이 error를 보게 되는데 이는 기본적으로 circleci가 필요로 하는 기본 설정값(.circleci/config.yaml)이 없어서 발생하는 오류이다.  \n\n```sh\n#!/bin/sh -eo pipefail\n# No configuration was found in your project. Please refer to https://circleci.com/docs/2.0/ to get started with your configuration.\n# \n# -------\n# Warning: This configuration was auto-generated to show you the message above.\n# Don't rerun this job. Rerunning will have no effect.\nfalse\n```\n\n위에서 보이는것 처럼 config를 체크하는것도 하나의 가상머신(컨테이너)이 진행하는데 CircleCI콘솔에서 `Spin up Environment` 로그를 보면 Docker(18.09.6)로 aws Linux기반으로 환경구성을 하는 것을 알 수 있다.\n\n```sh\nBuild-agent version 1.0.11727-b0960fc9 (2019-05-23T02:12:54+0000)\nDocker Engine Version: 18.09.6\nKernel Version: Linux 9a20a41aeae4 4.15.0-1035-aws #37-Ubuntu SMP Mon Mar 18 16:15:14 UTC 2019 x86_64 Linux\nStarting container bash:4.4.19\n  using image bash@sha256:9f0a4aa3c9931bd5fdda51b1b2b74a0398a8eabeaf9519d807e010b9d9d41993\n...\n```\n## CircleCI Grossary\n\n아래 링크는 CircleCI에서 자주 사용하는 용어들을 따로 정리한 페이지이다.  주로 컨테이너 기반으로 동작하기 때문에 용어들은 Docker에서 사용하는 용어과 겹치는 부분이 많다.   \n\n[https://circleci.com/docs/2.0/glossary/](https://circleci.com/docs/2.0/glossary/)\n\n위 링크 내용을 확인하면 `Orbs`라는 용어가 나오는데 이는 공유가능한 패키지로 Jenkins의 Plugin과 유사한 개념이라고 보면 된다. CircleCI에서 제공하는 자체 패키지 뿐만 아니라 3rd Party orbs를 제공하고 있다. \n\nMacOS에서는 brew를 통해 cli를 설치하고 orb 리스트를 확인하거나 [https://circleci.com/orbs/registry/](https://circleci.com/orbs/registry/)에서 확인할 수 있다.  \n\n\n```sh\n$ brew install circleci\n$ circleci orb list\nOrbs found: 43. Showing only certified orbs.\nAdd --uncertified for a list of all orbs.\n\ncircleci/android (0.1.0)\ncircleci/artifactory (1.0.0)\ncircleci/aws-cli (0.1.13)\ncircleci/aws-code-deploy (0.0.9)\ncircleci/aws-ecr (6.1.0)\ncircleci/aws-ecs (0.0.8)\ncircleci/aws-eks (0.1.0)\ncircleci/aws-s3 (1.0.11)\n...\ncircleci/jira (1.0.5)\ncircleci/jq (1.9.0)\ncircleci/kubernetes (0.3.0)\ncircleci/lein-nvd (0.0.2)\ncircleci/maven (0.0.8)\ncircleci/node (1.0.1)\n...\ncircleci/slack (3.2.0)\ncircleci/twilio (0.0.1)\ncircleci/welcome-orb (0.3.1)\n\nIn order to see more details about each orb, type: `circleci orb info orb-namespace/orb-name`\n\nSearch, filter, and view sources for all Orbs online at https://circleci.com/orbs/registry/\n```\n\n## Circle Architecture\n\n![circle-arch](https://circleci.com/docs/assets/img/docs/arch.png)\n\nGitHub 또는 Bitbucket에서 관리하는 Repository가 CircleCI 프로젝트로 승인되면 최초에는 컨테이너나 가상머신환경(2core 4GB)이 프로비저닝 되고 자동으로 테스트가 진행된다.  \n\n테스트가 완료된 후 성공 또는 실패에 대한 Alert설정(Email, Slack)이 가능하고 각 단계(job, workflow)에 대한 결과는 각 단계별 세부 정보 페이지에서 확인할 수 있다.  \n\n또한 배포는 AWS CodeDeploy, AWS ECS, AWS S3, AWS EKS, Google Kubernetes Engine (GKE) 및 Heroku 등 다양한 환경에 코드를 배포하도록 구성 할 수 있다. 이외의 클라우드 서비스 배포는 SSH를 통해 직접 사용하거나 terraform과 같은 도구를 가지고 해당 클라우드 서비스의 API통해 자동화가 가능한 구조로 되어있다.  \n\n## CircelCI AWS EKS\n\n[https://github.com/CircleCI-Public/circleci-demo-aws-eks](https://github.com/CircleCI-Public/circleci-demo-aws-eks)\n\n위 demo는 CircleCI를 이용하여 다음과 같은 workflow (상세내용 아래 config.yaml 참고)를 수행한다.  \n\n```yaml\nversion: 2.1\n\norbs:\n    aws-eks: circleci/aws-eks@0.1.0\n    aws-ecr: circleci/aws-ecr@3.1.0\n    kubernetes: circleci/kubernetes@0.3.0\n```\n공통적으로 orb를 3가지를 추가하였기 때문에 각단계마다 관련 orb를 추가하는 단계를 거치게 된다.\n\n1. 환경설정 및 Docker Build 및 ECR로 Push\n  ```yaml\n  workflows:\n    deployment:\n      jobs:\n        - aws-ecr/build_and_push_image:\n            name: build-and-push-image\n            account-url: AWS_ECR_URL\n            region: AWS_DEFAULT_REGION\n            repo: eks_orb_demo_app\n            dockerfile: ~/project/demo_app/Dockerfile\n            path: ~/project/demo_app\n            tag: ${CIRCLE_SHA1}\n            # repository가 없을 경우 생성하는 옵션\n            # create-repo: true\n  ```\n  * environment variables 설정 \n    * Project - BUILD SETTINGS - Environment Variables 에서 Key-Value형태로 입력  \n      * AWS_DEFAULT_REGION : `us-west-2`\n      * AWS_ECR_URL : `219547004475.dkr.ecr.us-west-2.amazonaws.com/eks_orb_demo_app`  \n   * github 연동 (ssh-key 연동 및 repo clone)\n   * aws cli 설치 및 AWS Access, Secret Key설정\n   * ECR Login\n   * Image Build ([https://github.com/ddiiwoong/circleci-demo-aws-eks/blob/master/demo_app/Dockerfile](https://github.com/ddiiwoong/circleci-demo-aws-eks/blob/master/demo_app/Dockerfile))\n  * Push Image to ECR\n\n1. EKS클러스터 생성 (No Scripts)\n  ```yaml\n  workflows:\n    deployment:\n      jobs:\n        - aws-eks/create-cluster:\n            cluster-name: eks-orb-demo-app-deployment\n            aws-region: $AWS_DEFAULT_REGION\n            requires:\n              - build-and-push-image\n  ```\n  * kops, kubectl, aws iam authenticator, aws cli 설치\n  * kubectl config 설정 (aws iam authenticator)\n  * eksctl 설치\n  * eksctl로 클러스터 생성 및 검증 (Cloudformation)\n    * variables 사전 설정가능 ($AWS_DEFAULT_REGION)\n\n2. Demo Application 배포\n  ```yaml\n  jobs:\n    deploy-application:\n      executor: aws-eks/python3\n      parameters:\n      ...\n      steps:\n        - checkout\n        - run:\n            name: Create deployment manifest\n            command: |\n              BUILD_DATE=$(date '+%Y%m%d%H%M%S')\n              cat deployment/demo-app-deployment.yaml.template |\\\n                sed \"s|DOCKER_IMAGE_NAME|<< parameters.docker-image-name >>|\\\n                  g;s|BUILD_DATE_VALUE|$BUILD_DATE|g;s|VERSION_INFO_VALUE|\\\n                  << parameters.version-info >>|g\" > deployment/demo-app-deployment.yaml\n        - aws-eks/update-kubeconfig-with-authenticator:\n            cluster-name: << parameters.cluster-name >>\n            install-kubectl: true\n            aws-region: << parameters.aws-region >>\n        - kubernetes/create-or-update-resource:\n            resource-file-path: \"deployment/demo-app-deployment.yaml\"\n            get-rollout-status: true\n            resource-name: deployment/demoapp\n        - kubernetes/create-or-update-resource:\n            resource-file-path: \"deployment/demo-app-service.yaml\"\n  ...\n  workflows:\n    deployment:\n      jobs:\n        - deploy-application:\n          cluster-name: eks-orb-demo-app-deployment\n          aws-region: $AWS_DEFAULT_REGION\n          docker-image-name: \"${AWS_ECR_URL}/eks_orb_demo_app:${CIRCLE_SHA1}\"\n          version-info: \"${CIRCLE_SHA1}\"\n          requires:\n            - aws-eks/create-cluster\n  ...\n  ```\n  * deployment manifest생성 (deployment template)\n  * kops, kubectl, aws iam authenticator, aws cli 설치 (orb설정: aws-eks,kubernetes)\n  * kubectl config 설정 (aws iam authenticator)\n  * resource(deployment, service) 배포 (orb설정: aws-eks,kubernetes)\n  * rollout 수행 (0->3)\n\n3. application test\n  ```yaml\n  workflows:\n    deployment:\n      jobs:\n        - test-application:\n          name: test-application\n          cluster-name: eks-orb-demo-app-deployment\n          aws-region: $AWS_DEFAULT_REGION\n          expected-version-info: \"${CIRCLE_SHA1}\"\n          requires:\n            - deploy-application\n  jobs:\n    test-application:\n      executor: aws-eks/python3\n      parameters:\n        ...\n      steps:\n        - aws-eks/update-kubeconfig-with-authenticator:\n            cluster-name: << parameters.cluster-name >>\n            install-kubectl: true\n            aws-region: << parameters.aws-region >>\n        - run:\n            name: Wait for service to be ready\n            command: |\n              kubectl get pods\n              kubectl get services\n              sleep 30\n              for attempt in {1..20}; do\n                EXTERNAL_IP=$(kubectl get service demoapp | awk '{print $4}' | tail -n1)\n                echo \"Checking external IP: ${EXTERNAL_IP}\"\n                if [ -n \"${EXTERNAL_IP}\" ] && [ -z $(echo \"${EXTERNAL_IP}\" | grep \"pending\") ]; then\n                  break\n                fi\n                echo \"Waiting for external IP to be ready: ${EXTERNAL_IP}\"\n                sleep 10\n              done\n              sleep 180\n              curl -s --retry 10 \"http://$EXTERNAL_IP\" | grep \"<< parameters.expected-version-info >>\"\n  ```\n  * kops, kubectl, aws iam authenticator, aws cli 설치 (orb설정: aws-eks,kubernetes)\n  * kubectl config 설정 (aws iam authenticator)\n  * External IP 체크 및 curl 테스트\n\n4. Demo Application 삭제\n  ```yaml\n  jobs:  \n    undeploy-application:\n      executor: aws-eks/python3\n      parameters:\n      ...\n      steps:\n        - aws-eks/update-kubeconfig-with-authenticator:\n            cluster-name: << parameters.cluster-name >>\n            install-kubectl: true\n            aws-region: << parameters.aws-region >>\n        - kubernetes/delete-resource:\n            resource-types: \"deployment,service\"\n            label-selector: \"app=demo\"\n            wait: true\n        - run:\n            name: Check on pod status\n            command: |\n              kubectl get pods\n  workflows:\n    deployment:\n      jobs:\n        - undeploy-application:\n          cluster-name: eks-orb-demo-app-deployment\n          aws-region: $AWS_DEFAULT_REGION\n          requires:\n            - test-application\n  ```\n  * kops, kubectl, aws iam authenticator, aws cli 설치 (orb설정: aws-eks,kubernetes)\n  * kubectl config 설정 (aws iam authenticator)\n  * deployment 삭제 및 상태 체크\n\n5. EKS클러스터 삭제 (No Scripts)\n  ```yaml\n  workflows:\n    deployment:\n      jobs:\n        - aws-eks/delete-cluster:\n          cluster-name: eks-orb-demo-app-deployment\n          aws-region: $AWS_DEFAULT_REGION\n          wait: true\n          requires:\n            - undeploy-application\n  ```\n  * kops, kubectl, aws iam authenticator, aws cli 설치 (orb설정: aws-eks,kubernetes)\n  * kubectl config 설정 (aws iam authenticator)\n  * eksctl 설치\n  * eksctl로 클러스터 삭제 및 검증 (Cloudformation)\n\n상세 config는 다음 링크에서 확인할 수 있다.  \n[https://github.com/ddiiwoong/circleci-demo-aws-eks/blob/master/.circleci/config.yml](https://github.com/ddiiwoong/circleci-demo-aws-eks/blob/master/.circleci/config.yml)\n\n## pipeline 수행\n\n위 Workflow를 수행하게 되면 마지막에 어플리케이션과 클러스터를 삭제하기 때문에 해당 workflow는 제외하고 수행한다. 해당 config를 commit하면 바로 해당 CI가 트리거 되어 시작되게 된다.\n\n```yaml\n      # - undeploy-application:\n      #     cluster-name: eks-orb-demo-app-deployment\n      #     aws-region: $AWS_DEFAULT_REGION\n      #     requires:\n      #       - test-application\n      # - aws-eks/delete-cluster:\n      #     cluster-name: eks-orb-demo-app-deployment\n      #     aws-region: $AWS_DEFAULT_REGION\n      #     wait: true\n      #     requires:\n      #     - undeploy-application\n```\n\n![result](/img/workflow-result.png)\n\n클러스터 구성포함해서 총 20분정도 소요되었다. 실제 `eksctl`로 프로비저닝하게 되면 CloudFormation으로 수행되기 때문에 약 15-20분 정도 소요되니 간단한 빌드,배포,테스트는 5분정도 소요된것을 알 수 있다.  \n\n삭제는 역순으로 진행하거나 위에 주석 처리된 영역만 수행하면 된다.\n\n## 정리\n간단하게 CircleCI를 가지고 GitOps와 CI/CD를 구성하는 데모를 진행하였다.  \n\nCircleCI는 Jenkins와 유사하지만 Public서비스이고 Slave관리에 대해서 의존적이지 않기 때문에 기존에 Jenkins를 사용한 경험이 있다면 아주 쉽게 구성할 수 있다.  \n\n[https://circleci.com/docs/2.0/migrating-from-jenkins](https://circleci.com/docs/2.0/migrating-from-jenkins)를 참고하면 Jenkins와 차이점을 알 수 있는데 가장 큰 장점은 병렬로 테스트나 Job을 수행할수 있다는 점과 Lambda와 같은 서버리스 앱을 배포할때 정말 서버리스 환경으로 구성할수 있다는 점이다.  \n\n이는 Git Repository를 연동할때도 Java Plugin을 설치해야하는 번거로움을 덜 수 있다. 가장 중요한건 SaaS라는 장점도 무시할수 없다. Travis 무료 플랜과 비교해도 작은 프로젝트의 경우 별도의 Docker environment(2core, 4GB)를 제공받기 때문에 지연없이 바로 빌드가 가능하다는 점이다.  \n\n다음번 포스팅에는 terraform을 통해 ECR과 ECS로 배포하는 workflow를 살펴보려고 한다."
    },
    {
      "id": "kubernetes/CKAD-1/",
      "metadata": {
        "permalink": "/kubernetes/CKAD-1/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-06-10-CKAD-1.md",
        "source": "@site/blog/2019-06-10-CKAD-1.md",
        "title": "Yaml 작성 기본 Tip",
        "description": "CKAD검정시 YAML 작성 tip",
        "date": "2019-06-10T00:00:00.000Z",
        "formattedDate": "June 10, 2019",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "CKAD",
            "permalink": "/tags/ckad"
          },
          {
            "label": "Exam",
            "permalink": "/tags/exam"
          },
          {
            "label": "Tip",
            "permalink": "/tags/tip"
          }
        ],
        "readingTime": 1.1,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Yaml 작성 기본 Tip",
          "comments": true,
          "classes": "wide",
          "description": "CKAD검정시 YAML 작성 tip",
          "slug": "kubernetes/CKAD-1/",
          "date": "2019-06-10T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "CKAD",
            "Exam",
            "Tip"
          ]
        },
        "prevItem": {
          "title": "CircleCI - GitHub 연동 및 EKS 구성하기",
          "permalink": "/devops/circleci/"
        },
        "nextItem": {
          "title": "Knative on EKS",
          "permalink": "/kubernetes/knative-on-aws/"
        }
      },
      "content": "최근 포스팅을 할 여유가 되지 않아 일단 틈틈히 준비중인 CKAD 준비하는 팁이나 정보등을 먼저 정리하고자 한다.  \n그중 기본이 되는 YAML을 최초 작성할때 팁을 정리해봤다.  \n\n## Manifest YAML작성 Tip\nCLI안에서 Manifest YAML파일을 작성하는것은 쉽지 않다. 특히 시험중에 Copy/Paste를 하는것은 어렵고 느리게 진행될수 있으므로 CLI안에서 해결하는것이 좋다.\n\n### kubectl run 활용\n[https://kubernetes.io/docs/reference/kubectl/conventions/](https://kubernetes.io/docs/reference/kubectl/conventions/#generators)\n\n위 링크를 잘 기억해놓고 실제 시험을 볼때 template작성을 위해 아래와 같이 Manifest를 해보는것이 가장 좋다.\n\n#### Pod Manifest YAML\n```sh\n$ kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml\n```\n\n#### Deployment YAML\n```sh\n$ kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml\n```\n#### YAML로 저장\n```sh\n$ kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml > nginx-deployment.yaml\n```"
    },
    {
      "id": "kubernetes/knative-on-aws/",
      "metadata": {
        "permalink": "/kubernetes/knative-on-aws/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-05-14-knative-on-aws.md",
        "source": "@site/blog/2019-05-14-knative-on-aws.md",
        "title": "Knative on EKS",
        "description": "EKS에서도 Knative를 구성해보자",
        "date": "2019-05-14T00:00:00.000Z",
        "formattedDate": "May 14, 2019",
        "tags": [
          {
            "label": "Knative",
            "permalink": "/tags/knative"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "FaaS",
            "permalink": "/tags/faa-s"
          },
          {
            "label": "AWS",
            "permalink": "/tags/aws"
          },
          {
            "label": "Lambda",
            "permalink": "/tags/lambda"
          },
          {
            "label": "Serverless",
            "permalink": "/tags/serverless"
          },
          {
            "label": "EKS",
            "permalink": "/tags/eks"
          }
        ],
        "readingTime": 11.905,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Knative on EKS",
          "comments": true,
          "classes": "wide",
          "description": "EKS에서도 Knative를 구성해보자",
          "slug": "kubernetes/knative-on-aws/",
          "date": "2019-05-14T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Knative",
            "Kubernetes",
            "FaaS",
            "AWS",
            "Lambda",
            "Serverless",
            "EKS"
          ]
        },
        "prevItem": {
          "title": "Yaml 작성 기본 Tip",
          "permalink": "/kubernetes/CKAD-1/"
        },
        "nextItem": {
          "title": "eksworkshop",
          "permalink": "/kubernetes/eksworkshop/"
        }
      },
      "content": "이번에는 Knative를 EKS에 구성해보려고 한다. 4월은 발표 및 여러가지 업무로 인해서 포스팅을 할 시간이 부족했지만 5월부터 조금씩 여유가 생겨서 더 바빠지기전에 좀 써보려고 한다. 사실 [Facebook @최용호](https://www.facebook.com/yongho1037)님께서 한번 해보는것도 좋겠다고 하셔서 테스트를 진행하였다.  \n\n## Knative on EKS\n\n제목만 보면 Lambda나 ECS, Batch 서비스가 있는데 왜 이걸 써야하지? 라는 생각부터 하는 사람도 있을것 같다. 하지만 이전 포스팅들과 발표에서도 여러번 이야기 한것처럼 Knative는 간단하게 서버리스 워크로드를 빌드, 배포, 관리하기 위한 Kubernetes기반 FaaS플랫폼이고 현재 [CNCF Landscape](https://landscape.cncf.io/)에서 가장 빠른 속도로 개발되고 있는 오픈소스 프로젝트 중 하나이다.\n\n## EKS 배포\n\n앞선 [eksworkshop 포스팅](https://ddii.dev/kubernetes/eksworkshop/)에서처럼 간단한 배포를 위해 `eksctl`로 2-node 클러스터를 배포한다.  \n사전 준비사항은 [이전 포스팅](https://ddii.dev/kubernetes/eksworkshop/)이나 [https://eksctl.io/](https://eksctl.io/)을 참고하자.  \n\n```sh\n$ eksctl create cluster --name=eksworkshop-eksctl --nodes=2 --node-ami=auto\n```\n\n생성된 클러스터 상태를 확인한다. \n\n```sh\n$ kubectl get nodes\nNAME                                                STATUS    ROLES     AGE       VERSION\nip-192-168-24-168.ap-northeast-2.compute.internal   Ready     <none>    2m        v1.11.9\nip-192-168-78-204.ap-northeast-2.compute.internal   Ready     <none>    2m        v1.11.9\n```\n\n## istio 설치\n\nKnative는 istio 의존성을 가지고 있기 때문에 Istio를 먼저 배포한다. 물론 [이전 포스팅](https://ddii.dev/kubernetes/knative-using-gloo/)에서 설명한것 처럼 Gloo를 사용해도 되지만 이번에는 Istio를 설치하였다.  \n\n```sh\n$ kubectl apply -f https://github.com/knative/serving/releases/download/v0.5.0/istio-crds.yaml \n$ kubectl apply -f https://github.com/knative/serving/releases/download/v0.5.0/istio.yaml\n```\n\n배포된 Pods, Services, Replicasets 을 확인한다.\n\n```sh\n$ kubectl get pods -n istio-system\nNAME                                     READY     STATUS      RESTARTS   AGE\ncluster-local-gateway-65c8b667c8-269cf   1/1       Running     0          17m\nistio-citadel-76bd44d8f7-5cltj           1/1       Running     0          17m\nistio-cleanup-secrets-7jpth              0/1       Completed   0          17m\nistio-egressgateway-b9d56b4f8-pdjhs      1/1       Running     0          17m\nistio-galley-7db7db89db-5stkp            1/1       Running     0          17m\nistio-ingressgateway-f77fbc787-npw9d     1/1       Running     0          17m\nistio-pilot-69f975bf4f-4dq4d             2/2       Running     0          17m\nistio-pilot-69f975bf4f-q9g8g             2/2       Running     0          16m\nistio-pilot-69f975bf4f-tnm92             2/2       Running     0          16m\nistio-policy-8db48cbcd-dl2th             2/2       Running     0          17m\nistio-security-post-install-xv8z6        0/1       Completed   0          17m\nistio-sidecar-injector-cd54ffccd-kj2n6   1/1       Running     0          17m\nistio-telemetry-d78cd45db-8x2cw          2/2       Running     0          17m\n\n$ kubectl get svc -n istio-system\nNAME                     TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                                                                                                                   AGE\ncluster-local-gateway    ClusterIP      10.100.97.146    <none>                                                                        80/TCP,443/TCP,31400/TCP,15011/TCP,8060/TCP,15030/TCP,15031/TCP                                                           17m\nistio-citadel            ClusterIP      10.100.147.235   <none>                                                                        8060/TCP,9093/TCP                                                                                                         17m\nistio-egressgateway      ClusterIP      10.100.25.44     <none>                                                                        80/TCP,443/TCP                                                                                                            17m\nistio-galley             ClusterIP      10.100.158.45    <none>                                                                        443/TCP,9093/TCP                                                                                                          17m\nistio-ingressgateway     LoadBalancer   10.100.62.157    a4e1d5201758f11e9b7f402c4d4b0376-510368564.ap-northeast-2.elb.amazonaws.com   80:31000/TCP,443:30753/TCP,31400:31922/TCP,15011:31331/TCP,8060:30389/TCP,853:30257/TCP,15030:30827/TCP,15031:30153/TCP   17m\nistio-pilot              ClusterIP      10.100.25.194    <none>                                                                        15010/TCP,15011/TCP,8080/TCP,9093/TCP                                                                                     17m\nistio-policy             ClusterIP      10.100.73.149    <none>                                                                        9091/TCP,15004/TCP,9093/TCP                                                                                               17m\nistio-sidecar-injector   ClusterIP      10.100.117.18    <none>                                                                        443/TCP                                                                                                                   17m\nistio-telemetry          ClusterIP      10.100.87.12     <none>                                                                        9091/TCP,15004/TCP,9093/TCP,42422/TCP                                                                                     17m\n\n$ kubectl get rs -n istio-system\nNAME                               DESIRED   CURRENT   READY     AGE\ncluster-local-gateway-65c8b667c8   1         1         1         17m\nistio-citadel-76bd44d8f7           1         1         1         17m\nistio-egressgateway-b9d56b4f8      1         1         1         17m\nistio-galley-7db7db89db            1         1         1         17m\nistio-ingressgateway-f77fbc787     1         1         1         17m\nistio-pilot-69f975bf4f             3         3         3         17m\nistio-policy-8db48cbcd             1         1         1         17m\nistio-sidecar-injector-cd54ffccd   1         1         1         17m\nistio-telemetry-d78cd45db          1         1         1         17m\n```\n\nIstio injection을 하기 위해 배포할 defaults namespace 전체에 Labeling을 한다.\n\n```sh\n$ kubectl label namespace default istio-injection=enabled\nnamespace/default labeled\n\n$ kubectl get namespaces --show-labels\nNAME           STATUS    AGE       LABELS\ndefault        Active    43m       istio-injection=enabled\nistio-system   Active    27m       istio-injection=disabled\nkube-public    Active    43m       <none>\nkube-system    Active    43m       <none>\n```\n\n## Knative 설치\n\nbuild, evening, serving 및 모니터링 리소스를 배포한다.\n\n```sh\nkubectl apply -f https://github.com/knative/serving/releases/download/v0.5.0/serving.yaml \\\n   -f https://github.com/knative/build/releases/download/v0.5.0/build.yaml \\\n   -f https://github.com/knative/eventing/releases/download/v0.5.0/release.yaml \\\n   -f https://github.com/knative/eventing-sources/releases/download/v0.5.0/eventing-sources.yaml \\\n   -f https://github.com/knative/serving/releases/download/v0.5.0/monitoring.yaml \\\n   -f https://raw.githubusercontent.com/knative/serving/v0.5.0/third_party/config/build/clusterrole.yaml\n```\n\n모든 Knative namespaces 및 resources를 확인한다.\n\n```sh\n$ kubectl get namespaces | grep knative\nknative-build        Active    8m\nknative-eventing     Active    8m\nknative-monitoring   Active    8m\nknative-serving      Active    8m\nknative-sources      Active    8m\n\n$ kubectl get pods -n knative-serving\nNAME                          READY     STATUS    RESTARTS   AGE\nactivator-664b5b9598-dsjvq    2/2       Running   0          10m\nautoscaler-64d5bd84b8-bqghq   2/2       Running   0          10m\ncontroller-658b9d5c6c-tnwvz   1/1       Running   0          10m\nwebhook-5dffbfbb8b-525vt      1/1       Running   0          10m\n\n$ kubectl get pods -n knative-build \nNAME                                READY     STATUS    RESTARTS   AGE\nbuild-controller-86f5b5b96d-zg8j2   1/1       Running   0          11m\nbuild-webhook-6fddd7c6df-68fkw      1/1       Running   0          11m\n\n$ kubectl get pods -n knative-eventing \nNAME                                            READY     STATUS    RESTARTS   AGE\neventing-controller-6fdccd8c95-bckws            1/1       Running   0          11m\nin-memory-channel-controller-6fddb6655f-vbc64   1/1       Running   0          11m\nin-memory-channel-dispatcher-7684cd7c7d-ftqhc   2/2       Running   2          11m\nwebhook-d496c66bd-688xz                         1/1       Running   0          11m\n```\n\n```sh\n$ kubectl get pods -n knative-monitoring \nNAME                                  READY     STATUS    RESTARTS   AGE\nelasticsearch-logging-0               1/1       Running   0          14m\nelasticsearch-logging-1               1/1       Running   0          12m\ngrafana-754bc795bb-wxwml              1/1       Running   0          14m\nkibana-logging-7f7b9698bc-rrnw6       1/1       Running   0          14m\nkube-state-metrics-5bccdf746f-fhv7t   4/4       Running   0          12m\nnode-exporter-w9jlq                   2/2       Running   0          14m\nnode-exporter-wgv2j                   2/2       Running   0          14m\nprometheus-system-0                   1/1       Running   0          14m\nprometheus-system-1                   1/1       Running   0          14m\n```\n\n모니터링 리소스도 확인한다. 위의 리소스를 보면 Fluentd가 배포되지 않은 상태이므로 DaemonSet으로 동작할수 있도록 아래와 같이 설정하면 DaemonSet을 확인할 수 있다.  \n\n```sh\n$ kubectl label nodes --all beta.kubernetes.io/fluentd-ds-ready=\"true\"\n```\n\n## Build에서 사용할 Docker Credential 설정\n\n일단 Knative Build를 수행할때 일반적으로 Container Registry를 많이 사용하기 때문에 Registry Credential 설정을 해야한다.  \n\nECR의 경우 [https://github.com/knative/build-templates](https://github.com/knative/build-templates)의 ecr_helper를 사용하면 쉽게 ECR account를 설정할 수 있지만 `Serving`단계에서 401에러가 나는 이유를 잡지 못해서 일단 Dockerhub를 가지고 진행하였다.\n\n간단한 데모코드는 아래 repository에 미리 작성해놨다.  \n\n[https://github.com/ddiiwoong/hello-python.git](https://github.com/ddiiwoong/hello-python.git)\n\n`docker-secret.yaml` 을 보면 dockerhub push를 위해 basic-auth를 수행하게 되는데 dockerhub id/password 를 base64로 encoding해서 Secret으로 저장한다.  \n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: basic-user-pass\n  annotations:\n    build.knative.dev/docker-0: https://index.docker.io/v1/\ntype: kubernetes.io/basic-auth\ndata:\n  # Use 'echo -n \"username\" | base64' to generate this string\n  username: BASE64_ENCODED_USERNAME\n  # Use 'echo -n \"password\" | base64' to generate this string\n  password: BASE64_ENCODED_PASSWORD\n```\n\n그리고 ServiceAccount `build-bot`을 생성하기 위한 yaml (sa.yaml)를 작성한다.  \n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: build-bot\nsecrets:\n  - name: basic-user-pass\n```\n\n위에서 작성한 Secret(basic-user-pass)과 ServiceAccount(build-bot)를 배포한다.  \n\n\n```sh\n$ kubectl apply -f docker-secret.yaml\nsecret \"basic-user-pass\" created\n$ kubectl apply -f sa.yaml\nserviceaccount \"build-bot\" created\n```\n\n저장된 Secret(basic-user-pass)과 ServiceAccount(build-bot)를 확인한다.\n\n```sh\n$ kubectl get secret\nNAME                        TYPE                                  DATA      AGE\nbasic-user-pass             kubernetes.io/basic-auth              2         2h\nbuild-bot-token-gfkg9       kubernetes.io/service-account-token   3         2h\nbuilder-token-kp7ww         kubernetes.io/service-account-token   3         10h\ndefault-token-9cpp5         kubernetes.io/service-account-token   3         10h\necr-creds                   kubernetes.io/basic-auth              2         10h\nistio.build-bot             istio.io/key-and-cert                 3         2h\nistio.builder               istio.io/key-and-cert                 3         10h\nistio.default               istio.io/key-and-cert                 3         10h\nistio.knative-serve         istio.io/key-and-cert                 3         2h\nknative-serve-token-9j82l   kubernetes.io/service-account-token   3         2h\n\n$ kubectl get sa\nNAME            SECRETS   AGE\nbuild-bot       2         2h\nbuilder         2         10h\ndefault         1         10h\nknative-serve   2         2h\n```\n\n## Python 코드 및 Dockerfile 작성\n\nTARGET 환경변수를 받아서 Hello World와 같이 출력하는 간단한 Flask기반 앱을 작성한다.  \n\n간단한 데모코드는 아래 repository에 미리 작성해놨다.  \n\n[https://github.com/ddiiwoong/hello-python.git](https://github.com/ddiiwoong/hello-python.git)\n\n```py\nimport os\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    target = os.environ.get('TARGET', 'NOT SPECIFIED')\n    return 'Hello World: {}!\\n'.format(target)\n\nif __name__ == \"__main__\":\n    app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 8080)))\n```\n\n위 앱(app.py)을 배포하는 Dockerfile을 작성한다.\n\n```dockerfile\nFROM python:alpine\n\nENV APP_HOME /app\nCOPY . $APP_HOME\nWORKDIR $APP_HOME\n\nRUN pip install Flask\n\nENTRYPOINT [\"python\"]\nCMD [\"app.py\"]\n```\n\n## Knative Build \n\n미리 Dockerhub에서 `hello-python` repository(docker.io/ddiiwoong/hello-python)를 생성한다.  \n그리고 위에서 생성한 `build-bot` 계정과 image tag `hello-python`정보를 Build template에 작성하여 배포한다.  \n아래 Knative Build 과정은 `kaniko executor`를 사용하여 다음과 같은 과정을 수행한다.  \n1. `spec.source` 에서 Source Clone (git)를 수행하고 이후  \n2. `spec.steps` 에서 Docker Build, Tag, Registry Login, Push를 수행한다.  \n\n```yaml\napiVersion: build.knative.dev/v1alpha1\nkind: Build\nmetadata:\n  name: python-build\nspec:\n  serviceAccountName: build-bot\n  source:\n    git:\n      url: https://github.com/ddiiwoong/hello-python.git\n      revision: master\n  steps:\n  - name: build-and-push\n    image: gcr.io/kaniko-project/executor:v0.1.0\n    args:\n    - --dockerfile=/workspace/Dockerfile\n    - --destination=docker.io/ddiiwoong/hello-python:latest\n```\n\n`build`를 수행한다. \n\n```sh\n$ kubectl apply -f build.yaml\nbuild.build.knative.dev/python-build created\n\n$ kubectl get build\nNAME           SUCCEEDED   REASON    STARTTIME   COMPLETIONTIME\npython-build   True                  18m\n```\n\n정상적으로 Build가 완료되면 Dockerhub에 정의한 TAG(ddiiwoong/hello-python)로 이미지가 등록된걸 확인할 수 있다.  \n```sh\n$ docker pull ddiiwoong/hello-python\nUsing default tag: latest\nlatest: Pulling from ddiiwoong/hello-python\ne7c96db7181b: Pull complete\n799a5534f213: Pull complete\n913b50bbe755: Pull complete\n11154abc6081: Pull complete\nc805e63f69fe: Pull complete\n6eabcf0f7a50: Pull complete\n74101057f4ec: Pull complete\nDigest: sha256:51dc4a7ce38a5e7894adcfc00eaee6c5ea6aca1ef6c7521f9b7ea6382c013b9b\nStatus: Downloaded newer image for ddiiwoong/hello-python:latest\n```\n\n## Knative Serving 으로 배포\n\n#### service.yaml\n```yaml\napiVersion: serving.knative.dev/v1alpha1\nkind: Service\nmetadata:\n  name: helloworld-python\n  namespace: default\nspec:\n  runLatest:\n    configuration:\n      revisionTemplate:\n        spec:\n          container:\n            image: ddiiwoong/hello-python:latest\n            env:\n            - name: TARGET\n              value: \"Python Sample v1 with Knative on EKS\"\n```\n\n빌드된 image (ddiiwoong/hello-python:latest)로 Knative Service를 생성한다.  \n\n```sh\n$ kubectl apply -f service.yaml\nservice.serving.knative.dev/helloworld-python created\n```\n\n`istio-system` Namespace에 `istio-ingressgateway` Service를 확인한다.  \n\n```sh\nkubectl get svc istio-ingressgateway --namespace istio-system\nNAME                   TYPE           CLUSTER-IP      EXTERNAL-IP                                                                    PORT(S)                                                                                                                   AGE\nistio-ingressgateway   LoadBalancer   10.100.208.80   a220723d475df11e980220a02e220b34-2021915778.ap-northeast-2.elb.amazonaws.com   80:31581/TCP,443:31490/TCP,31400:30367/TCP,15011:32495/TCP,8060:31418/TCP,853:30310/TCP,15030:32405/TCP,15031:31410/TCP   13h\n```\n  \nALB: a220723d475df11e980220a02e220b34-2021915778.ap-northeast-2.elb.amazonaws.com  \n\n위와 같이 AWS LoadBalancer로 배포면 ALB가 자동으로 생성되므로 추후 eksctl로 클러스터를 삭제하기 전에 반드시 LoadBalancer 형태로 배포된 서비스를 삭제하고 진행해야 한다.  \n\nKnative Service를 확인한다. \n```sh\n$ kubectl get ksvc\nNAME                DOMAIN                                  LATESTCREATED             LATESTREADY               READY     REASON\nhelloworld-python   helloworld-python.default.example.com   helloworld-python-4rw95   helloworld-python-4rw95   True\n```\n\n위에서 얻은 두가지 정보(LoadBalancer, Domain)로 생성된 app을 테스트한다.  \nKnative는 내부적으로 example.com이라고 하는 기본 domain을 사용하므로 내부적으로는 `helloworld-python.default.example.com`으로 curl을 실행하게 된다.  \n\n```sh\n$ curl -H \"Host:helloworld-python.default.example.com\" http://a220723d475df11e980220a02e220b34-2021915778.ap-northeast-2.elb.amazonaws.com\nHello World: Python Sample v1 with Knative on EKS!\n```\n\n`Cold Start`(default timeout 5분) 때문에 잠시 응답이 늦어질 수도 있지만(2-5초) 잠시 기다리면 위처럼 결과를 확인할 수 있다.  \n\n## 정리\n\n위에서도 잠깐 언급했지만 Lambda나 ECS, Batch 서비스가 있는데 Knative on EKS 를 구지 왜 하느냐라고 궁금해하는 분들이 계실지도 모른다. 하지만 다들 아래와 같은 고민을 해봤을거라 생각한다.  \n\n컨테이너와 Kubernetes가 DevOps도구로서의 표준이 되어가는 시대에 Lambda를 쓸지 오픈소스를 쓸지에 대한 고민은 결국 이식성과 벤더 종속성을 제거하고 운영효율화 측면에서 그 답을 찾을수 있을것 같다.  \n\n현재 몸담고 있는 최근 프로젝트에서 Lambda, ECS, Batch 등을 사용하는 경우가 많아졌는데 실제 엔터프라이즈에서 정해진 자원내에서 정해진 일을 할때는 매니지드 서비스가 적합하다고 생각한다. 하지만 On-Premise 또는 그에 준하는 Kubernetes 클러스터를 운영하는 조직에서는 Knative를 사용하여 컨테이너 기반 서비리스 워크로드를 구현하는 것이 향후 Hybrid 관점에서 확장성과 벤더 종속성을 제거하는데 큰 도움이 될것이라 생각한다.  \n\n조금씩 Kubernetes 및 생태계 Learning에 대한 피로도가 증가하고 있고 Hype Driven Development(설레발 주도개발)가 되는것 같아서 아쉬운 부분은 있지만 현재 가장 핫한 기술이고 관심도가 높기 때문에 배워두면 언젠가는 써먹게 될거라 확신한다.  \n\n다시한번 Conference driven development(architecture)가 되지 않도록 자중하고 Loudest Guy가 되지 않도록 조심해야할 것 같다."
    },
    {
      "id": "kubernetes/eksworkshop/",
      "metadata": {
        "permalink": "/kubernetes/eksworkshop/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-03-29-eksworkshop.md",
        "source": "@site/blog/2019-03-29-eksworkshop.md",
        "title": "eksworkshop",
        "description": "eskworkshop 튜토리얼을 실행하고 간단한 Microservice앱을 배포해보자",
        "date": "2019-03-29T00:00:00.000Z",
        "formattedDate": "March 29, 2019",
        "tags": [
          {
            "label": "eks",
            "permalink": "/tags/eks"
          },
          {
            "label": "aws",
            "permalink": "/tags/aws"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "eskworkshop",
            "permalink": "/tags/eskworkshop"
          },
          {
            "label": "kubectl",
            "permalink": "/tags/kubectl"
          },
          {
            "label": "Managed Kubernetes",
            "permalink": "/tags/managed-kubernetes"
          }
        ],
        "readingTime": 14.23,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "eksworkshop",
          "comments": true,
          "classes": "wide",
          "description": "eskworkshop 튜토리얼을 실행하고 간단한 Microservice앱을 배포해보자",
          "slug": "kubernetes/eksworkshop/",
          "date": "2019-03-29T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "eks",
            "aws",
            "Kubernetes",
            "eskworkshop",
            "kubectl",
            "Managed Kubernetes"
          ]
        },
        "prevItem": {
          "title": "Knative on EKS",
          "permalink": "/kubernetes/knative-on-aws/"
        },
        "nextItem": {
          "title": "git-sync",
          "permalink": "/kubernetes/git-sync/"
        }
      },
      "content": "## Amazon EKS (Elastic Container Service for Kubernetes)\n\nKubernetes Managed Service인 Amazon EKS가 2018년 7월 출시되고 2019년 1월 정식으로 서울리전에 출시되었다. 개인적으로 완전관리형 Kubernetes 출시가 늦어져서 AWS 행보가 다소 늦다고 생각은 했으나 ALB와 VPC연동, 여러가지 기존 OpenSource와의 연결고리를 배제하고 자체 Managed서비스와 연동할것들이 많기 때문에 당연히 타사에 비해 늦어진걸로 보인다. 언제나 그랬지만 오픈소스를 받아들이는 느낌이 아니라 뭔가 완성된 제품을 쓰는 느낌(?)이다. 물론 불편한 부분과 감수해야할 내용들은 조금 있지만 기존 AWS 충성 User에게 호응을 얻을수 있기 때문이 아닐까라는 생각을 해보면서 포스팅을 시작하려고 한다.  \n\n오픈소스가 아닌 Managed서비스에 대한 리뷰는 처음이지만 4월 10일 [AWS판교소모임](https://www.meetup.com/ko-KR/awskrug/events/260024327/) 발표도 있고, 실제 고려중인 아키텍처에 포함이 되어야 하는 EKS에 대해서 살펴보고 eskworkshop 튜토리얼을 실행해보면서 다른 관리형 Kubernetes 서비스들에 비해 어떤 사항을 좀더 고민해야 하는지 정리해보고 넘어가면 좋을듯 하다.\n\n## eksworkshop.com\n\n[https://eksworkshop.com/](https://eksworkshop.com/)  \nKubernete를 처음접하는 유저를 위한 기본 개념과 아키텍처, 그리고 VPC, ALB를 활용하여 EKS에 대한 설치, 구성, 데모앱 배포 등을 해볼수 있는 튜토리얼 사이트이다.  \n\n[AWSKRUG](https://www.facebook.com/groups/awskrug/)에서 한글화 작업도 진행중이다.  [한글화 링크](https://awskrug.github.io/eks-workshop/deploy/)\n\n## eksworkshop 따라하기전 사전 준비사항\n\neksworkshop에서는 기본적으로 workshop이라는 신규 IAM 계정을 생성하고 Cloud9 Workspace 와 몇가지 설정들을 진행하지만 [AWS판교소모임](https://www.meetup.com/ko-KR/awskrug/events/260024327/)을 위해 최대한 비용이 드는 구성요소를 배제하고 작성하고자 한다.\n\n### AWS account\nFree Tier는 EKS를 자유롭게 활용할수 없다.  \n관련 issue - [https://github.com/aws/containers-roadmap/issues/78](https://github.com/aws/containers-roadmap/issues/78)  \n\n실제 사용중인 계정이나 Credit이 확보된 계정이 필요하다.  \n\n### IAM 설정 (JSON template)\nEKSworkshop에서는 Full administrator 계정을 필요로 하지만 eksctl로 배포를 진행하므로 그 기준으로 IAM설정을 진행한다.  \n\n[terraform eks iam 설정](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/eks_test_fixture/README.md)을 참고하려고 했지만 eksctl과 terraform과의 약간 다른 방식의 배포로 인해 어쩔수없이 EKS Full Access권한을 할당하였다.  \n(다른 유경험자의 도움이 필요한 상황 ㅠㅠ)\n\n자세한 JSON 내용은 [링크](https://github.com/ddiiwoong/eksworkshop/blob/master/iam_for_eksworkshop.json)를 참고한다.  \n\n### kubectl, aws-iam-authenticator\n* kubectl : kubernetes CLI\n* aws-iam-authenticator : AWS IAM Authenticator CLI\n\n#### kubectl config를 저장하기 위해 .kube directory를 생성\n\n```bash\n$ mkdir -p ~/.kube\n``` \n\n#### kubectl 설치 (linux)\n```bash\n$ sudo curl --silent --location -o /usr/local/bin/kubectl \"https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/kubectl\"\n$ sudo chmod +x /usr/local/bin/kubectl\n```\n\n#### kubectl 설치 (MacOS Homebrew)\nMacOS는 brew로 설치하였다.  \n[https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-with-homebrew-on-macos](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-with-homebrew-on-macos)\n```bash\n$ brew install kubernetes-cli\n```\n\n#### kubectl 설치 (windows PowerShell)\n[https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/install-kubectl.html](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/install-kubectl.html)\n\n```\ncurl -o kubectl.exe https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/windows/amd64/kubectl.exe\n```\n\n\n#### kubectl 설치 확인\n\n현재 MacOS에서는 1.11.7 버전이 설치되어 있다. (다른경로로 설치됨)\n```bash\n$ kubectl version --short --client\nClient Version: v1.11.7-dispatcher\n```\n\n#### aws-iam-authenticator 설치\nAmazon EKS는 IAM을 사용하여 Kubernetes용 AWS IAM Authenticator를 통해 Kubernetes 클러스터에 인증을 제공한다.  Go(버전 1.7이상)가 설치되어 있으면 아래와 같이 진행하면 된다. \n```bash\n$ go get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator\n$ sudo mv ~/go/bin/aws-iam-authenticator /usr/local/bin/aws-iam-authenticator\n```\n\n만약 Go설치가 안되어 있다면 다음 링크를 통해 설치 진행할수 있다.  \n[https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/install-aws-iam-authenticator.html](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/install-aws-iam-authenticator.html)\n\n#### aws-iam-authenticator binary 다운로드\n\nLinux: [https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/aws-iam-authenticator](https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/aws-iam-authenticator)  \nMacOS: [https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/darwin/amd64/aws-iam-authenticator](https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/darwin/amd64/aws-iam-authenticator)  \nWindows: [https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/windows/amd64/aws-iam-authenticator.exe](https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/windows/amd64/aws-iam-authenticator.exe)\n\n##### MacOS의 경우 \n```bash\n$ curl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/darwin/amd64/aws-iam-authenticator\n$ chmod +x ./aws-iam-authenticator\n$ cp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator && export PATH=$HOME/bin:$PATH\n```\n\n##### MacOS bash 환경변수 추가\n```bash\n$ echo 'export PATH=$HOME/bin:$PATH' >> ~/.bash_profile\n```\n\n##### MacOS zsh 환경변수 추가\n```bash\n$ echo 'export PATH=$HOME/bin:$PATH' >> ~/.zshrc\n```\n\n#### aws-iam-authenticator binary 확인\n```bash\n$ aws-iam-authenticator help\n```\n\n### eksctl\n\n![mascot](https://github.com/weaveworks/eksctl/blob/master/logo/eksctl.png?raw=true)\n\neksctl은 weaveworks에서 contribute하고 있는 오픈소스로 EKS 클러스터를 생성하는 간단한 CLI 도구이다. Go로 작성되어 있고 CloudFormation을 기본으로 동작한다.  \n\n[https://eksctl.io/](https://eksctl.io/)  \n[https://github.com/weaveworks/eksctl](https://github.com/weaveworks/eksctl)\n\n#### eksctl binary 다운로드\n```bash\n$ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n$ sudo mv -v /tmp/eksctl /usr/local/bin\n```\n\n#### MacOS Homebrew 설치\n```bash\n$ brew tap weaveworks/tap\n$ brew install weaveworks/tap/eksctl\n```\n\n#### Windows 설치 (PowerShell, chocolatey)\n```\nchocolatey install eksctl\n```\n\n#### eksctl 동작확인\n```bash\n$ eksctl version\n```\n\n#### CLI API 자격증명 구성\n\n사전설정한 aws configure가 있는지 확인. 기존 Terraform이나 kops 사용한적이 있으면 건너뛰어도 된다.  \n```bash\n$ ls  ~/.aws\n```\n없을경우 aws 자격증명을 생성한다.\n\n#### ~/.aws/credentials\n```\n[default]\naws_access_key_id={EXAMPLE}\naws_secret_access_key={EXAMPLEKEY}\n```\n\n#### ~/.aws/config\n```\n[default]\nregion=ap-northeast-2\noutput=json\n```\n\n## EKS 배포\nkubectl, aws-iam-authenticator, eksctl, AWS 자격증명 환경까지 구성되어 있으면 바로 배포가 가능하다.  \n\n```bash\n$ eksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto\n[ℹ]  using region ap-northeast-2\n[ℹ]  setting availability zones to [ap-northeast-2a ap-northeast-2c ap-northeast-2a]\n[ℹ]  subnets for ap-northeast-2a - public:192.168.0.0/19 private:192.168.96.0/19\n[ℹ]  subnets for ap-northeast-2c - public:192.168.32.0/19 private:192.168.128.0/19\n[ℹ]  subnets for ap-northeast-2a - public:192.168.64.0/19 private:192.168.160.0/19\n[ℹ]  nodegroup \"ng-cfb3cb01\" will use \"ami-0c7972077aa002104\" [AmazonLinux2/1.11]\n[ℹ]  creating EKS cluster \"eksworkshop-eksctl\" in \"ap-northeast-2\" region\n[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup\n[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-2 --name=eksworkshop-eksctl'\n[ℹ]  creating cluster stack \"eksctl-eksworkshop-eksctl-cluster\"\n[ℹ]  creating nodegroup stack \"eksctl-eksworkshop-eksctl-nodegroup-ng-cfb3cb01\"\n[ℹ]  --nodes-min=2 was set automatically\n[ℹ]  --nodes-max=2 was set automatically\n[✔]  all EKS cluster resource for \"eksworkshop-eksctl\" had been created\n[✔]  saved kubeconfig as \"/Users/ddii/.kube/config\"\n[ℹ]  nodegroup \"ng-cfb3cb01\" has 0 node(s)\n[ℹ]  waiting for at least 2 node(s) to become ready in \"ng-cfb3cb01\"\n[ℹ]  nodegroup \"ng-cfb3cb01\" has 2 node(s)\n[ℹ]  node \"ip-192-168-42-42.ap-northeast-2.compute.internal\" is ready\n[ℹ]  node \"ip-192-168-66-165.ap-northeast-2.compute.internal\" is ready\n[ℹ]  kubectl command should work with \"/Users/ddii/.kube/config\", try 'kubectl get nodes'\n[✔]  EKS cluster \"eksworkshop-eksctl\" in \"ap-northeast-2\" region is ready\n```\n\n서울리전(ap-northeast-2)기준 약 15-20분이 소요되며 `eksctl` 기본 설정값은 다음과 같다.\n\n* 클러스터명은 자동생성, --name 옵션으로 지정가능 (eksworkshop-eksctl)\n* CloudFormation : eksctl-{$Cluster_Name}-cluster\n* m5.large * 2 instances ([EKS Instance Type](https://github.com/awslabs/amazon-eks-ami/blob/7f6c8cb3597e17f6e5f7df96d12bccf5604dc909/amazon-eks-nodegroup.yaml) NodeInstanceType.AllowedValues 참고)\n* Default AMI : AWS EKS AMI (custom EKS AMI 가능 - Packer활용)\n* Default Region : us-west-2\n* dedicated VPC : 192.168.0.0/16 \n* kubernetes version : 1.11.x ([EKS Version](https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/platform-versions.html) 참고)\n* StorageClass : gp2 ([AWS EBS](https://kubernetes.io/docs/concepts/storage/storage-classes/#aws-ebs) 참고)\n* CNI : [Amazon VPC](https://github.com/aws/amazon-vpc-cni-k8s)\n* Node Autoscaler : --node-min, --node-max Auto Scaling 설정\n* 기본 Pod 개수 : [참고](https://github.com/awslabs/amazon-eks-ami/blob/7f6c8cb3597e17f6e5f7df96d12bccf5604dc909/files/eni-max-pods.txt)\n* nodegroup : worker가 포함되는 group \n* kubeconfig : ~/.kube/config 로 통합됨\n \n### Config File 사용\n\n[https://github.com/weaveworks/eksctl/tree/master/examples](https://github.com/weaveworks/eksctl/tree/master/examples)를 참고하여 `YAML`형태로 작성하여 배포가능하다. \n\n```bash\n$ eksctl create cluster -f example.yaml\n```\n\n기존에 관리하는 VPC subnet정보 및 AutoScaling, AZ(availabilityZones)설정, nodegroup 관리, node Instance에 preBootstrapCommand등을 아래 예시와 같이 미리 작성하면 GitOps측면에서 활용도가 더욱 높아질수 있다.\n\n#### 05-advanced-nodegroups.yaml\n```yaml\n# An advanced example of ClusterConfig object with customised nodegroups:\n--- \napiVersion: eksctl.io/v1alpha4\nkind: ClusterConfig\n\nmetadata:\n  name: cluster-5\n  region: eu-west-2\n\nnodeGroups:\n  - name: ng1-public\n    instanceType: m5.xlarge\n    minSize: 2\n    maxSize: 8\n    labels:\n      nodegroup-type: frontend-workloads\n    iam:\n      withAddonPolicies:\n        autoScaler: true\n\n  - name: ng2-private-a\n    instanceType: m5.large\n    desiredCapacity: 2\n    labels:\n      nodegroup-type: backend-cluster-addons\n    privateNetworking: true\n    preBootsrapCommand:\n      # allow docker registries to be deployed as cluster service \n      - 'echo {\\\"insecure-registries\\\": [\\\"172.20.0.0/16\\\",\\\"10.100.0.0/16\\\"]} > /etc/docker/daemon.json'\n      - \"systemctl restart docker\"\n\n  - name: ng3-private-b\n    instanceType: c3.8xlarge\n    desiredCapacity: 4\n    labels:\n      nodegroup-type: very-special-science-workloads\n    privateNetworking: true\n    availabilityZones: [\"eu-west-2a\"] # use single AZ to optimise data transfer between isntances\n    preBootstrapCommand:\n      # disable hyperthreading\n      - \"for n in $(cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | cut -s -d, -f2- | tr ',' '\\n' | sort -un); do echo 0 > /sys/devices/system/cpu/cpu${n}/online; done\"\n\n# cluster AZs must be set explicitly for single AZ nodegroup example to work\navailabilityZones: [\"eu-west-2a\", \"eu-west-2b\", \"eu-west-2c\"]\n```\n\n## Kubernetes 대시보드 배포\n\nKubernetes 공식 대시보드는 기본으로 배포되지 않기 때문에 수동으로 배포해야한다. 설치 방법은 [공식 문서](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/)에서 확인가능하다.  \n\n위에서 구성된 클러스터에서 Kubernetes 대시보드를 배포한다.\n```bash\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n```\n\n접속을 위해 kube-proxy 기능을 활용하여 8080포트로 expose를 진행한다. 모든 인터페이스에서 필터링없이 접속이 가능하도록 옵션을 지정한다. \n아래 명령은 터미널에서 백그라운드로 계속 동작한다.  \n```bash\n$ kubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true &\nW0328 16:39:09.061754    9100 proxy.go:139] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious\n```\n\nTOKEN으로 접속하기 위해 aws-iam-authenticator를 통해 해당 클러스터 token을 확인한다.  \n```bash\n$ aws-iam-authenticator token -i eksworkshop-eksctl --token-only\nk8s-aws-v1.aHR0cHM6Ly9zdHMuYXAtbm9ydGhlYXN0LTIuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFKMjVNVjJSVVZQNlRWTURBJTJGMjAxOTAzMjglMkZhcC1ub3J0aGVhc3QtMiUyRnN0cyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMTkwMzI4VDA3NDEwNVomWC1BbXotRXhwaXJlcz0wJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCUzQngtazhzLWF3cy1pZCZYLUFtei1TaWduYXR1cmU9Yjc0NzkzYzUwOTU5NDYwMzMxMjY2YjExYWY4ODBkM2Q2OWQ5MWRhYzFhZWY1NjZmZTAwNTNlNWY2MTM0NGFlZQ\n```\n\n그냥 localhost:8080 으로만 접속하면 구성된 클러스터 kube API list를 확인되므로 아래 경로로 접속한다.  \n위에서 출력된 TOKEN값으로 로그인한다. Token 세션 Timeout이 있으니 세션만료시 `aws-iam-authenticator` 명령을 통해 갱신값을 입력하면 된다.  \n```\nhttp://localhost:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login\n```\n![dashboard](/img/k8s_dashboard.png)\n\n\n## Microservice app 배포\n가장 기본적인 [Sock Shop](https://microservices-demo.github.io/)을 배포하기 위해 Git Clone 수행\n```bash\n$ git clone https://github.com/microservices-demo/microservices-demo.git sockshop\n$ cd sockshop/deploy/kubernetes\n```\n\n`NodePort`로 되어있는 Front-End Service를 `LoadBalancer`로 수정한다.\n```bash\n$ sed -i 's/NodePort/LoadBalancer/g' complete-demo.yaml\n$ cat complete-demo.yaml | grep LoadBalancer\n type: LoadBalancer\n```\n\n\nnamespace 생성 및 sock-shop 배포\n```bash\n$ kubectl create namespace sock-shop\n$ kubectl apply -f complete-demo.yaml\n$ kubectl get all\n```\n\n서비스 접속확인을 위해서는 ALB배포시간(DNS전파)이 일정 소요되므로 잠시후 접속을 시도한다.\n```bash\n$ kubectl get svc\nNAME           TYPE           CLUSTER-IP       EXTERNAL-IP                                                                 PORT(S)        AGE\ncarts          ClusterIP      10.100.84.58     <none>                                                                      80/TCP         1h\ncarts-db       ClusterIP      10.100.227.156   <none>                                                                      27017/TCP      1h\ncatalogue      ClusterIP      10.100.206.52    <none>                                                                      80/TCP         1h\ncatalogue-db   ClusterIP      10.100.119.66    <none>                                                                      3306/TCP       1h\nfront-end      LoadBalancer   10.100.249.164   a4c42cfe951be11e9bb8c0a8cd8a2e5d-8156023.ap-northeast-2.elb.amazonaws.com   80:30001/TCP   1h\norders         ClusterIP      10.100.160.17    <none>                                                                      80/TCP         1h\norders-db      ClusterIP      10.100.70.203    <none>                                                                      27017/TCP      1h\npayment        ClusterIP      10.100.57.233    <none>                                                                      80/TCP         1h\nqueue-master   ClusterIP      10.100.146.109   <none>                                                                      80/TCP         1h\nrabbitmq       ClusterIP      10.100.32.115    <none>                                                                      5672/TCP       1h\nshipping       ClusterIP      10.100.180.174   <none>                                                                      80/TCP         1h\nuser           ClusterIP      10.100.211.41    <none>                                                                      80/TCP         1h\nuser-db        ClusterIP      10.100.87.142    <none>                                                                      27017/TCP      1h\n```\n  \n\n브라우저에서 ALB주소 `a4c42cfe951be11e9bb8c0a8cd8a2e5d-8156023.ap-northeast-2.elb.amazonaws.com` 로 접속하여 sock-shop Demo Web을 확인할수 있다.  \n\n![sockshop](/img/sock-shop.png)\n\n\n## EKS 클러스터 및 삭제\n\n위에서 외부접속을 위해 LoadBalancer를 수동으로 설정하였으므로 EC2 - Load Balancer서 프로비저닝된 ALB를 삭제하고 진행해야 한다.  \n\n클러스터에서 실행 중인 모든 서비스를 다시 확인하고 EXTERNAL-IP값과 연결된 모든 서비스를 삭제한다.  \n```bash\n$ kubectl get svc --all-namespaces\n$ kubectl delete svc front-end\n```\n\nALB와 VPC 삭제가 완료된것을 확인하고 클러스터를 삭제하자.  \n\n```bash\n$ eksctl delete cluster --name=eksworkshop-eksctl\n[ℹ]  deleting EKS cluster \"eksworkshop-eksctl\"\n[ℹ]  will delete stack \"eksctl-eksworkshop-eksctl-nodegroup-ng-3af535b7\"\n[ℹ]  waiting for stack \"eksctl-eksworkshop-eksctl-nodegroup-ng-3af535b7\" to get deleted\n[ℹ]  will delete stack \"eksctl-eksworkshop-eksctl-cluster\"\n[✔]  kubeconfig has been updated\n[✔]  the following EKS cluster resource(s) for \"eksworkshop-eksctl\" will be deleted: cluster. If in doubt, check CloudFormation console\n```\n\n## 정리\n서울 리전(ap-northeast-2)에 EKS가 오픈되고 Production환경을 EKS로 넘어가는 기업들이 많아지고 있는건 아주 긍정적인 현상이다.  \n또한 엄청난 속도의 기술개발과 다양한 툴들로 Kubernetes Cluster를 구성하거나 Microservice형태의 App을 배포하는것은 점점 대중화 되어가고 있다.  \n실제 Production에서 구현을 하기 위해서는 보안 및 성능을 동시에 고려한 네트워크(CNI), Ingress 설정이나 전체 클러스터 퍼포먼스 측면에서의 파라미터 튜닝이 더욱 더 중요해지고 관련된 DevOps(SRE) 인력들의 중요도가 높아질것은 분명해 보인다.  \n\nEKS를 오픈소스 측면에서 고려했을때에는 예전에 페이스북이나 다른곳에서 종종 이야기 했던것처럼 AWS 고유의 Lock-in 전략을 엄청나게 고민하고 발표한듯한 생각이 한 느낌을 지울수는 없는건 사실이지만 훌륭한 제품인건 확실하고 단기간에 서비스 성장속도를 낼 수 있는 서비스라 생각한다.  \n\n마지막으로 Managed Kubernetes의 선택은 엔지니어의 몫으로 보고 각 벤더별 비교한 아래 링크를 참조해서 선정 기준을 잡으면 더욱 좋을것 같다.  \n[https://docs.google.com/spreadsheets/d/1U0x4-NQegEPGM7eVTKJemhkPy18LWuHW5vX8uZzqzYo/edit#gid=0](https://docs.google.com/spreadsheets/d/1U0x4-NQegEPGM7eVTKJemhkPy18LWuHW5vX8uZzqzYo/edit#gid=0)"
    },
    {
      "id": "kubernetes/git-sync/",
      "metadata": {
        "permalink": "/kubernetes/git-sync/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-03-21-git-sync.md",
        "source": "@site/blog/2019-03-21-git-sync.md",
        "title": "git-sync",
        "description": "git-sync에 대해서 알아본다.",
        "date": "2019-03-21T00:00:00.000Z",
        "formattedDate": "March 21, 2019",
        "tags": [
          {
            "label": "git-sync",
            "permalink": "/tags/git-sync"
          },
          {
            "label": "GitOps",
            "permalink": "/tags/git-ops"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "ssh git",
            "permalink": "/tags/ssh-git"
          },
          {
            "label": "sidecar",
            "permalink": "/tags/sidecar"
          },
          {
            "label": "sidecar pattern",
            "permalink": "/tags/sidecar-pattern"
          }
        ],
        "readingTime": 8.095,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "git-sync",
          "comments": true,
          "classes": "wide",
          "description": "git-sync에 대해서 알아본다.",
          "slug": "kubernetes/git-sync/",
          "date": "2019-03-21T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "git-sync",
            "GitOps",
            "Kubernetes",
            "ssh git",
            "sidecar",
            "sidecar pattern"
          ]
        },
        "prevItem": {
          "title": "eksworkshop",
          "permalink": "/kubernetes/eksworkshop/"
        },
        "nextItem": {
          "title": "VScode Server",
          "permalink": "/tools/vscode-server/"
        }
      },
      "content": "git repo를 kubernetes volume으로 구현할수 있는 sidecar pattern `git-sync` 프로젝트에 대해서 알아본다.\n\n## git-sync\n\n[Kubernetes Github](https://github.com/kubernetes)에 방문하면 다양한 프로젝트들을 볼 수 있다.  \n[Kubernetes Project](https://github.com/kubernetes/kubernetes)부터 minikube, kubeadm, kubectl, kubelet, dashboard등 필수적으로 필요하거나 많이 사용되는 프로젝트를 볼 수 있는데,\n기존에 storage volume 으로 활용되던 [gitrepo](https://kubernetes.io/docs/concepts/storage/volumes/#gitrepo)가 `deprecated` 되어서 방법을 찾다가 유사한 프로젝트를 공식 repo에서 우연히 발견하게 되었다.  \n\n[git-sync](https://github.com/kubernetes/git-sync)는 sidecar 방식으로 git repository를 clone하는 프로젝트이다.  \n\n최초 한번 clone도 가능하고 일정한 간격으로 끌어와서 응용프로그램에 사용할 수 있고 원하는 branch, Tag 또는 특정 git hash 기반으로 pulling이 가능하다.  \n\nupstream의 repository에서 대상이 변경되었을때 다시 pulling하고, webhook기능을 추가하여 비동기성으로 POST 요청이 있을때만 git-sync를 수행할수 있기 때문에 \nContinuous Deployment를 간단하게 구현하는데 활용될 수 있다. \n\n## GitHub SSH설정\n\ngit 내용을 pulling을 할때 https, ssh 방법을 사용하는데 GitHub ssh key를 kubernetes cluster의 secret으로 사용을 할수 있기 때문에 ssh 방식을 사용하는 방법을 작성하였다.  \n\n아래 모든 과정은 MacOS에서 진행하였고 다른 OS는 아래 링크에서 확인 가능하다.  \n[https://help.github.com/en/articles/connecting-to-github-with-ssh](https://help.github.com/en/articles/connecting-to-github-with-ssh)\n\n사용하고자 하는 터미널에서 등록키를 확인하자.\n\n### 기존에 등록된 SSH key 확인\n```\n$ ls -al ~/.ssh\n```\n\n### 새로운 SSH key 생성\n```\n$ ssh-keygen -t rsa -b 4096 -C \"ddiiwoong@gmail.com\"\n# Start the SSH key creation process\n> Enter file in which the key is (/Users/you/.ssh/id_rsa): [Hit enter]\n> Key has comment '/Users/you/.ssh/id_rsa'\n> Enter new passphrase (empty for no passphrase): [Type new passphrase]\n> Enter same passphrase again: [One more time for luck]\n> Your identification has been saved with the new passphrase.\n```\n\n### ssh agent 실행중인지 확인\n```\neval “$(ssh-agent -s)”\n> Agent pid 59566\n```\n\n### 생성된 키를 keychain에 저장 및 확인\n```\n$ ssh-add -K ~/.ssh/id_rsa\n$ cat ~/.ssh/id_rsa.pub\n```\n\n### 복사 및 github에 추가\n```\n$ pbcopy < ~/.ssh/id_rsa.pub\n```\nSetting - SSH and GPG keys - SSH keys - New SSH key  \n`Title`은 구분자로 입력하고 GitHub password를 한번더 입력하고 완료한다.  \n\n### 터미널에서 SSH접속 확인\n```\n$ ssh -T git@github.com\nHi ddiiwoong! You've successfully authenticated, but GitHub does not provide shell access.\n```\n\n## git-sync를 위한 secret 등록\n\n[https://github.com/kubernetes/git-sync/blob/master/docs/ssh.md](https://github.com/kubernetes/git-sync/blob/master/docs/ssh.md)\n\n### secret 생성\n위에서 생성한 SSH key를 Kubernetes Cluster에 Secret resource로 저장을 한다.\n\n```\n$ ssh-keyscan github.com > /tmp/known_hosts\n# github.com:22 SSH-2.0-babeld-9d924d26\n# github.com:22 SSH-2.0-babeld-9d924d26\n# github.com:22 SSH-2.0-babeld-9d924d26\n```\n\nknown_hosts와 key를 Secret으로 저장한다.\n```\n$ kubectl create secret generic git-creds \\\n    --from-file=ssh=$HOME/.ssh/id_rsa \\\n    --from-file=known_hosts=/tmp/known_hosts\n\n$ kubectl get secret git-creds\nNAME        TYPE      DATA      AGE\ngit-creds   Opaque    2         1d\n```\n\n## sample ngnix 배포\n\n기본적으로 `git-sync/cmd/git-sync/main.go` 소스를 확인하면 여러가지 flag를 확인할 수 있는데 주로 사용하는 옵션은 다음과 같다.\n* ssh : pulling 방식 (default=false)\n* root : git clone이 수행되는 root directory (default=\"$HOME/git\")\n* repo : clone 대상 Repository (default=\"\")\n* branch : branch (default=master)\n* rev : git revision (tag or hash) to check out\n* depth : commit depth (default=0)\n* dest : repository 배포 directory\n\n기타 옵션들은 `git-sync/cmd/git-sync/main.go` 에서 확인이 가능하며 해당 옵션들을 변수로 처리하여 활용하면 된다.\n\n### git-sync-demo.yaml 작성\n\n```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: git-sync-demo\n  name: git-sync-demo\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: git-sync-demo\n  template:\n    metadata:\n      labels:\n        app: git-sync-demo\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: git-sync-volume\n          mountPath: /usr/share/nginx\n      - name: git-sync\n        image: k8s.gcr.io/git-sync:v3.1.1\n        imagePullPolicy: Always\n        args:\n         - \"-ssh\"\n         - \"-repo=git@github.com:ddiiwoong/git-sync-demo.git\"\n         - \"-root=/usr/share/nginx\"\n         - \"-dest=html\"\n         - \"-branch=master\"\n         - \"-depth=1\"\n        volumeMounts:\n        - name: git-sync-volume\n          mountPath: /usr/share/nginx\n        - name: git-secret\n          mountPath: /etc/git-secret\n      volumes:\n      - name: git-sync-volume\n        emptyDir: {}\n      - name: git-secret\n        secret:\n          secretName: git-creds\n          defaultMode: 288 # = mode 0440\n      securityContext:\n        fsGroup: 65533 # to make SSH key readable\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: git-sync-demo\nspec:\n  type: NodePort\n  selector:\n    app: git-sync-demo\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n```\n### 확인사항\n일단 ngnix로 1.14-alpine Image를 기본으로 하고 git-sync container가 sidecar로 들어가도록 작성하였다.  \n실제 동작하는 순서대로 manifest를 아래서부터 살펴보자.\n\n* git-sync-volume은 `emptyDir`, git-secret은 `secret` volume 설정\n  ```\n  ...\n      volumes:\n      - name: git-sync-volume\n        emptyDir: {}\n      - name: git-secret\n        secret:\n          secretName: git-creds\n          defaultMode: 288 # = mode 0440\n  ...\n  ```\n* sidecar container image를 `k8s.gcr.io/git-sync:v3.1.1`로 설정\n* `git-sync-volume`, `git-sync-volume` volume을 `git-sync` sidecar에 마운트\n* 위에서 이야기한 `git-sync` flag, args로 설정\n  ```\n  ...\n      containers:\n      - name: git-sync\n        image: k8s.gcr.io/git-sync:v3.1.1\n        imagePullPolicy: Always\n        args:\n         - \"-ssh\"\n         - \"-repo=git@github.com:ddiiwoong/git-sync-demo.git\"\n         - \"-root=/usr/share/nginx\"\n         - \"-dest=html\"\n         - \"-branch=master\"\n         - \"-depth=1\"\n        volumeMounts:\n        - name: git-sync-volume\n          mountPath: /usr/share/nginx\n        - name: git-secret\n          mountPath: /etc/git-secret\n  ...\n  ```\n* nginx 기본 위치가 `/usr/share/nginx` 이므로 mount 위치를 git-sync-volume으로 설정\n  ```\n  ...\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: git-sync-volume\n          mountPath: /usr/share/nginx\n  ...\n  ```\n\n위 nginx를 배포하고 접속하면 `Hello git-sync demo v1.0`를 확인할 수 있다.\n\n결국 git repo code (static html page)는 `/usr/share/nginx/html` 위치에 clone 되는것처럼 보이지만 \n실제 clone 위치를 확인해보면 symbolic link로 특정 revision dir를 가리키고 있다.\n\n```\n$ kubectl exec -it git-sync-demo-665c9c9ddf-6nwc4 sh\nDefaulting container name to nginx.\nUse 'kubectl describe pod/git-sync-demo-665c9c9ddf-6nwc4 -n default' to see all of the containers in this pod.\n/ # cd /usr/share/nginx/\n/usr/share/nginx # ls -al\ntotal 16\ndrwxrwsrwx    4 root     nogroup       4096 Mar 21 06:54 .\ndrwxr-xr-x    1 root     root          4096 Mar  8 03:09 ..\ndrwxr-sr-x    9 65533    nogroup       4096 Mar 21 06:54 .git\nlrwxrwxrwx    1 65533    nogroup         44 Mar 21 06:54 html -> rev-07aa36f719091d75b5665203fa5846a549e7d540\ndrwxr-sr-x    2 65533    nogroup       4096 Mar 21 06:54 rev-07aa36f719091d75b5665203fa5846a549e7d540\n/usr/share/nginx # cat ./html/index.html\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Hello git-sync demo</title>\n  </head>\n  <body>\n    <h1>Hello git-sync demo v1.0</h1>\n  </body>\n</html>\n```\n\ngithub에 있는 위 index.html 내용을 수정하고 commit을 하게 되면 실시간으로 아래와 같이 revision 정보 및 내용이 바뀐것을 확인할수 있다.  \n\n```\n/usr/share/nginx # ls -al\ntotal 16\ndrwxrwsrwx    4 root     nogroup       4096 Mar 21 13:40 .\ndrwxr-xr-x    1 root     root          4096 Mar  8 03:09 ..\ndrwxr-sr-x    9 65533    nogroup       4096 Mar 21 13:40 .git\nlrwxrwxrwx    1 65533    nogroup         44 Mar 21 13:40 html -> rev-b125908649135856d79c515c17decba68797a6cb\ndrwxr-sr-x    2 65533    nogroup       4096 Mar 21 13:40 rev-b125908649135856d79c515c17decba68797a6cb\n/usr/share/nginx # cat ./html/index.html\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>Hello git-sync demo</title>\n  </head>\n  <body>\n    <h1>Hello git-sync demo v1.1</h1>\n  </body>\n</html>\n```\n\n## 정리\ngit-sync를 간단하게 테스트해봤다.  \n간단하게 `sidecar` 방식의 clone tool로서 `git-sync`를 활용하면 여러가지로 충분히 활용이 가능할 것이다.  \n포스팅을 작성하면서 떠오른 활용용도를 정리하면 아래와 같다. 어찌보면 sidecar pattern의 활용방안이라고도 볼수 있다.\n\n* CDN을 사용하지 않고 git에서 소규모로 컨텐츠를 가져올때\n* DBMS가 필요없을 정도의 적은 데이터를 가져올때\n* jekyll이나 hugo 같은 정적 사이트(블로그)의 sidecar 패턴 (GitPage처럼 markdown을 추가하고 git commit하면 바로 사이트에 반영되는 방식)\n* nginx, haproxy, apache와 같이 config 변경이 필요할때 webhook방식으로 설정을 변경한후 pod를 재기동하는 GitOps 구현"
    },
    {
      "id": "tools/vscode-server/",
      "metadata": {
        "permalink": "/tools/vscode-server/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-03-20-vscode-server.md",
        "source": "@site/blog/2019-03-20-vscode-server.md",
        "title": "VScode Server",
        "description": "code-server를 사용하여 로컬 VScode 설치없이 원격 환경에서 사용해보자.",
        "date": "2019-03-20T00:00:00.000Z",
        "formattedDate": "March 20, 2019",
        "tags": [
          {
            "label": "vscode",
            "permalink": "/tags/vscode"
          },
          {
            "label": "code-server",
            "permalink": "/tags/code-server"
          },
          {
            "label": "kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "online coding",
            "permalink": "/tags/online-coding"
          },
          {
            "label": "GoormIDE",
            "permalink": "/tags/goorm-ide"
          },
          {
            "label": "Cloud9",
            "permalink": "/tags/cloud-9"
          }
        ],
        "readingTime": 5.13,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "VScode Server",
          "comments": true,
          "classes": "wide",
          "description": "code-server를 사용하여 로컬 VScode 설치없이 원격 환경에서 사용해보자.",
          "slug": "tools/vscode-server/",
          "date": "2019-03-20T00:00:00.000Z",
          "categories": [
            "Tools"
          ],
          "tags": [
            "vscode",
            "code-server",
            "kubernetes",
            "online coding",
            "GoormIDE",
            "Cloud9"
          ]
        },
        "prevItem": {
          "title": "git-sync",
          "permalink": "/kubernetes/git-sync/"
        },
        "nextItem": {
          "title": "K3s on Raspberry Pi Cluster",
          "permalink": "/kubernetes/k3s-homelab/"
        }
      },
      "content": "IntelliJ IDEA를 사용하다가 개발업무를 거의 손놓다시피 하다보니 라이센스를 연장하지 못하게 되었고 주로 Local에서 VScode를 사용하고 있다. 현재 부서와 업무도 바뀌었고 워낙 고가다보니 IDE를 부서비로 구매하도 어렵다. 뭐 얼마나 한다고 말할수도 있겠지만 업무특성상 개발툴을 사달라고 할수 없고 최근 크롬북이나 아이패드 프로같은 태블릿을 가지고 다니시는 분들도 많고 단순 필기나 메모가 아닌 온라인 환경에서 블로깅 포스팅 정도는 할수 있다는 가정으로 Web IDE를 구성하는 포스팅을 시작해본다. \n\n## Web IDE\n이번 포스팅을 쓰기 시작하면서 2017년 AWS re;invent에선가 Cloud9 제품이 출시되어서 Lambda에서 사용했던 장면이 갑자기 떠올랐다. Cloud 9은 [https://github.com/c9/core](https://github.com/c9/core)를 기반으로 instance를 띄는것을 기본으로 한다. Lambda의 기본 에디터도 나쁘진 않지만 Cloud9에서 강조하는 부분은 코드 협업이다. \n\n![cloud9](https://d1.awsstatic.com/product-marketing/Tulip/C9-Collab-Image@3x.e03a65d9488633c154358430540ab363dd1e8f45.png)\n\nAWS에서 서비스로 출시되기 이전에도 Dockerhub에서도 종종 확인할수 있었지만 현재는 찾아보기 어렵다. \n\n국내에는 Cloud 형태로 [GoormIDE](https://ide.goorm.io/) 제품이 있다.  \nFree 에디션도 있으니 따로 확인해보면 된다. (응원합니다!)\n\n이외에도 [https://github.com/theia-ide/theia](https://github.com/theia-ide/theia), [https://github.com/codercom/code-server](https://github.com/codercom/code-server)와 같은 Web기반 오픈소스가 존재하고 둘다 상용이나 Beta형태로 서비스 중이다. \n\n## code-server\n\n[code-server](https://github.com/codercom/code-server)는 원격서버 형태로 동작하는 브라우저 기반 [VSCode](https://github.com/Microsoft/vscode) IDE이다.\n\n그런데 왜 구지 VScode 설치형을 냅두고 Server로 구동하느냐? 아래와 같이 설명하고 있다.\n* Chromebook, Table(IPAD) 에서 Coding 가능\n* 저사양 Client에서도 Cloud 기반 Server의 이점 사용가능\n* Battery! (제일 중요포인트) \n\n구동방식은 여러방식이 존재한다. \n* Hosted : [coder](https://coder.com/) - Enterprise \n* Docker\n    ```\n    $ docker run -t -p 127.0.0.1:8443:8443 -v \"${PWD}:/root/project\" codercom/code-server code-server --allow-http --no-auth\n    ```\n* Bianry\n    ```\n    $ code-server <initial directory to open>\n    ```\n\n## code-server kubernetes에 배포\n\n공식 [Image](https://hub.docker.com/r/codercom/code-server)와 [Dockerfile](https://github.com/codercom/code-server/blob/master/Dockerfile)는 해당 링크를 참조한다.\n살짝 변경해서 재빌드하려고 했는데 맥북 메모리가 부족한지 `yarn build`할때 `Serve fails with Error Code 137` 가 발생하였다. 이문제는 나중에 해결하기로 하고 일단 배포하자.  \n\n로컬 minikube에서 테스트하기 위해  \n[https://github.com/codercom/code-server/blob/master/deployment/deployment.yaml](https://github.com/codercom/code-server/blob/master/deployment/deployment.yaml) 에서 ClusterIP를  NodePort로 수정하고 패스워드를 로그에서 확인하지 않고 사용할수 있도록 `1234`로 설정하고 배포한다. \n\n### deployment.yaml\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: code-server\n---\napiVersion: v1\nkind: Service\nmetadata:\n name: code-server\n namespace: code-server\nspec:\n ports:\n - port: 8443\n   name: https\n   protocol: TCP\n selector:\n   app: code-server\n type: NodePort\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: code-server\n  name: code-server\n  namespace: code-server\nspec:\n  selector:\n    matchLabels:\n      app: code-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: code-server\n    spec:\n      containers:\n      - image: codercom/code-server\n        imagePullPolicy: Always\n        name: code-server\n        ports:\n        - containerPort: 8443\n          name: https\n        args:\n         - \"--password=1234\"\n```\n```\n$ kubectl create -f deployment.yaml\n```\n\n이후 서비스 확인 및 minikube service로 expose 시킨다.\n\n```\n$ kubectl get svc\nNAME          TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\ncode-server   NodePort   10.111.206.64   <none>        8443:32665/TCP   15m\n\n$ minikube service code-server\n\n$ minikube service list\n|-------------|---------------|-----------------------------|\n|  NAMESPACE  |     NAME      |             URL             |\n|-------------|---------------|-----------------------------|\n| code-server | code-server   | http://192.168.99.102:32665 |\n| default     | git-sync-demo | http://192.168.99.102:31595 |\n| default     | kubernetes    | No node port                |\n| kube-system | kube-dns      | No node port                |\n|-------------|---------------|-----------------------------|\n```\n\n해당 URL로 접속하여 패스워드 `1234`를 입력하면 vscode가 원격으로 실행된 것을 확인할수 있다.\n![vscode](/img/vscode.png)\n\n## 정리\n실제 사용을 해보면 아직 버그가 많다. 애드온이나 플러그인 설치시 제대로 동작을 하지 않는 경우도 있었고 \n[git-sync](https://github.com/kubernetes/git-sync) 프로젝트와 연동을 통해 실제 gitOPS를 구현해보고자 하였으나 배포후에 mount된 repository volume에 변경사항이 발생시 갑자기 UI가 먹통이 되는 경우가 발생하기도 하였다. Kubernetes플랫폼을 개발자에게 PaaS형태로 제공하는 경우 webIDE의 좋은 옵션이 될수 있을것 같다."
    },
    {
      "id": "kubernetes/k3s-homelab/",
      "metadata": {
        "permalink": "/kubernetes/k3s-homelab/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-03-11-k3s-homelab.md",
        "source": "@site/blog/2019-03-11-k3s-homelab.md",
        "title": "K3s on Raspberry Pi Cluster",
        "description": "이번 포스트에서는 k3s를 라즈베리파이 클러스터에 올려보고 활용방안에 대해서 고민해본다",
        "date": "2019-03-11T00:00:00.000Z",
        "formattedDate": "March 11, 2019",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Rancher",
            "permalink": "/tags/rancher"
          },
          {
            "label": "K3s",
            "permalink": "/tags/k-3-s"
          },
          {
            "label": "Raspberry",
            "permalink": "/tags/raspberry"
          },
          {
            "label": "Homelab",
            "permalink": "/tags/homelab"
          }
        ],
        "readingTime": 12.09,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "K3s on Raspberry Pi Cluster",
          "comments": true,
          "classes": "wide",
          "description": "이번 포스트에서는 k3s를 라즈베리파이 클러스터에 올려보고 활용방안에 대해서 고민해본다",
          "slug": "kubernetes/k3s-homelab/",
          "date": "2019-03-11T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "Rancher",
            "K3s",
            "Raspberry",
            "Homelab"
          ]
        },
        "prevItem": {
          "title": "VScode Server",
          "permalink": "/tools/vscode-server/"
        },
        "nextItem": {
          "title": "Cloud-Native Microservices Demo Application with OpenCensus",
          "permalink": "/kubernetes/microservices-demo/"
        }
      },
      "content": "이번에는 경량 Kubernetes라고 이야기하는 [K3s](https://github.com/rancher/k3s)를 Raspberry Pi 클러스터상에 구동하려고 한다. \n순수하게 개인의견으로 작성하였고 절대 제품이나 부품홍보를 하고자 하는 의도는 전혀 없다.\n\n## K3s?\n\n**K3s**는 Rancher Lab에서 최소자원을 사용하는 Kubernetes 클러스터 구성을 위한 솔루션으로 시작되었고 2019년 3월 12일 현재 0.2버전이 릴리즈된 상태이다. 바이너리 전체가 40mb가 되지 않고 설치가 쉽다는 점에서 최근 트위터 상에서 이슈가 되고 있는 프로젝트라고 할 수 있다. \n\n주로 Edge, IoT 등 저전력, 저사양 기반 ARM계열 컴퓨팅에 최적화 되어 있고 실제 실험적이긴 하지만 간단한 기능이나 baremetal 기반 클러스터 테스트를 집에서 해보기에는 딱 좋은 프로젝트라 할 수 있다. 이미 vSphere, OpenStack기반으로 테스트는 차고 넘치게 해봤지만 일단 물리적인 케이스부터 보고나면 하드웨어를 좋아하는 사람들에게는 아주 재미있는 장난감이 아닐수 없을 것이다. \n\n[K3s Github](https://github.com/rancher/k3s) 상세 설명에 보면 Cloud Provider, Storage Plugin은 제거하였고  default 저장소가 `etcd`가 아닌 `sqlite3`으로 되어있다고 한다. \n\n## 사전 준비사항\n\n* 최소 2개 이상의 Raspberry Pi 2B/3B/3B+, 4ea  \n오픈마켓에서 할인쿠폰 적용해서 Raspberry Pi 3B+, 4ea를 `166,820원`(대당 42,000원) 정도에 구매하였다.  \n최저가 검색으로 대당 46,000원 정도 했던것 같다. \n\n* Stackable Case  \n[iUniker Raspberry Pi Cluster Case](https://www.amazon.com/gp/product/B07CTG5N3V/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1) 강추!! (개인적으로 쿨러와 디자인이 맘에 듬)  \n배송비포함 29.19달러, 약 `33,000원`\n\n* micro SDHC 4ea  \n16GB로도 충분하지만 삼성전자 micro SDHC CLASS10 UHS-I EVO (32GB), 4ea를 오픈마켓에서 사게되면 배송료 포함해서 8,500원 * 4ea = `34,000원`에 구매가 가능하다. 오픈마켓에서는 개당 배송료를 내야한다. 하지만 쿠팡에서는 2ea를 로켓배송으로 15,380원에 구매가 가능하므로 약 `31,000원`에 4개를 구매할수 있다. \n\n* 멀티충전기  \n1만원대 후반에서 2만원 초반이면 6포트 충전기를 구매할수 있는데 구매했던 가장 큰 기준은 Pi 4대를 동시에 2.5A 전류를 안정적으로 공급하려면 최대 10A를 지원하는 멀티 충전기를 사야했었고 4-5포트 짜리 충전기들은 대부분 최대 전류가 8A로 충족하지 못해 4포트만 사용하더라도 안정적인 전류 공급을 위해 6포트 충전기로 선택하였다.  \n쿠팡 로켓배송 - 포*지 가정용 6포트 급속 멀티 충전기, `22,900원`\n\n* micro 5pin 20cm 2.4A 지원 케이블 4ea  \nPi 권장 전류가 2.5A라고 했지만 2.4A, 3A 짜리중에 저렴한 2.4A 지원 숏케이블로 구매하였다.  \n오픈마켓에서 배송료 포함, `7800원`\n\n* UTP Cat5e 30cm 케이블 4ea  \n그냥 제일싼걸로 오픈마켓에서 배송료 포함 `4,100원`에 구매하였다.\n\n* 기가비트 지원되는 5포트 이상 스위치 허브(공유기)  \n집에 있던 공유기 활용 (iptime A1004) 하였지만 최저 5포트 이상 스위치 허브중 제일 싼 모델은 `16,000원`대로 가격 형성중이다. \n\n실제 위 스펙으로 대충 구매를 진행하게 되면 18.4 + 3.3 + 3.1 + 2.3 + 0.8 + 0.4 + 1.6 = `29.9만원` 정도 소요가 될 것으로 예상된다. 집에 굴러다는 부품이나 충전기, 케이블, SD카드들을 활용하면 `25만원` 이내로도 충분히 가능하다.\n\n## 구매 제품 조립\n조립은 그다지 어렵지 않은데 케이스 구매시 제공되는 방열판(CPU, GPU, RAM)을 꼼꼼하게 붙이고 쿨러를 GPIO에 연결한다.  \n그렇게 어려운 점은 없지만 본체를 케이스에 고정할때 너트가 작아 손이 큰 사람은 조금 힘들 수도 있을 것 같다. \n\n케이스의 CPU 쿨러가 화룡점정이다.\n\n![cases](/img/cases.jpg)  \n![heat](/img/heat.jpg)  \n![cooler](/img/cooler.jpg)\n![3stack](/img/3stack.jpg)\n![charger](/img/charger.jpg)\n![full](/img/fullstack.jpg)\n![complete](/img/complete.jpg)\n\n## OS 설치\n\nSD카드에 설치는 MacOS상에서 진행하였다.\n\n[Raspbian Lite](https://www.raspberrypi.org/downloads/raspbian/)를 내려받아 [Etcher](https://www.balena.io/etcher/)를 사용하여 OS를 설치한다. \n\n자세한 추가적인 설치방법은 다음 링크를 통해서도 확인이 가능하다.  \n[Installing operating system images](https://www.raspberrypi.org/documentation/installation/installing-images/README.md)  \n\n## 환경 설정\nMac에서는 `/Volumes/boot`에 마운트가 된다. OS마다 다르지만 Linux에서는 `/mnt/boot`, Windows에서는 `boot` 로 마운트가 된다.\n\nSSH Service 자동 활성화를 위해 위 OS별 mount된 root 경로에 ssh 빈 파일을 생성하게 되면 reboot이후에 SSH 접속이 가능해진다.\n```\n$ sudo touch /Volumes/boot/ssh\n```\n\n또한 container 사용을 위해 root경로의 `cmdline.txt` 파일 마지막에 cgroup 설정을 추가한다.  \n```\ncgroup_memory=1 cgroup_enable=memory\n```\n\nSD카드를 만들고 각각의 Pi에 장착후에 UTP케이블과 전원을 모두 연결한다.\n부팅이 완료되면 default id/pass 인 `pi/raspberry`로 로그인 하고 `sudo raspi-config` 를 통해 패스워드 변경, hostname 설정, GPU memory split 설정 등을 완료하자.  \n나중에는 PXE booting 및 ansible 자동화로 구현하면 무인환경 설치가 가능할것 같다. (Edge Computing)\n\n```\n┌───────────────────┤ Raspberry Pi Software Configuration Tool (raspi-config) ├────────────────────┐\n│                                                                                                  │\n│        1 Change User Password Change password for the current user                               │\n│        2 Network Options      Configure network settings                                         │\n│        3 Boot Options         Configure options for start-up                                     │\n│        4 Localisation Options Set up language and regional settings to match your location       │\n│        5 Interfacing Options  Configure connections to peripherals                               │\n│        6 Overclock            Configure overclocking for your Pi                                 │\n│        7 Advanced Options     Configure advanced settings                                        │\n│        8 Update               Update this tool to the latest version                             │\n│        9 About raspi-config   Information about this configuration tool                          │\n│                                                                                                  │\n│                                                                                                  │\n│                                                                                                  │\n│                           <Select>                           <Finish>                            │\n│                                                                                                  │\n└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n```\n* 1 Change User Password\n* 2 Network Options - hostname\n    * k3s-master, k3s-slave-01, k3s-slave-02, k3s-slave-03\n* 4 Localisation Options - TimeZone\n    * Asia, Seoul\n* 7 Advanced Options - GPU Memory split \n    * 16mb\n\n모두 완료가 되었으면 pi들을 재기동하자.\n```\n$ sudo reboot\n```\n\n## k3s 클러스터 생성\n\n### Server 기동\n\narmhf(arm hard float) 지원이 되는 최신 릴리즈 v0.2.0를 다운받는다.  \n```\n$ wget -O k3s https://github.com/rancher/k3s/releases/download/v0.2.0/k3s-armhf && \\\n  chmod +x k3s && \\\n  sudo mv k3s /usr/local/bin/k3s\n```\n\nMaster node 기동 (백그라운드)\n```\n$ sudo k3s server &\n```\n\n해당 node를 Control plane 형태로 분리시켜 workload에서 제외하려면 `--disable-agent` 옵션을 사용한다.\n```\n& k3s server --disable-agent\n```\n\n정상적으로 k3s 기동이 완료되면 `/etc/rancher/k3s/k3s.yaml`에서 Kubeconfig를 확인할수 있다. 그리고 node 상태도 확인이 가능하다.\n```\n$ sudo k3s kubectl get nodes\nNAME          STATUS   ROLES    AGE   VERSION\nk3s-master    Ready    <none>   21h   v1.13.4-k3s.1\n```\n\n### node 추가\n\n`/var/lib/rancher/k3s/server/manifests` 에서 TOKEN을 확인한다. \n\n```\n$ sudo cat /var/lib/rancher/k3s/server/node-token\nK100fa5235031f2b8e92e01b8bd3255142422a7aeaa47657ad4c68969d35cddbf3a::node:431342ac6204466e8f81445edb8c2e3a\n```\n\nworker node에 접속한다음 동일하게 최신 릴리즈 v0.2.0를 다운받는다.  \n```\n$ wget -O k3s https://github.com/rancher/k3s/releases/download/v0.2.0/k3s-armhf && \\\n  chmod +x k3s && \\\n  sudo mv k3s /usr/local/bin/k3s\n```\n\n위에서 나온 TOKEN값과 Kube API Endpoint정보로 node들을 차례로 추가 시키면 모든 작업이 완료된다. \n```\n$ export NODE_TOKEN=\"K100fa5235031f2b8e92e01b8bd3255142422a7aeaa47657ad4c68969d35cddbf3a::node:431342ac6204466e8f81445edb8c2e3a\"\n$ export MASTER_IP=\"https://192.168.0.14:6443\"\n$ sudo k3s agent --server https://${MASTER_IP}:6443 --token ${NODE_TOKEN} &\n```\n\n완성된 클러스터를 확인한다. 외부 로컬에서 확인하려면 `/etc/rancher/k3s/k3s.yaml` 파일을 `~/.kube/config`에 추가하면 된다. 클러스터 내부에서 kubectl 명령은 k3s 바이너리 내부에 포함되어 있으므로 `sudo k3s kubectl` 명령을 사용하였다.\n\n```\n$ sudo k3s kubectl get node -o wide\nNAME          STATUS   ROLES    AGE   VERSION         INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME\nk3s-master    Ready    <none>   22h   v1.13.4-k3s.1   192.168.1.14   <none>        Raspbian GNU/Linux 9 (stretch)   4.14.79-v7+      containerd://1.2.4+unknown\nk3s-slave01   Ready    <none>   21h   v1.13.4-k3s.1   192.168.1.15   <none>        Raspbian GNU/Linux 9 (stretch)   4.14.79-v7+      containerd://1.2.4+unknown\nk3s-slave02   Ready    <none>   10h   v1.13.4-k3s.1   192.168.1.16   <none>        Raspbian GNU/Linux 9 (stretch)   4.14.79-v7+      containerd://1.2.4+unknown\nk3s-slave03   Ready    <none>   10h   v1.13.4-k3s.1   192.168.1.17   <none>        Raspbian GNU/Linux 9 (stretch)   4.14.79-v7+      containerd://1.2.4+unknown\n```\n\nnode 상세정보를 보면 기본적으로 K8s `v1.13.4`, runtime은 `containerd`를 사용하고 있음을 알 수 있다.  \n\n```\n$ sudo k3s kubectl get svc --all-namespaces\nNAMESPACE     NAME         TYPE           CLUSTER-IP     EXTERNAL-IP                 PORT(S)                      AGE\ndefault       kubernetes   ClusterIP      10.43.0.1      <none>                      443/TCP                      21h\nkube-system   kube-dns     ClusterIP      10.43.0.10     <none>                      53/UDP,53/TCP,9153/TCP       21h\nkube-system   traefik      LoadBalancer   10.43.19.160   192.168.1.14,192.168.1.15   80:32304/TCP,443:31690/TCP   21h\n```\n\nRancher쪽에서도 오늘날짜(3/12)로 F5가 Nginx를 인수하는것을 예견했던 것일까?  \nService를 확인하면 traefik이 기본으로 되어있다. 아래처럼 기본적으로 loadbalancer로 활용되고 있는 traefik을 Helm Chart CRD를 통해 배포된것을 확인할 수 있다. 또한 얼마전 졸업한 CoreDNS도 보인다.   \n\n```\n$ sudo k3s kubectl get pod --all-namespaces\nNAMESPACE     NAME                             READY   STATUS      RESTARTS   AGE\nkube-system   coredns-7748f7f6df-qflx9         1/1     Running     0          21h\nkube-system   helm-install-traefik-dqqg9       0/1     Completed   0          21h\nkube-system   svclb-traefik-598fd65c97-4xtkf   2/2     Running     0          21h\nkube-system   svclb-traefik-598fd65c97-vbsqv   2/2     Running     0          19h\nkube-system   traefik-6876857645-2sqg9         1/1     Running     0          21h\n\n$ sudo k3s kubectl get crd\nNAME                            CREATED AT\naddons.k3s.cattle.io            2019-03-11T16:46:22Z\nhelmcharts.k3s.cattle.io        2019-03-11T16:46:22Z\nlistenerconfigs.k3s.cattle.io   2019-03-11T16:46:22Z\n```\n\n## 정리\n아주 적은비용(?)으로 취미삼아 k3s 클러스터를 구성해봤다.  \n아직 ARM계열에서 kubernetes workload를 구동하는 것은 시기상조이긴 하지만 기존에 kubeadm을 가지고 pi에 배포하는것에 비하면 설치 난이도나 자원사용량 측면에서 장점이 많은 프로젝트이다.  \n해외 블로그나 트위터를 보면 최근 `k3s`에 대한 관심도가 높아지는것을 확인할 수 있는데 단순히 취미생활만이 아니라 IoT, Edge에서의 Serverless Workload 수행이라던지 ARM 계열 최적화된 모습만으로도 충분히 가능성은 보여준것 같다.  \nRancher 2.0 이후로 Kubernetes 연관된 관심도가 떨어졌었는데 엔지니어들의 관심을 끄는데는 성공한듯 하고 AWS의 Greengrass, Firecracker와 동일선상에서 봐도 견줄만한 가치가 있다고 생각된다."
    },
    {
      "id": "kubernetes/microservices-demo/",
      "metadata": {
        "permalink": "/kubernetes/microservices-demo/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-03-07-microservices-demo.md",
        "source": "@site/blog/2019-03-07-microservices-demo.md",
        "title": "Cloud-Native Microservices Demo Application with OpenCensus",
        "description": "Hipster Shop: Cloud-Native Microservices Demo Application 를 살펴보고 안에 들어있는 여러가지 오픈소스들을 알아본다",
        "date": "2019-03-07T00:00:00.000Z",
        "formattedDate": "March 7, 2019",
        "tags": [
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Istio",
            "permalink": "/tags/istio"
          },
          {
            "label": "Stackdriver",
            "permalink": "/tags/stackdriver"
          },
          {
            "label": "Prometheus",
            "permalink": "/tags/prometheus"
          },
          {
            "label": "gRPC",
            "permalink": "/tags/g-rpc"
          },
          {
            "label": "OpenCensus",
            "permalink": "/tags/open-census"
          },
          {
            "label": "skaffold",
            "permalink": "/tags/skaffold"
          }
        ],
        "readingTime": 13.585,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Cloud-Native Microservices Demo Application with OpenCensus",
          "comments": true,
          "classes": "wide",
          "description": "Hipster Shop: Cloud-Native Microservices Demo Application 를 살펴보고 안에 들어있는 여러가지 오픈소스들을 알아본다",
          "slug": "kubernetes/microservices-demo/",
          "date": "2019-03-07T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Kubernetes",
            "Istio",
            "Stackdriver",
            "Prometheus",
            "gRPC",
            "OpenCensus",
            "skaffold"
          ]
        },
        "prevItem": {
          "title": "K3s on Raspberry Pi Cluster",
          "permalink": "/kubernetes/k3s-homelab/"
        },
        "nextItem": {
          "title": "Knative with Gloo",
          "permalink": "/kubernetes/knative-using-gloo/"
        }
      },
      "content": "[https://github.com/GoogleCloudPlatform/microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo)에서 소개된 Demo Application인 `Hipster Shop`을 Kubernetes 기반으로 배포하고 관련 오픈소스들을 더 알아보고자 한다.\n\n위 링크에 가서 보면 알수 있지만 `Hipster Shop` 아래 그림처럼 10개의 Microservice로 구성되어 있고 상품을 검색 및 구매할 수있는 웹 기반 이커머스 Application으로 구성 되어있다.\n\n![arch](https://github.com/GoogleCloudPlatform/microservices-demo/raw/master/docs/img/architecture-diagram.png)\n\n각각의 서비스는 **gRPC** 로 통신하고 외부 사용자만 HTTP로 접근한다. 모든 서비스는 서로 다른 언어(Go, C#, Node.js, Python, Java)로 구성되어 있고 대부분의 Microservice들은 `Istio` service mesh 형태로 구성할수 있도록 되어있다. `Skaffold`를 통해 배포하고 `OpenCensus`라고 하는 gRPC/HTTP기반 Tracing tool을 활용하여 Google `Stackdriver`로 보내도록 되어있지만  Prometheus에 통합하는 방향으로 작성하기 위해서 Prometheus 기반으로 Metric을 수집하는 Fork된 데모 Application을 검색을 통해 찾을수 있었다.  \n[https://github.com/census-ecosystem/opencensus-microservices-demo](https://github.com/census-ecosystem/opencensus-microservices-demo)  \n\n## OpenCensus\n\n[OpenCensus(OC)](https://opencensus.io/)는 Microservice 및 Monolith 서비스를 Tracing 및 Metric Monitoring 할 수 있는 라이브러리를 제공하는 오픈소스이다.\n\n아래와 같이 다양한 언어를 지원 한다.\n* Java , Go, Python, C++, Nodejs, Erlang, Ruby, PHP, C#\n\n또한 수집된 데이터를 Prometheus, Stackdriver Tracing and Monitoring, DataDog, Graphite, Zipkin, Jaeger, Azure Insights 등 과 같은 백엔드 Application으로 내보낼 수 있기 때문에 개발자, 운영자 측면에서 좋은 선택사항이 될 수 있다.  \n\nMicroservice와 같은 분산 시스템에서 개발자/운영자 관점의 가장 중요한 미션은 각각의 수행되는 서비스들의 실행 시간을 확인하고 상호 API간 통신이 얼마나 걸리는지를 확인하고 Span(아래 그림참조)에서 가장 지연이 발생하는 서비스를 빨리 찾아내 확인하고 조치하는 것이라 할 수 있다.\n\n![span](https://www.jaegertracing.io/img/spans-traces.png)\n\nOpenCensus는 주로 두가지 기능으로 활용된다.  \n첫번째는 Metric 수집이고 두번째는 Tracing 기능이다.  \nLog같은 경우 현재 미지원이지만 다음 메이저 릴리즈에 추가될 예정이라고 하니 조금더 지켜보면 좋을것 같다.\n\n* Metrics\n  * 데이터베이스 및 API의 응답시간, 요청 content length, open file descriptor 수와 같이 추적하고자하는 정량 데이터를 말한다. Metric 을 시각화해서 보면 응용 프로그램 및 서비스의 성능과 품질을 측정하는 데 도움이 될 수 있다. 예를 들면 요청 응답시간의 평균값이나 cache hit/miss 수와 같은 것들이 될 수 있다. \n\n* Traces\n  * 서비스 요청에 대한 애플리케이션 또는 서비스 구조를 확인할수 있고 모든 서비스 간 데이터 흐름을 시각화하여 아키텍처상의 병목 현상을 파악하는 데 도움이 된다.\n\n## Hipster Shop Demo\n\n위에서 언급했던 내용처럼 GCP에서 작성한 [Hipster Shop Demo](https://github.com/GoogleCloudPlatform/microservices-demo)는 minikube 및 GCP 데모로 되어있고 코드안에 기본 Metric 설정이 Stackdriver으로 되어있어 Prometheus Exporter 적용을 하려면 코드 수정이 필요하기 때문에 Prometheus기반으로 작성된 [Forked Repo](https://github.com/census-ecosystem/opencensus-microservices-demo)를 살펴보기로 하였다.\n\n\n### Requirement\n현재 가지고 있는 MacOS 환경에서 구동하였다. 최소 스펙은 따로 기재하지 않았으나 K8s 1.11 이상을 권장한다.\n* kubectl : v1.10.7\n* Minikube : v0.34.1\n* Kubernetes : v1.13.3\n* [skaffold](https://github.com/GoogleContainerTools/skaffold/#installation) : v0.24\n\n### minikube 기동\n최소 3 CPU, 6Gb Memory가 필요하다. 그냥 minikube를 구동시기면 4 CPU, 8Gb 로 구동이 되기 때문에 별다른 옵션 없이 default로 구동하면 된다. \n```bash\n$ minikube start\n$ kubectl get nodes\nNAME       STATUS    ROLES     AGE       VERSION\nminikube   Ready     master    6h        v1.13.3\n```\n\n### Repository Clone\n```bash\n$ git clone https://github.com/census-ecosystem/opencensus-microservices-demo.git\n$ cd opencensus-microservices-demo\n```\n\n내부 구조를 살펴보면 기본적으로 [skaffold](https://github.com/GoogleContainerTools/skaffold)를 활용하여 배포를 진행을 하는 것을 알수있다.  \n`skaffold`는 로컬에서 Kubernetes 기반 어플리케이션 개발과 배포(CD)를 빠르게 도와주는 CLI tool이다. 소스코드의 변화를 감지하여 build, registry push/tagging, deploy까지 자동으로 할 수 있는 로컬 기반 도구이다.\n\n`skaffold dev`는 로컬 환경의 반복적인 개발에 활용하고 실제 배포는 CI Process에서 `skaffold run`을 통해 배포를 진행할 수 있다.  \n\n![skaffold demo](https://github.com/GoogleContainerTools/skaffold/raw/master/docs/static/img/intro.gif)\n\nKubernetes 배포툴에 대해 비교한 글은 [블로그 링크](https://blog.hasura.io/draft-vs-gitkube-vs-helm-vs-ksonnet-vs-metaparticle-vs-skaffold-f5aa9561f948/)를 통해 확인할 수 있다.\n\n아래에서는 `skaffold`에 대한 자세한 내용은 미뤄두고 배포하는 도구로서만 설명한다.  \n\n기본적으로 구성을 하고자 하는 내용은 helm처럼 template 파일을 사용하게 되는데 프로젝트 root에 `skaffold.yaml` 에 build를 위한 image name, tag, src 위치등 기본적인 내용을 기재한다. 파일내용을 살펴보면 build에 관련된 내용들을 작성하고 deploy할 manifests의 위치까지 지정하도록 되어있다. 로컬환경에서 확인을 위해 grafana, prometheus, jaeger가 추가된 것을 확인할 수 있다.\n\n```yaml\napiVersion: skaffold/v1alpha2\nkind: Config\nbuild:\n  tagPolicy:\n    gitCommit: {}\n  artifacts:\n  - imageName: gcr.io/opencensus-microservices-demo/emailservice\n    workspace: src/emailservice\n  - imageName: gcr.io/opencensus-microservices-demo/productcatalogservice\n    workspace: src/productcatalogservice\n  - imageName: gcr.io/opencensus-microservices-demo/recommendationservice\n    workspace: src/recommendationservice\n  - imageName: gcr.io/opencensus-microservices-demo/shippingservice\n    workspace: src/shippingservice\n  - imageName: gcr.io/opencensus-microservices-demo/checkoutservice\n    workspace: src/checkoutservice\n  - imageName: gcr.io/opencensus-microservices-demo/paymentservice\n    workspace: src/paymentservice\n  - imageName: gcr.io/opencensus-microservices-demo/currencyservice\n    workspace: src/currencyservice\n  - imageName: gcr.io/opencensus-microservices-demo/cartservice\n    workspace: src/cartservice\n  - imageName: gcr.io/opencensus-microservices-demo/frontend\n    workspace: src/frontend\n  - imageName: gcr.io/opencensus-microservices-demo/loadgenerator\n    workspace: src/loadgenerator\n  - imageName: gcr.io/opencensus-microservices-demo/adservice\n    workspace: src/adservice\n  - imageName: gcr.io/opencensus-microservices-demo/grafana\n    workspace: src/grafana\n  - imageName: gcr.io/opencensus-microservices-demo/prometheus\n    workspace: src/prometheus\n  - imageName: gcr.io/opencensus-microservices-demo/jaeger\n    workspace: src/jaeger\ndeploy:\n  kubectl:\n    manifests:\n    - ./kubernetes-manifests/**.yaml\n```\n\nGo로 작성된 Frontend microservice을 살펴보자. [**./src/frontend/main.go**]\n\n### library 추가 및 http handler 초기화\n\nGo기반 exporter 패키지(jaeger,prometheus)를 추가적으로 import 하고 http handler를 위한 [ochttp 패키지](https://godoc.org/go.opencensus.io/plugin/ochttp)를 추가하였다. \n```go\nimport (\n...\n        \"go.opencensus.io/exporter/jaeger\"\n        \"go.opencensus.io/exporter/prometheus\"\n        \"go.opencensus.io/plugin/ochttp\"\n        \"go.opencensus.io/plugin/ochttp/propagation/b3\"\n...\n)\nfunc main() {\n...\n        var handler http.Handler = r\n        handler = &logHandler{log: log, next: handler}\n        handler = ensureSessionID(handler)      \n        // add opencensus instrumentation\n        handler = &ochttp.Handler{ \n                Handler:     handler,\n                Propagation: &b3.HTTPFormat{}}\n        log.Infof(\"starting server on \" + addr + \":\" + srvPort)\n        log.Fatal(http.ListenAndServe(addr+\":\"+srvPort, handler))\n}\n```\n\n### exporter 등록(Jaeger Tracing 및 Prometheus exporter)\n\n예시처럼 각각의 서비스에 jaeger와 prometheus exporter Endpoint를 쉽게 등록할수 있다.  \n또한 initTracing() 에서는 데모를 위해 trace.AlwaysSample()을 사용하였다. 실제 운영환경에서는 [다음 링크](https://github.com/census-instrumentation/opencensus-specs/blob/master/trace/Sampling.md)를 참고해서 사용하는 것을 권고하고 있다. \n\n```go\n...\nfunc initJaegerTracing(log logrus.FieldLogger) {\n        // Register the Jaeger exporter to be able to retrieve\n        // the collected spans.\n        exporter, err := jaeger.NewExporter(jaeger.Options{\n                Endpoint: \"http://jaeger:14268\",\n                Process: jaeger.Process{\n                        ServiceName: \"frontend\",\n                },\n        })\n        if err != nil {\n                log.Fatal(err)\n        }\n        trace.RegisterExporter(exporter)\n}\n\nfunc initTracing(log logrus.FieldLogger) {\n        // This is a demo app with low QPS. trace.AlwaysSample() is used here\n        // to make sure traces are available for observation and analysis.\n        // In a production environment or high QPS setup please use\n        // trace.ProbabilitySampler set at the desired probability.\n        trace.ApplyConfig(trace.Config{\n                DefaultSampler: trace.AlwaysSample(),\n        })\n        initJaegerTracing(log)\n}\n\nfunc initPrometheusStatsExporter(log logrus.FieldLogger) *prometheus.Exporter {\n\texporter, err := prometheus.NewExporter(prometheus.Options{})\n\n\tif err != nil {\n\t\tlog.Fatal(\"error registering prometheus exporter\")\n\t\treturn nil\n\t}\n\n\tview.RegisterExporter(exporter)\n\treturn exporter\n}\nfunc startPrometheusExporter(log logrus.FieldLogger, exporter *prometheus.Exporter) {\n\taddr := \":9090\"\n\tlog.Infof(\"starting prometheus server at %s\", addr)\n\thttp.Handle(\"/metrics\", exporter)\n\tlog.Fatal(http.ListenAndServe(addr, nil))\n}\n...\n```\n\n### Demo Application 배포\n\nminikube에 Hipster Shop Demo를 배포한다. 단순하게 `skaffold run` 명령으로 진행하면 된다.\n\n```\n$ skaffold run\n```\n\n현재 사용중인 2018 Macbook Pro(3.1 GHz Intel Core i7, 16GB) 상의 Docker기반 minikube 환경으로도 배포를 하였는데 시간이 꽤 소요되었다.(20분이상)  \n코드를 실시간으로 수정하고 빌드, 배포되는 것은 `skaffold dev` 명령으로 확인할 수 있다. 진행되는 과정을 보면 [draft.sh](https://draft.sh/) 프로젝트와도 꽤 유사하다고 볼 수 있다.  \n\n에러없이 run이 실행되고 난후 minikube에 배포된 pod와 service를 확인한다. 중간에 loadgenerator가 init인 이유는 minikube 자원이 부족해서 발생하는 현상이다.\n\n```bash\n$ kubectl get pod\nNAME                                     READY     STATUS     RESTARTS   AGE\nadservice-7c7d687dcb-xzr4m               1/1       Running    1          4h\ncartservice-f54bcb9ff-tcfgn              1/1       Running    4          4h\ncheckoutservice-576446687b-95bwj         1/1       Running    1          4h\ncurrencyservice-5bd99bf97d-28mtz         1/1       Running    1          4h\nemailservice-6cb587798d-wwzdh            1/1       Running    1          4h\nfrontend-6bf9796f7b-ch9pl                1/1       Running    4          4h\ngrafana-6678c5c5d9-2qx75                 1/1       Running    1          4h\njaeger-5b66bdf7f7-csdzx                  1/1       Running    2          4h\nloadgenerator-7c4f446774-68djg           0/1       Init:0/1   1          4h\npaymentservice-fc4c8589-wrfg7            1/1       Running    1          4h\nproductcatalogservice-84878c8b9c-jhgnw   1/1       Running    1          4h\nprometheus-58d98b7578-87td6              1/1       Running    1          4h\nrecommendationservice-8564f9d894-smlpf   1/1       Running    1          4h\nredis-cart-798bc66d58-xn6ff              1/1       Running    1          4h\nshippingservice-789656f6bc-rgzrp         1/1       Running    1          4h\n\n$ kubectl get svc\nNAME                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                            AGE\nadservice               ClusterIP      10.107.196.115   <none>        9555/TCP,9090/TCP                                                  4h\ncartservice             ClusterIP      10.98.151.164    <none>        7070/TCP                                                           4h\ncheckoutservice         ClusterIP      10.97.9.197      <none>        5050/TCP                                                           4h\ncurrencyservice         ClusterIP      10.103.112.225   <none>        7000/TCP                                                           4h\nemailservice            ClusterIP      10.97.184.174    <none>        5000/TCP                                                           4h\nfrontend                ClusterIP      10.103.40.138    <none>        80/TCP,9090/TCP                                                    4h\nfrontend-external       LoadBalancer   10.108.170.241   <pending>     80:31944/TCP                                                       4h\ngrafana                 ClusterIP      10.104.141.254   <none>        3000/TCP                                                           4h\ngrafana-external        LoadBalancer   10.102.166.138   <pending>     3000:30459/TCP                                                     4h\njaeger                  ClusterIP      10.98.71.173     <none>        9411/TCP,5775/UDP,6831/UDP,6832/UDP,5778/TCP,16686/TCP,14268/TCP   4h\njaeger-external         LoadBalancer   10.96.164.126    <pending>     16686:32362/TCP                                                    4h\nkubernetes              ClusterIP      10.96.0.1        <none>        443/TCP                                                            6h\npaymentservice          ClusterIP      10.109.31.241    <none>        50051/TCP                                                          4h\nproductcatalogservice   ClusterIP      10.101.124.106   <none>        3550/TCP                                                           4h\nprometheus              ClusterIP      10.103.107.213   <none>        9090/TCP                                                           4h\nrecommendationservice   ClusterIP      10.104.225.28    <none>        8080/TCP                                                           4h\nredis-cart              ClusterIP      10.101.24.157    <none>        6379/TCP                                                           4h\nshippingservice         ClusterIP      10.104.224.18    <none>        50051/TCP                                                          4h\n```\n\n### 서비스 접속 및 Metric/Tracing 확인\n로컬 minikube환경이기 때문에 external service가 pending이므로 service를 minikube NAT IP로 expose 시킨다.\n\n```bash\n$ minikube service frontend-external\n$ minikube service grafana-external\n$ minikube service jaeger-external\n$ minikube service list\n|-------------|-----------------------|-----------------------------|\n|  NAMESPACE  |         NAME          |             URL             |\n|-------------|-----------------------|-----------------------------|\n| default     | adservice             | No node port                |\n| default     | cartservice           | No node port                |\n| default     | checkoutservice       | No node port                |\n| default     | currencyservice       | No node port                |\n| default     | emailservice          | No node port                |\n| default     | frontend              | No node port                |\n| default     | frontend-external     | http://192.168.99.101:31944 |\n| default     | grafana               | No node port                |\n| default     | grafana-external      | http://192.168.99.101:30459 |\n| default     | jaeger                | No node port                |\n| default     | jaeger-external       | http://192.168.99.101:32362 |\n| default     | kubernetes            | No node port                |\n| default     | paymentservice        | No node port                |\n| default     | productcatalogservice | No node port                |\n| default     | prometheus            | No node port                |\n| default     | recommendationservice | No node port                |\n| default     | redis-cart            | No node port                |\n| default     | shippingservice       | No node port                |\n| kube-system | kube-dns              | No node port                |\n|-------------|-----------------------|-----------------------------|\n```\n\n3개의 서비스로 각각 접속이 되는것을 확인할수 있다.\nGrafana 대시보드로 들어가면 현재 수집되는 prometheus source(http://prometheus:9090)를 통해 OpenCensus기반 Application Metric을 확인할 수 있다.\n\n![hipster_grafana](/img/hipster_grafana.png)\n\n그림에서 보면 전체 서비스 응답에 대한 99% 백분위 지연시간이 944ms 인것을 확인할 수 있다. \n\n또한 Jaeger를 통해 DAG(Directed acyclic graph) 및 서비스간 Tracing 을 확인할 수 있다.\n\n![DAG](/img/dag.png)\n\n![tracing](/img/tracing.png)\n\n## 정리\nOpenCensus 기반으로 개발자가 코드를 작성하고 Microservice를 minikube에서 배포하고 Prometheus, Jaeger Exporter 연동을 통해 시스템뿐만이 아닌 Application기반 Metrics/Stats을 수집하고 개발자가 작성한 코드를 직접 Tracing하는 간단한 데모를 진행하였다. (Istio를 포함해서 Public환경에 배포해봐도 좋은 공부가 될 것 같다)  \n\n향후 [OpenMetric](https://openmetrics.io/)과 [Opencensus](https://opencensus.io/)가 실제 개발자 기반으로 활성화되고 적용이 된다면 Telemetric 측면에서 많은 Use-Case가 도출될 수 있을것 같다.  \n\n위에서 언급했듯이 Prometheus기반 Kubernetes 클러스터를 운영하고 있는 팀의 경우 개발자의 작성 코드를 최소화할 수 있는 도구로서 충분히 활용될 수 있어 보인다.  \n\n꼭 Cloud Native 기반 Web 개발이 아니더라도 기존 공장, 금융, 병원 등 의 IoT나 센서/설비를 위한 비즈니스에도 Backend로서 확장성있는 도구로서 활용이 될 수 있을것 같다."
    },
    {
      "id": "kubernetes/knative-using-gloo/",
      "metadata": {
        "permalink": "/kubernetes/knative-using-gloo/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-02-28-knative-using-gloo.md",
        "source": "@site/blog/2019-02-28-knative-using-gloo.md",
        "title": "Knative with Gloo",
        "description": "Istio에 종속되어있는 Knative가 아닌 경량화된 Ingress 오픈소스 Gloo를 활용한 Knative 설치 및 활용",
        "date": "2019-02-28T00:00:00.000Z",
        "formattedDate": "February 28, 2019",
        "tags": [
          {
            "label": "Knative",
            "permalink": "/tags/knative"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "FaaS",
            "permalink": "/tags/faa-s"
          },
          {
            "label": "Serverless",
            "permalink": "/tags/serverless"
          },
          {
            "label": "CRDs",
            "permalink": "/tags/cr-ds"
          },
          {
            "label": "CloudEvents",
            "permalink": "/tags/cloud-events"
          },
          {
            "label": "Mesh",
            "permalink": "/tags/mesh"
          },
          {
            "label": "Gloo",
            "permalink": "/tags/gloo"
          }
        ],
        "readingTime": 8.575,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Knative with Gloo",
          "comments": true,
          "classes": "wide",
          "description": "Istio에 종속되어있는 Knative가 아닌 경량화된 Ingress 오픈소스 Gloo를 활용한 Knative 설치 및 활용",
          "slug": "kubernetes/knative-using-gloo/",
          "date": "2019-02-28T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Knative",
            "Kubernetes",
            "FaaS",
            "Serverless",
            "CRDs",
            "CloudEvents",
            "Mesh",
            "Gloo"
          ]
        },
        "prevItem": {
          "title": "Cloud-Native Microservices Demo Application with OpenCensus",
          "permalink": "/kubernetes/microservices-demo/"
        },
        "nextItem": {
          "title": "Knative CLI - knctl",
          "permalink": "/kubernetes/knative-knctl/"
        }
      },
      "content": "## Knative Routing\nKnative는 앞에서도 몇번 언급하였지만 기본적으로 `Routing`을 사용하여 외부에 노출할 서비스들에 대한 HTTP Endpoint를 제공한다. 어떻게 보면 기본적으로 API Gateway 역할을 하기도 하고 Ingress 역할을 하기도 한다. 보통 Service mesh인 `Istio`를 사용하여 ingress를 구현하는것이 당연하다고 생각하기도 하지만 Istio의 모든 기능이 Knative에 필요하지는 않고 설치되는것 자체가 리소스 소모가 꽤 된다는것은 설치 해본사람은 알고 있을것이다. \n\n## Service \n### Kubernetes\n![ingress](https://www.nginx.com/wp-content/uploads/2017/09/NGINX-Plus-Features-Kubernetes-Ingress-Controller-644x372@2x.png)  \n이미지출처 : https://www.nginx.com/blog/announcing-nginx-ingress-controller-for-kubernetes-release-1-3-0/\n\n\nKubernetes에서는 일반적으로 서비스 접속을 구현하게 되면 기본적으로 Pod와 Service를 생성하고 Ingress를 사용하여 클러스터 내부로 들어오는 트래픽을 처리하게 된다.\n\n### Knative\n![Serving](https://i1.wp.com/blog.openshift.com/wp-content/uploads/intro.png?w=499&ssl=1)  \n이미지출처 : https://blog.openshift.com/knative-serving-your-serverless-services/\n\n\nKnative에서는 앞선 Knative 관련 포스팅에서도 설명했듯이 `Automatic scaling up and down to zero` 특성을 가지고 있기에 Pod가 최초 실행되어있지 않은 상태에서 트래픽이 들어오게 되면 [Knative Serving Activator](https://github.com/knative/serving/blob/master/docs/scaling/DEVELOPMENT.md)에 의해서 Pod가 없는 Revision을 확인하고 Cold Start 형태로 프로비저닝하게 된다. 나는 이게 진정한 서버리스라고 생각하지만 주변에 반박하시는 분들도 간혹 있다.\n\n이후 Pod가 Warm 상태가 되고 나면 Istio Route(Ingress Gateway)를 통해 트래픽이 Pod로 전달되어 통신이 이뤄지게 된다.\n\n현재 Knative는 현재 Ingress Gateway 의존성을 가지고 있고 Envoy기반 Service Mesh인 `Istio`, Envoy기반 API Gateway인 `Gloo` 두가지 옵션으로 Ingress 구현이 가능하다.\n\n\n## Istio \nKnative는 기본적으로 Ingress Gateway기능을 탑재하고 있는데 이는 Istio의 기능중 하나다.  \nIstio는 다음과 같은 Core Feature를 가진다. 상세한 내용은 [https://istio.io/docs/concepts/what-is-istio/](https://istio.io/docs/concepts/what-is-istio/) 에서 확인하면 된다.\n\n* Traffic management\n* Security\n* Policies and Telemetry\n* Performance and Scalability\n\nIstio는 48개의 `CRDs`(CustomResourceDefinition objects)를 가지고 있는데 이중 Knative Serving에서 사용하는건 `VirtualService` 단 하나다.\n\n## Gloo\n[Gloo](https://gloo.solo.io/)는 Kubernetes-native ingress controller이자 [Next Generation API Gateway](https://medium.com/solo-io/announcing-gloo-the-function-gateway-3f0860ef6600) 를 위해 시작된 프로젝트이다. 실제 Redhat에서 Openshift기반 Microservice 및 Istio 개발업무를 하다가 최근에 solo.io의 CTO로 이직한 [Christian Posta](https://blog.christianposta.com/)가 밀고 있는 프로젝트이기도 하다. \n\n\n![gloo](https://cdn-images-1.medium.com/max/1600/0*Z0Jb5DJFOyeY91sN.)\n\n\n`Gloo`는 Envoy Proxy 기반으로 동작하며 \n기존 Legacy부터 Container서비스, FaaS(AWS Lambda, Azure Functions, GCP Functions)영역의 Application들을 REST, gRPC, SOAP, Web Socker기반으로 Aggregate 해서 Function 기반 추상화를 구현해 주는 오픈소스 프로젝트라 정의 할 수 있다. \n\nIstio의 Ingress기능외의 여러가지 부가 기능(Telemetry, Security, Policy Enforcement)들은 Knative에서는 필요로 하지 않는다. \n\nKnative API Gateway 로서 Istio가 아닌 Gloo가 조금더 경량화된 대안으로 결정되었고 Gloo를 통해 Knative 설치가 가능하게 되었다. 단, Knative Eventing 컴포넌트는 현재 지원하지 않는다고 한다. \n\n## Install Knative with Gloo\n\n참고: [Install with Gloo](https://github.com/knative/docs/blob/master/install/Knative-with-Gloo.md)\n\n간단하게 gloo와 Knative 설치를 해보자.\n\n### Requirements\n* Kubernetes cluster v1.11 or newer \n* Enable [MutatingAdmissionWebhook admission controller](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#how-do-i-turn-on-an-admission-controller)\n* kubectl v1.10 or newer\n* Bash in Mac or Linux\n\n### Install Glooctl\n\ngloo CLI (glooctl) Download  \n[https://github.com/solo-io/gloo/releases](https://github.com/solo-io/gloo/releases)\n\n또는 직접 Download\n```\n$ curl -sL https://run.solo.io/gloo/install | sh\nAttempting to download glooctl version v0.8.1\nDownloading glooctl-darwin-amd64...\nDownload complete!, validating checksum...\nChecksum valid.\nGloo was successfully installed 🎉\n\nAdd the gloo CLI to your path with:\n  export PATH=$HOME/.gloo/bin:$PATH\n\nNow run:\n  glooctl install gateway     # install gloo's function gateway functionality into the 'gloo-system' namespace\n  glooctl install ingress     # install very basic Kubernetes Ingress support with Gloo into namespace gloo-system\n  glooctl install knative     # install Knative serving with Gloo configured as the default cluster ingress\nPlease see visit the Gloo Installation guides for more:  https://gloo.solo.io/installation/\n```\n\nPATH 등록\n```\n$ export PATH=$HOME/.gloo/bin:$PATH\n```\n\ngloo CLI 확인\n```\n$ glooctl --version\nglooctl version 0.8.1\n```\n\nGCP 무료플랜으로 3-node 클러스터를 생성한다.\n```\n$ gcloud container clusters create gloo \\\n  --region=asia-east1-a \\\n  --cluster-version=latest \\\n  --machine-type=n1-standard-2 \\\n  --enable-autorepair \\\n  --num-nodes=3\n```\n\ncluster 생성된것을 확인하고 cluster-admin 권한을 할당한다.\n```\n$ kubectl get nodes\nNAME                                  STATUS    ROLES     AGE       VERSION\ngke-gloo-default-pool-f6bcc479-f8v9   Ready     <none>    9m        v1.11.7-gke.6\ngke-gloo-default-pool-f6bcc479-fl78   Ready     <none>    9m        v1.11.7-gke.6\ngke-gloo-default-pool-f6bcc479-gfgw   Ready     <none>    9m        v1.11.7-gke.6\n\n$ kubectl create clusterrolebinding cluster-admin-binding \\\n>   --clusterrole=cluster-admin \\\n>   --user=$(gcloud config get-value core/account)\nYour active configuration is: [cloudshell-25974]\nclusterrolebinding.rbac.authorization.k8s.io \"cluster-admin-binding\" created\n```\n\nGloo와 Knative 설치를 한다. 미리 `glooctl install knative --dry-run` 으로 전체 manifest를 확인할 수 있다. \n```\n$ glooctl install knative\n```\n\n위에서 설치 과정은 생략했지만 Istio에 비해 CRD 개수가 적은 것을 알수있다. 또한 설치된 컴포넌트 역시 Istio에 비해서 간소화된 것을 알수 있다. \n```\n$ kubectl get pods --namespace gloo-system                                                                                         \nNAME                                   READY     STATUS    RESTARTS   AGE\nclusteringress-proxy-59fd6fb56-dmwwm   1/1       Running   0          7m\ndiscovery-779884d4cc-xlql2             1/1       Running   6          7m\ngloo-844fc79445-f4zvg                  1/1       Running   6          7m\ningress-7d75c99874-s4m76               1/1       Running   6          7m\n\n$ kubectl get pods --namespace knative-serving\nNAME                          READY     STATUS    RESTARTS   AGE\nactivator-746f6bb684-49tfh    1/1       Running   0          12m\nautoscaler-955f679cd-tx5vw    1/1       Running   0          12m\ncontroller-7fc84c6584-jbn69   1/1       Running   0          12m\nwebhook-7797ffb6bf-6pgsw      1/1       Running   0          12m\n```\n\n이전 포스팅에서도 사용했던 `gcr.io/knative-sample/helloworld-go` 이미지를 활용하여 샘플앱 Knative Service를 만든다.\n\n#### service.yaml\n```\n$ vi service.yaml\n\napiVersion: serving.knative.dev/v1alpha1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  runLatest:\n    configuration:\n      revisionTemplate:\n        spec:\n          container:\n            image: gcr.io/knative-sample/helloworld-go\n            env:\n              - name: TARGET\n                value: \"Go Sample v1\"\n```\n\n```\n$ kubectl apply --filename service.yaml\nservice.serving.knative.dev \"helloworld-go\" created\n```\n\n앞에서도 설명했지만 `Automatic scaling up and down to zero` 으로 Cold Start가 되고 잠시후에 아래와 같이 Knative Service를 확인할 수 있다. \n```\n$ kubectl get ksvc helloworld-go -n default  --output=custom-columns=NAME:.metadata.name,DOMAIN:.status.domain]($ kubectl get ksvc helloworld-go -n default  --output=custom-columns=NAME:.metadata.name,DOMAIN:.status.d\nomain\nNAME            DOMAIN\nhelloworld-go   helloworld-go.default.example.com\n```\n\nGloo Ingress를 확인한다. GKE에서 설치했기 때문에 자동으로 LoadBalancer가 연동되어 있는것을 확인할 수 있다. \n```\n$ kubectl get svc -n gloo-system\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\nclusteringress-proxy   LoadBalancer   10.3.244.54    34.**.**.54   80:30978/TCP,443:32448/TCP   39m\ngloo                   ClusterIP      10.3.243.231   <none>        9977/TCP                     39m\n\n$ glooctl proxy url --name clusteringress-proxy\nhttp://34.**.**.54:80\n```\n\n위에서 얻은 두가지 정보로 생성된 app을 테스트한다. Cold Start(default timeout 5분) 때문에 응답이 늦어질 수도 있지만 잠시 기다리면 응답을 확인할 수 있다.\n```\n$ curl -H \"Host: helloworld-go.default.example.com\" http://34.**.**.54:80\nHello Go Sample v1!\n```\n\n물론 `Revision`이나 `Route`를 활용하여 Knative의 기능에 대해서도 확인이 가능하다.\n\n## 정리\nGloo는 Knative ClusterIngress CRD를 기반으로 동작하는 Istio의 대안으로서 가능성을 보여주고 있다. 이외에도 The Service Mesh Orchestration Platform `SuperGloo`, Debugger for microservices `Squash` 등 다양한 Mesh Layer기반의 오픈소스들을 확인할수 있다. 또다른 스쳐지나갈수도 있는 오픈소스일수도 있겠지만 현재 개발되는 로드맵(https://www.solo.io/)을 보면 Knative가 고도화되는 여정에 같이 가는 모습을 확인할 수 있다. \n\nnext-generation API Gateway로서 다양한 프로토콜을 지원하기 때문에 (HTTP1, HTTP2, gRPC, REST/OpenAPISpec, SOAP, WebSockets, Lambda/Cloud Functions) 더욱더 Microservices 및 Serverless Workload를 수행하기에 더욱 적합한 오픈소스로 보인다. \n\n## 다음 주제\n현재 해보고 싶은것은 베어메탈 Kubernetes Cluster기반 BGP로 동작하는 [MetalLB](https://metallb.universe.tf/)나 [Cillium on AWS](https://cilium.io/try-eks/) 인데 시간나는 대로 테스트 해봐야 겠다."
    },
    {
      "id": "kubernetes/knative-knctl/",
      "metadata": {
        "permalink": "/kubernetes/knative-knctl/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-02-18-knative-knctl.md",
        "source": "@site/blog/2019-02-18-knative-knctl.md",
        "title": "Knative CLI - knctl",
        "description": "Knative CLI tool knctl에 대해 알아보자",
        "date": "2019-02-18T00:00:00.000Z",
        "formattedDate": "February 18, 2019",
        "tags": [
          {
            "label": "Knative",
            "permalink": "/tags/knative"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "FaaS",
            "permalink": "/tags/faa-s"
          },
          {
            "label": "Serverless",
            "permalink": "/tags/serverless"
          },
          {
            "label": "CRDs",
            "permalink": "/tags/cr-ds"
          },
          {
            "label": "CloudEvents",
            "permalink": "/tags/cloud-events"
          },
          {
            "label": "Mesh",
            "permalink": "/tags/mesh"
          },
          {
            "label": "knctl",
            "permalink": "/tags/knctl"
          }
        ],
        "readingTime": 14.54,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Knative CLI - knctl",
          "comments": true,
          "classes": "wide",
          "description": "Knative CLI tool knctl에 대해 알아보자",
          "slug": "kubernetes/knative-knctl/",
          "date": "2019-02-18T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Knative",
            "Kubernetes",
            "FaaS",
            "Serverless",
            "CRDs",
            "CloudEvents",
            "Mesh",
            "knctl"
          ]
        },
        "prevItem": {
          "title": "Knative with Gloo",
          "permalink": "/kubernetes/knative-using-gloo/"
        },
        "nextItem": {
          "title": "Hybrid Cloud",
          "permalink": "/cloud/Hybrid/"
        }
      },
      "content": "## knctl\n`knctl` 은 Knative CLI 툴로 간단하게 knative cluster를 만들고 Knative를 추상화해서 앱까지 배포할 수 있는 오픈소스이다. \n\n#### 참고\n* https://github.com/cppforlife/knctl  \n* https://developer.ibm.com/blogs/2018/11/12/knctl-a-simpler-way-to-work-with-knative/  \n* https://starkandwayne.com/blog/public-traffic-into-knative-on-gke/\n\n### Knative 다시 살펴보기\n\n[앞선 포스팅에서도 이야기](https://ddii.dev/kubernetes/knative/) 했지만 기존 FaaS(AWS Lambda, Google Cloud Funtions, Azure Function) 과는 다른 Serverless 개념으로 받아들어야 한다.\n\n다시 한번 특징을 나열해보면 아래와 같다.\n\n- Serverless Container의 신속한 배치 \n- Automatic scaling up and down to zero\n- Istio를 백엔드로 활용하여 Routing 구현\n- 배포 된 코드 및 config의 특정 시점 스냅 샷\n\n그리고 다음과 같은 CRDs(Custom Resource Definitions)로 구성된 오브젝트들로 정의된다.\n\n- `Route`는 사용자 서비스에 대한 HTTP endpoint와 Routing을 제공한다.\n- `Revisions`은 code(function)와 config로 구성된 불변의 스냅샷. Route를 통해 endpoint를 할당받지 못한 Revision은 자동으로 kubernetes resource에서 삭제됨\n- `Configuration`은 요구되는 Revision 최신 상태를 기록하고 생성하고 추적할수 있음. 소스 패키지(git repo나 archive)를 컨테이너로 변환하기 위한 내용이나 메타데이터등을 포함시킬수 있음. \n- `Service`는 `Routes`와 `Configurations` 리소스의 추상화된 집합체. 모든 워크로드의 lifecycle을 관리함. 트래픽을 항상 최신의 Revision으로 route되도록 정의할수 있음\n\n하나씩 조금 자세히 이야기 하면 아래처럼 정리 할수 있다.\n\n### Route\n**Route**는 사용자 서비스(Code와 Configuration의 Revision정보)의 네트워크 Endpoint를 제공한다. kubernetes namespace는 여러개의 `Route`를 가질수 있다. `Route`는 하나 이상의 `Revisions`을 가지면서 수명이 길고 안정적인 HTTP Endpoint를 제공한다. 기본구성은 `Route` 객체가 `Configuration`에 의해 생성된 최신의 Revision으로 트래픽을 자동으로 지정한다. 조금더 복잡한 경우로는 istio의 기능을 활용하여 트래픽을 백분율 기준으로 `Route`를 지정할 수 있다. \n\n### Revision\n**Revision**은 Code와 Configuration의 불변의 스냅샷이다. 하나의 `Revision`은 컨테이너 이미지를 참조하고 선택적으로 `Build`를 참조할 수 있다. `Revision`은 **Configuration**이 업데이트 시 생성된다.  \n`Route`를 통해 http주소 지정이 불가능한 `Revision`은 폐기 되고 관련된 kubernetes 리소스가 삭제가 된다. 시간이 지남에 따라 `Configuration`이 생성한 `Revision` 히스토리가 제공되고 사용자는 이전 `Revision`로 쉽게 롤백 할 수 있다.  \n\n### Configuration\n`Configuration`은 최신의 `Revision`상태를 설명하고, 생성하고, 원하는 상태가 갱신될때 `Revision`의 상태를 추적한다. `Configuration`은 `Build`를 참조하여 소스(git repo 또는 archive)를 컨테이너로 변환하는 방법에 대한 가이드가 포함되어 있거나 단순히 컨테이너 이미지 및 수정에서 필요한 메타 데이터 만 참조 할 수 있다. \n\n### Product Integration\n\n2019년 2월 현재 0.3이 릴리스되고 있고 벌써 여러 제품에 통합이 되고 있다. \n\n최근 IBMthink 2019에서 Managed Knative (Experimental)를 내놓기도 하였다.  \nhttps://www.ibm.com/blogs/bluemix/2019/02/announcing-managed-knative-on-ibm-cloud-kubernetes-service-experimental/ \n\nIstio를 포함한 Knative 마저도 품는 모습으로 managed kubernetes 영역에서 글로벌 플레이어들 모두 서로 치고나가는 모습들을 볼수 있다. \n\n작년 11월에는 Gitlab 제품안에 serverless라는 extension형태의 서비스가 추가 되기도 하였고,  \nhttps://about.gitlab.com/press/releases/2018-12-11-gitlab-and-triggermesh-announce-gitlab-serverless.html\n\ntriggermesh 라는 곳에서는 serverless management platform이라는 이름으로 knative 기반 멀티 서버리스 플랫폼을 출시하기도 하였다.  \nhttps://triggermesh.com/serverless_management_platform/\n\nPivotal Function Service (PFS), Google GKE SERVERLESS ADD-ON 등은 아직 early access 신청만 받고 있는 상태이다.\n\n오늘은 간단하게 배포할수 있는 툴 knctl과 관련 use-case를 소개하고자 한다.\n\n### Kubernetes Cluster 생성\n\n일단 GKE Free tier에서 Cluster를 하나 생성하자.\n\n```\ngcloud container clusters create knative \\\n  --region=asia-east1-a \\\n  --cluster-version=latest \\\n  --machine-type=n1-standard-2 \\\n  --enable-autoscaling --min-nodes=1 --max-nodes=5 \\\n  --enable-autorepair \\\n  --scopes=service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore \\  \n  --num-nodes=3\n```\n\ncluster 생성된것을 확인하고 cluster-admin 권한을 할당한다. \n```\n$ kubectl get nodes\nNAME                                     STATUS    ROLES     AGE       VERSION\ngke-knative-default-pool-d1a39347-5m5t   Ready     <none>    1m        v1.11.7-gke.4\ngke-knative-default-pool-d1a39347-l6zh   Ready     <none>    1m        v1.11.7-gke.4\ngke-knative-default-pool-d1a39347-qv5r   Ready     <none>    1m        v1.11.7-gke.4\n\n$ kubectl create clusterrolebinding cluster-admin-binding \\\n  --clusterrole=cluster-admin \\\n  --user=$(gcloud config get-value core/account)\nYour active configuration is: [cloudshell-4728]\nclusterrolebinding.rbac.authorization.k8s.io \"cluster-admin-binding\" created\n```\n\n## knctl 설치\n\n이번 포스팅에서는 Mac OS 설치 기준으로 작성하였다.\n\n#### homebrew 설치\n```\nbrew install starkandwayne/kubernetes/knctl\n```\n\n#### binary\nhttps://github.com/cppforlife/knctl/releases \n```\n$ wget https://github.com/cppforlife/knctl/releases/download/v0.1.0/knctl-darwin-amd64\n$ mv knctl-* /usr/local/bin/knctl\n$ chmod +x /usr/local/bin/knctl\n```\n\n## knctl 로 Knative 배포\n\n설치한 knctl로 Knative 배포를 진행한다. 설치되는 내용을 지켜보고 있으면 `istio`를 먼저 배포하고 그다음에 `Knative`를 설치하는 것을 확인할 수 있다. 배포되는 모듈들의 상태를 하나하나 체크해서 배포하기 때문에 설치상에 과정들을 확인할 수 있다. \n\n```\n$ knctl install --exclude-monitoring\n```\n\n테스트를 위한 namespace `hello-test`를 생성한다.\n```\n$ kubectl create namespace hello-test\nnamespace \"hello-test\" created\n```\n\nknctl deploy 명령으로 최초 revision을 배포한다.  \n아래 결과를 보면 hello-00001 이라고 하는 최초의 revision을 작성하기 때문에 `latest` tag를 달고 배포를 하게 된다. \n```\n$ knctl deploy \\\n      --namespace hello-test \\\n      --service hello \\\n      --image gcr.io/knative-samples/helloworld-go \\\n      --env TARGET=Rev1\n\nName  hello\n\nWaiting for new revision to be created...\n\nTagging new revision 'hello-00001' as 'latest'\n\nTagging new revision 'hello-00001' as 'previous'\n\nAnnotating new revision 'hello-00001'\n\nWaiting for new revision 'hello-00001' to be ready for up to 5m0s (logs below)...\n\nhello-00001 > hello-00001-deployment-5cdbfc9bc9-hks6t | 2019/02/17 22:27:50 Hello world sample started.\n\nRevision 'hello-00001' became ready\n\nContinuing to watch logs for 5s before exiting\n\nSucceeded\n```\n\n```\nkubectl get svc knative-ingressgateway -n istio-system\nNAME                     TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                                                                                                                   AGE\nknative-ingressgateway   LoadBalancer   10.63.253.209   34.***.***.248   80:32380/TCP,443:32390/TCP,31400:32400/TCP,15011:30082/TCP,8060:31125/TCP,853:32009/TCP,15030:31102/TCP,15031:31631/TCP   6h\n```\n\n위처럼 Knative가 프로비저닝 되면서 ingress-gateway가 하나 생성이 되어있는 것을 확인할 수 있고 knctl로도 ingress를 확인이 가능하다.\n```\n$ knctl ingress list\nIngresses\n\nName                    Addresses     Ports  Age\nknative-ingressgateway  34.***.***.248  80     6h\n                                        443\n                                        31400\n                                        15011\n                                        8060\n                                        853\n                                        15030\n                                        15031\n\n1 ingresses\n\nSucceeded\n```\n\n## Knative custom domain 연결\n\nDomain이 별도로 없기 때문에 Knative는 내부적으로 example.com이라고 하는 기본 domain을 사용한다. 그래서 실제 `knctl curl` 명령은 내부적으로 `hello.hello-test.example.com`으로 curl을 실행하게 되고 해당 결과를 아래와 같이 확인할 수 있다.\n\n```\n$ knctl curl --service hello -n hello-test\nRunning: curl '-sS' '-H' 'Host: hello.hello-test.example.com' 'http://34.***.***.248:80'\n\nHello Rev1!\n\nSucceeded\n```\n\nkubernetes node가 3개이므로 3개의 pod가 생성된 것을 확인할 수 있다. 일정시간(default:5분)이 지나면 `zero to scale` 관점에서 pod가 종료되므로 다시 확인할때는 다시 curl 명령을 날리게 되면 다시 pod가 올라오게 된다. 해당 개념은 FaaS또는 AWS Lambda에서 Cold-Start와 동일한 것이라 볼 수 있다.  \n\nAWS Cold Start 참고 : https://novemberde.github.io/aws/2018/02/02/Lambda_coldStart.html\n```\n$ kubectl get po\nNAME                                      READY     STATUS    RESTARTS   AGE\nhello-00001-deployment-5cdbfc9bc9-hks6t   3/3       Running   0          4m\n```\n\n가지고 있는 도메인이 있다면 위에서 나온 `34.***.***.248` IP를 domain에 매핑해보자.\n아래에서는 기존 보유중인 skcloud.io 도메인을 연결하였다.\n```\n$ dig knative.skcloud.io\n;; ANSWER SECTION:\nknative.skcloud.io.     603     IN      A       34.***.***.248\n```\n\nknctl domain을 이용하여 default domain을 `knative.skcloud.io`로 변경한다.\n```\n$ knctl domain create -d knative.skcloud.io --default\nSucceeded\n```\n\nknctl routes 명령으로 해당 hello-test app의 Endpoint 정보를 확인할 수 있다.\n```\n$ knctl routes list -n hello-test\nRoutes in namespace 'hello-test'\n\nName   Domain                               Traffic        Annotations  Conditions  Age\nhello  hello.hello-test.knative.skcloud.io  100% -> hello  -            3 OK / 3    1h\n\n1 routes\n\nSucceeded\n```\n\n5분이상 기다렸다가 curl로 확인하면 Cold-Start 되는 시간(몇초) 지연이 발생하고 결과를 확인할 수 있다.\n이후에는 바로 응답을 확인할 수 있다. \n```\n$ curl http://hello.hello-test.knative.skcloud.io/\nHello Rev1!\n```\n\n## revision 추가\n\n이번에는 revision을 추가해보자. TARGET environment 변수를 `Rev2`로 수정하고 배포를 한다.\n기존 hello-00002 revision이 최신 revision으로 갱신되어 배포가 되는것을 확인할 수 있다.\n```\n$ knctl deploy --service hello \\\n    --image gcr.io/knative-samples/helloworld-go \\\n    --env TARGET=Rev2\nName  hello\n\nWaiting for new revision (after revision 'hello-00001') to be created...\n\nTagging new revision 'hello-00002' as 'latest'\n\nTagging older revision 'hello-00001' as 'previous'\n\nAnnotating new revision 'hello-00002'\n\nWaiting for new revision 'hello-00002' to be ready for up to 5m0s (logs below)...\n\nhello-00002 > hello-00002-deployment-6cf86bbfc7-sblz9 | 2019/02/17 23:25:43 Hello world sample started.\n\nRevision 'hello-00002' became ready\n\nContinuing to watch logs for 5s before exiting\n\nSucceeded\n```\n\n신규 `revision` 서비스를 추가된것을 확인할 수 있다. 마찬가지로 몇초간의 Cold-Start delay가 발생할 수도 있다. \n```\n$ curl http://hello.hello-test.knative.skcloud.io/\nHello Rev2!\n\n$ knctl curl --service hello\nRunning: curl '-sS' '-H' 'Host: hello.hello-test.knative.skcloud.io' 'http://34.***.***.248:80'\n\nHello Rev2!\n\nSucceeded\n```\n\nrevision list를 확인해보면 현재 latest, previous TAG정보를 확인할 수 있다.\n```\n$ knctl revision list --service hello\nRevisions for service 'hello'\n\nName         Tags      Annotations  Conditions  Age  Traffic\nhello-00002  latest    -            1 OK / 4    14m  100% -> hello.hello-test.knative.skcloud.io\nhello-00001  previous  -            1 OK / 4    4h   -\n\n2 revisions\n\nSucceeded\n```\n\n## Blue/Green 배포\n\nBlue/Green Deploy는 knctl rollout 명령으로 수행할수 있다.  \nrollout 할때 `--managed-route=false` 옵션을 줘야 특정 비율로 routing이 가능하다.  \n아래 예시는 TARGET environment 변수를 `blue`, `green`으로 바꿔가면서 배포를 진행하였다.\n```\n$ knctl deploy --service hello \\\n    --image gcr.io/knative-samples/helloworld-go \\\n    --env TARGET=blue \\\n    --managed-route=false\nName  hello\n\nWaiting for new revision (after revision 'hello-00002') to be created...\n\nTagging new revision 'hello-00003' as 'latest'\n\nTagging older revision 'hello-00002' as 'previous'\n\nAnnotating new revision 'hello-00003'\n\nWaiting for new revision 'hello-00003' to be ready for up to 5m0s (logs below)...\n\nRevision 'hello-00003' became ready\n\nContinuing to watch logs for 5s before exiting\n\nhello-00003 > hello-00003-deployment-99478dcc5-jf267 | 2019/02/17 23:48:20 Hello world sample started.\n\nSucceeded\n```\n\nrevision list를 확인하면 아래와 같이 `latest`로 Traffic 전체가 routing 되는 것을 확인할 수 있다.\n```\n$ knctl revision list --service hello\nRevisions for service 'hello'\n\nName         Tags      Annotations  Conditions  Age  Traffic\nhello-00005  latest    -            4 OK / 4    44s  100% -> hello.hello-test.knative.skcloud.io\nhello-00004  previous  -            4 OK / 4    2m   -\nhello-00003  -         -            1 OK / 4    5m   -\nhello-00002  -         -            1 OK / 4    28m  -\nhello-00001  -         -            1 OK / 4    4h   -\n\n5 revisions\n\nSucceeded\n```\n\n이후에 rollout을 통해 previous로 90%, latest로 10%로 변경을 하면 즉시 반영이 되고 실제 트래픽도 분산되어 들어온다. %가 낮은 쪽으로 routing이 될 경우 Cold-Start가 발생하게 되면 delay는 발생하게 된다.\n```\n$ knctl rollout --route hello -p hello:latest=10% -p hello:previous=90%\nSucceeded\n\n$ knctl revision list\nRevisions\n\nService  Name         Tags      Annotations  Conditions  Age  Traffic\nhello    hello-00005  latest    -            2 OK / 4    1h   10% -> hello.hello-test.knative.skcloud.io\n~        hello-00004  previous  -            2 OK / 4    1h   90% -> hello.hello-test.knative.skcloud.io\n~        hello-00003  -         -            1 OK / 4    1h   -\n~        hello-00002  -         -            1 OK / 4    1h   -\n~        hello-00001  -         -            1 OK / 4    5h   -\n\n5 revisions\n\nSucceeded\n```\n\n간단하게 curl 반복문을 작성하여 돌려보자. \n```\n#!/bin/sh\nwhile true\ndo\ncurl -sS --max-time 3 http://hello.hello-test.knative.skcloud.io/\ndone\n```\n\n간단하게 위 sh을 돌리면 아래와 같이 Cold-Start delay가 발생할때 time out이 발생하고 이후 green revision으로 접속이 되는것을 볼 수 있다. \n```\n$ ./test.sh\ncurl: (28) Operation timed out after 3002 milliseconds with 0 bytes received\nHello blue!\nHello blue!\nHello blue!\nHello blue!\nHello blue!\ncurl: (28) Operation timed out after 3003 milliseconds with 0 bytes received\nHello blue!\nHello blue!\nHello blue!\nHello blue!\nHello blue!\nHello blue!\nHello green!\nHello blue!\nHello blue!\nHello blue!\n```\n\n## 정리\n지금까지 `knctl`을 사용하여 간단하게 knative를 배포하고 custom domain을 연결하여 blue-green 배포까지 해봤다. \n이외에도 Knative Build를 활용하여 Docker image 작업을 하거나 서비스 카탈로그 등을 연동하여 외부 DBaaS를 연동하는 use-case등을 찾아볼수 있다.\n\n아직 초기 단계이지만 Knative는 istio와 함께 IBM, Google, Pivotal등 global player들의 차세대 오픈소스로 부상하고 있다고 볼 수 있다.  \n\n`Zero to scale` 이라는 슬로건아래 Serverless, FaaS 사상을 기반으로 build, serving, event, routing이라고 하는 Cloud Computing 추상화의 끝판으로 진화하고 있다. 앞으로 어떤 모습으로 진화될지 궁금하고 다음번에는 MQ나 Pub/sub를 연동하거나 멀티 클라우드 환경에서 동작하는 모습을 보여주는 것도 좋을것 같다. 희망사항이지만 올해 OpenInfraDay나 Kubernetes Day Korea 행사에서 Hands-on을 진행해보는것도 좋지 않을까?"
    },
    {
      "id": "cloud/Hybrid/",
      "metadata": {
        "permalink": "/cloud/Hybrid/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-02-10-Hybrid.md",
        "source": "@site/blog/2019-02-10-Hybrid.md",
        "title": "Hybrid Cloud",
        "description": "Hybrid Cloud",
        "date": "2019-02-10T00:00:00.000Z",
        "formattedDate": "February 10, 2019",
        "tags": [
          {
            "label": "Hybrid",
            "permalink": "/tags/hybrid"
          },
          {
            "label": "Multi",
            "permalink": "/tags/multi"
          },
          {
            "label": "Outpost",
            "permalink": "/tags/outpost"
          },
          {
            "label": "VMware",
            "permalink": "/tags/v-mware"
          }
        ],
        "readingTime": 9.375,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Hybrid Cloud",
          "comments": true,
          "classes": "wide",
          "description": "Hybrid Cloud",
          "slug": "cloud/Hybrid/",
          "date": "2019-02-10T00:00:00.000Z",
          "categories": [
            "Cloud"
          ],
          "tags": [
            "Hybrid",
            "Multi",
            "Outpost",
            "VMware"
          ]
        },
        "prevItem": {
          "title": "Knative CLI - knctl",
          "permalink": "/kubernetes/knative-knctl/"
        },
        "nextItem": {
          "title": "SRE (Site Reliablity Engineering)",
          "permalink": "/job/SRE/"
        }
      },
      "content": "오늘도 기술적인 이야기보다는 화두가 되고 있는 하이브리드 클라우드 이야기를 해보고자 한다.\n\n업계 사람들도 하이브리드, 엣지 클라우드 서비스를 긍정적인 시각으로 보고있지만 성숙도 측면에서 문제가 있어 도입을 꺼려왔던게 사실이다. 최근 동향을 봤을때 퍼블릭 클라우드 공급사에서도 프라이빗 클라우드를 포섭해야할 대상으로 인정하고 공격적으로 하이브리드 클라우드 솔루션을 개발하여 제공하려고 하는 움직임을 보이고 있다.\n\n엔터프라이즈에서는 입장에서는 Scale, DR측면에서 On-Prem 에서 퍼블릭으로 확장을 도모하고 있고 글로벌플레이어 입장에서는 퍼블릭에서 On-Prem으로 전이하는 모습으로 비즈니스를 진행하고 있다. 하지만 구글은 특별하게 컨테이너 기반으로 진행중이다. 누가 끝까지 살아남아 승자가 될지 아무도 모른다. \n\n앞으로 열릴 시장은 확실하고 꼭 필요하다는 것은 알지만 기술의 성숙도가 높지 않고 넘어야할 허들이 많다. 아직까지 연계 기술이나 생태계가 비어 있는 부분들이 존재하기 때문에 올해 말쯤 되면 여러 상품들이 출시되면서 정리 되지 않을까 싶다. \n\nhttps://wikibon.com/aws-outposts-enables-hybrid-cloud/\n\n위 포스팅에서도 2가지 의문점을 제기한다.\n\n* Will a data egress charge be applied to data resident on the disks to other on-premise workloads?  \n`예시) S3 → Outpost egress 트래픽(private n/w 이긴 하지만 aws가 돈을 안 받을것인가?, 결국 direct-link는 필수인건지?)`\n\n* Is the data on site under the legal control of AWS or the customer?  \n`데이터의 소유권 문제(이게 심각한 문제가 될수 있다)`\n\n그래서 AWS가 작년에 내놓은 Outpost의 첫그림은 [VMware on AWS](https://aws.amazon.com/ko/outposts/features/)로 작년 reinvent이후 많은 관심을 받았던 프로젝트이다. \n\n이외에도 메이저 사들도 [Azure Stack](https://azure.microsoft.com/ko-kr/overview/azure-stack/), [VMware on IBM Cloud](https://www.ibm.com/kr-ko/cloud/vmware), [VMware on Ncloud](https://www.ncloud.com/product/hybridPrivateCloud/vmwareOnNcloud)와 같은 하이브리드 서비스를 내놓고 있다. \n\n### Product별 기본 전략\n#### Azure Stack\n* 호환성 인증받은 서버를 On-prem에 구매/설치하고 Azure UI 및 API와 동일한 UI/UX로 private cloud 사용\n\n#### AWS Outpost\n* 고객 On-prem에 EC2 cloud instances 제공 (전용 HPC 하드웨어 - EC2 Nitro Platform)\n\n#### AWS Outpost(VMware based)\n* baremetal 전용 플랫폼으로 VMware - software, AWS - hardware 에 집중\n\n#### Google Cloud\n* K8s On-prem전략을 기본으로 Cisco와 하이브리드 전략 (based on Istio)\n\n#### IBM Cloud\n* 구글과 유사하게 Istio를 기본으로 하여 멀티, 하이브리드 클라우드 제품 출시\n* IKS on Vmware on IBM Cloud Baremetal 와 같은 상품도 출시\n\n\n대부분 VMware 협력을 기반으로 하이브리드 클라우드 전략을 전개하고 있고 내면을 들여다보면 물리적으로 다른곳에 있는 Overlay 네트워크에 대한 해결이 가장 문제이기 때문에 Direct-link나 VPN을 통해 연결하는것도 결국 클라우드 사업자가 네트워크 트래픽 비용을 가져가지 위한 전략으로서 보인다. \n\n실제 퍼블릭 클라우드 벤더의 Cash Cow는 Compute 자원보다는 네트워크 비용과 Blob Storage, API과금 등 에 대한 매출이 큰 비중을 차지한다고 봐야 한다. 혹자는 글로벌 네트워크 전용선에 대한 투자나 고성능 HPC도입 투자 비용에 대한 말들을 하지만 IBM같은 경우 글로벌 네트워크 무료 정책을 통해 Market Share를 획득하고자 하는 부분들을 보면 네트워크가 가장 핵심이 아닐까 싶다.  \n\n클라우드를 이야기 할때 기술 성숙도를 보면 Compute > Storage > Network 순으로 전이가 되는데 OpenStack이나 Kubernetes 프로젝트와 같은 오픈소스나 VMware NSX 플랫폼만 봐도 정말 네트워크가 중요한 영역이 되고 있음을 알수 있다. \n\n결국 핵심적으로 봐야하는 부분은 네트워크다. 네트워크 부분에서 아주 강력한 솔루션이 나와야 한다. 결국 회선 비용이 문제고 최근 넷플릭스와 SKB의 속도 분쟁만 봐도 결국 강자의 논리로 네트워크 문제가 해결되는 시대가 온 것이다.\n\n### 정리\n\n내가 생각한 서비스 사용자 측면에서 우리가 고려해야할 클라우드 옵션들을 정리해보면 아래와 같다.\n\n* 멀티 클라우드 : 퍼블릭 클라우드 to 퍼블릭 클라우드 운영 솔루션으로 프라이빗 클라우드 영역에는 거의 관여를 안하며, 솔루션에 따라 일부 가상화된 Computing 자원까지 모니터링 지원하기도 함. 멀티 클라우드 운영 플랫폼은 주로 클라우드 Brokerage Service 형태로 제공되어야 하므로 MSP역할이 중요해질 것 같다. 메가존, 베스핀글로벌과 같은곳 뿐아니라 자체적인 클라우드를 운영하는 대형 SI회사를 포함한 기존 IaaS 운영조직의 변화들도 눈여겨 봐야할것 같다. 마진율이 낮은 레드오션 싸움에서 누가 이기느냐가 중요하기 보다는 미래를 보고 투자하는 자세가 필요하다고 본다. \n\n* 프라이빗 클라우드 : 프라이빗 클라우드 전용 솔루션으로 벤더 또는 특정 IaaS 서비스에 최적화된 경우가 많으므로 VMware, OpenShift, OpenStack Managed 서비스등 기존 제품에서의 연장선에서 얼마나 오픈소스를 이해하고 프로덕션에 적용할수 있는 역량을 기르는것이 중요하다. \n\n* 하이브리드 클라우드 : 퍼블릭 클라우드 to 프라이빗 클라우드 운영 솔루션으로 프라이빗 클라우드의 인프라스트럭처까지 관여를 하며, 퍼블릭 클라우드 영역은 일반적으로 IaaS 영역과 일부 PaaS / 컨테이너까지 구성, 모니터링 지원해야 하고 이를 위한 3rd Party 솔루션들이 등장하고 있고 점차 영역을 확대할것이다. 이에 있어 컨테이너 플랫폼을 도입하는것이 가장 글로벌 플레이어와 격차를 줄이는데 효과적인 도구라 생각한다. 결국 하이브리드 클라우드 구현은 기존 Public과 Local Cloud간 네트워크 연계가 중요한 포인트이다.\n\n* 간단히 구현해 볼수 있는 General Hybrid Kubernetes Architecture를 간단히 그려봤다. 기본적으로 StrongSwan VPN을 가지고 CI/CD를 고려한 DevOps관점에서의 구성도이다.\n![hybrid-cloud](/img/hybrid-cloud.png)\n\n위에 내용과 함께 하이브리드 클라우드를 고려할때 `데이터 소유권 측면에서의 거버넌스` 와 `overlay 네트워크`에 대한 고민이 동행되었을때 성공적인 하이브리드 클라우드를 구축할수 있다라는 나 혼자만의 생각을 정리해봤다."
    },
    {
      "id": "job/SRE/",
      "metadata": {
        "permalink": "/job/SRE/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-01-30-SRE.md",
        "source": "@site/blog/2019-01-30-SRE.md",
        "title": "SRE (Site Reliablity Engineering)",
        "description": "SRE 소개",
        "date": "2019-01-30T00:00:00.000Z",
        "formattedDate": "January 30, 2019",
        "tags": [
          {
            "label": "SRE",
            "permalink": "/tags/sre"
          },
          {
            "label": "DevOps",
            "permalink": "/tags/dev-ops"
          }
        ],
        "readingTime": 12.76,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "SRE (Site Reliablity Engineering)",
          "comments": true,
          "classes": "wide",
          "description": "SRE 소개",
          "slug": "job/SRE/",
          "date": "2019-01-30T00:00:00.000Z",
          "categories": [
            "Job"
          ],
          "tags": [
            "SRE",
            "DevOps"
          ]
        },
        "prevItem": {
          "title": "Hybrid Cloud",
          "permalink": "/cloud/Hybrid/"
        },
        "nextItem": {
          "title": "2018 Retrospective",
          "permalink": "/retrospective/newyear-plan/"
        }
      },
      "content": "그동안 DevOps 담당자라고 부르짖던 사람들의 Role이 인프라 담당자인지 플랫폼 담당자인지 아니면 개발하는 운영자인지 헷갈릴때가 많았다.\n\n갑자기 국내에서도 SRE 채용 공고가 많아지는것을 보면서 내 자신도 한번 정리를 하고 가야할것 같은 생각이 들었다.\n\nhttps://docs.microsoft.com/ko-kr/learn/modules/intro-to-site-reliability-engineering/\n\n개념정리 측면에서 위 MS Azure의 온라인 교육내용을 정리해봤다.\n\n## SRE\n\nSRE(사이트 안정성 엔지니어링)란 조직이 해당 시스템, 서비스 및 제품에서 적절한 수준의 안정성을 달성하도록 지원하는 엔지니어링 분야이다. \n\n출처 : https://landing.google.com/sre/sre-book/chapters/introduction/\n\n기본적으로 운영경험이 있는 sysadmin, IT전문가, DevOps실행 담당자 등이 대상이 될 수 있다.\n\n\n### 적절한 수준의 안정성\n가장 핵심이 되는 단어는 `안정성`이다. 조직에서 만든 애플리케이션이 안정성이 부족하여 작동하지 않거나 불안정한 경우 실제 비즈니스에 피해를 줄 수 있다는건 누구나 알고 있는 당연한 이야기이지만 100%의 안정적인 시스템은 존재하지 않으므로 적절한 수준(정량적으로 측정하긴 어렵지만 이해관계자들이 납들할만한 수준)의 안정성을 달성하기 위한 일련의 작업들을 수행하는 것이 SRE의 기본 속성이라고 볼수 있다. \n\nSRE는 Google에서 2003년부터 7명의 소프트웨어 엔지니어로 구성된 팀에서 시작된 이후 (자세한 내용은 위 링크 ebook 참조) Google 사내에 널리 확산되고 내부적인 문화로 조성되는 중에 밖에서는 DevOps라고 하는 문화가 동시에 확산되었다 한다. 결국 동일한 문제를 해결하기 위한 측면에서 두개의 방법론은 다르다고 봐야한다. \n\n\n### 차이점\n* SRE - 안정성을 위한 엔지니어링, DevOps - 개발과 운영조직 각각의 사일로를 해체하기 위한 문화적인 움직임\n* SRE - 저는 SRE 입니다, DevOps - 저는 DevOps를 하는 운영자 또는 개발자 입니다.\n* SRE - 규범으로 인식, DevOps - 문화로 인식\n  \n### 공통점\n* 모니터링/식별 가능, 자동화\n\n\n## 주요 SRE원칙 : 선순환\n새로운 서비스의 상태의 지표는 어떻게 정의할까? 보통은 무엇을 `SLI(서비스 수준 지표)`로 사용할지를 정하는것으로 본다. 일반적으로 성공 및 실패 측정값(200 OK,500 Error), 응답시간(ms), 처리량 등의 조합으로 결정될 수 있다. 보통  카나리아 분석을 통해 Scoring을 한 `SLI`를 200 OK와 500 또는 에러코드의 비율로 결정하는 방법을 사용하는 경우가 많다.\n\n서비스 상태를 식별하는 지표를 정하고 나면 고객이나 우리가 원하는 안정성 수준을 결정해야 한다. 개발팀과 운영팀이 같이 만든 원하는 안전성 수준을 `SLO(서비스 수준 목표)`라고 말하면 된다. \n\n`SLO`는 Prometheus나 Datadog과 같은 모니터링 시스템을 활용하여 정확하게 측정하고 표시해야 한다. 서비스의 안정성에 대해서 명확하게 이해할수 있는 목표이기도 하다. 반드시 `SLO`를 만족하는지 못하는지 명확한 측정값으로 데이터가 존재해야 하고 `SLO`를 충족하지 못하면 문제가 있는것으로 판단하고 문제를 해결해야 한다. \n\n### 오류 예산\n오류 예산이라는 말을 명확히 이해하려면 흔히 고객과 체결하는 계약서 상의 명시된 서비스 가동율(SLA)을 생각하면 될 것 같다. 보통 99.9% 의 `SLO`를 정한다고 한다면 1년에 8시간 45분 57초의 서비스 다운타임을 허용하는 것이다. 오류 예산은 서비스의 완벽한 안정성과 원하는 안정성의 차이(100%-99.9%=0.01%)를 말한다. 0.01%로 즉 8시간정도의 오류 예산을 가지는 것은 그 시간을 모두 사용할 때까지 추가 릴리스나 패치를 진행할 수 있다는 이야기와 같다. 어떤 팀은 해당 예산을 신규 기능을 릴리스하는데 사용하기도 하고 장애가 발생했을때 문제의 원인을 발견하고 개선하는데 사용하기도 한다. \n\n일반적으로 서비스에 대한 오류 예산이 모두 사용되면 서비스가 원하는 안정성 수준으로 돌아갈때 까지 해당 서비스의 추가적인 릴리스를 보류하는게 일반적이긴 하지만 특정기간(월 또는 분기 또는 연간) 기준으로 계산되기 때문에 서비스 안정성이 정상 상태라면 오류 예산은 다시 주어지게 되므로 `선순환` 구조를 가진다고 볼 수 있다.\n\n### 비난 없는 장애 회고\n일반적으로 비즈니스에 크리티컬한 장애가 발생했을때 사후 분석(회고)을 하게 되는데 이때는 `비난 없는` 분석을 해야한다. 오래전 기억이지만 예전 직장 특정팀에서는 장애가 발생하면 유관자들이 모두 엔지니어 책상 뒤에 서서 모든 화살과 눈총을 주기 때문에 도저히 일을 할 수 없게 만든 경우가 많았었다. \n\n특정 운영자의 작업 실수로 인해 장애가 발생했다 하더라도 해당 이벤트가 발생하게 된 프로세스나 기술기반에 실패 원인을 파악하는데 중점을 두어야 할것이다.\n\n시스템의 안정성을 저하시키고 서비스를 중단시킨 작업(릴리스, 패치 등)을 사유로 개인에게 페널티를 주거나 고과에 반영하는 방법은 장애로 부터 교훈을 얻을수가 없고 해당 담당자의 충성도나 생산성 저하를 유발하는 안좋은 장치가 될 수 있다. 내 경험이기도 하지만 한동안 두려워서 아무 변경도 하지 못하는 경우가 종종 있었다. \n\n회사나 조직, 팀은 시스템 중단으로부터 교훈을 얻고 지속적으로 시스템을 개선할수 있다. 적절한 분석을 통해 후속조치를 수행함으로써 실패를 수용하는 것이 SRE의 핵심 원칙이다. 이러한 내용을 모두에게 공유하고 타 조직이나 팀에게 인사이트를 제공하는 선순환 구조를 만드는것 또한 SRE의 기본 원칙이라고 볼 수 있다.\n\n## 주요 SRE원칙 : 사람측면\n### Toil(수고스런 일, 번거로운 작업)\n항상 SRE에서는 Toil에 초점을 맞춰야 한다. Toil은 사람이 수행하는 운영 작업 중 특정한 패턴이나 특성이 있는 작업을 말한다. 종종 반복적이고 자동화 될 수 있는 작업일지라도 대부분 수동작업으로 처리를 한다. 서비스의 규모가 커지거나 고객이 많아지게 되면 보통 우리가 이야기 하는 티켓(또는 SR)의 수가 많아지고 공수가 많이 필요하게 된다. \n\n배포때마다 Config를 적용하거나 계정 등록, 볼륨이나 디스크 프로비저닝 등 수동으로 처리해야하는 모든 작업이 운영자에게는 부하로 다가올수 있다. 이러한 반복적인 작업을 티켓시스템을 통해 처리를 하게 되면 티켓 생성, 작업계획서 작성, 검토, 승인 단계를 그때 마다 해야하는 아주 번거로운 작업(Toil)을 해야한다.\n\nSRE는 최대한 이러한 반복적이고 번거로운 작업을 제거하기 위해 노력해야하고 자동화와 Self-Service로 개발자가 편하게 직접 사용할수 있도록 환경을 제공해 주어야 한다. 이러한 요청이나 티켓을 자동으로 처리할 수 있으면 조금 더 생산적인 일을 수행할수 있다. \n\n위에서 이야기한 적절한 안정성과 비슷한 맥락으로 생각하면 될 것 같다. 번거로운 작업을 없애는것이 다른 중요한 작업보다 우선순위가 떨어지기도 하지만 서비스 상에서 Toil을 제거하는것도 SRE의 주된 업무이다.\n\n## 운영작업의 비율\nToil을 제거하거나 서비스/시스템 안정성을 개선하기 위해 SRE는 장애를 해결하고 티켓을 처리하는 시간 비중을 50%를 넘겨서는 안된다. 티켓이 필요하지 않도록 개발자 Self-Service를 구성/제공하고 효율성을 증대시키기 위해서 IaC등(Shell, Ansible, Terraform)과 같이 자동화를 위한 코드를 만드는데에도 집중을 해야한다.\n\n## 정리\n지금까지 SRE에 대해서 간단하게 소개하고 이해하는 측면에서 정리해 봤다. \n\n[Service Reliability Hierarchy](https://landing.google.com/sre/sre-book/chapters/part3/)\n\n\n![hierarchy](https://lh3.googleusercontent.com/3gX2qgys2I-9HnEIvXUA10ed3AILvg5MclnKWBquEkJKP3g5_kD6WR7Ptwp3TwAGla1DuSmHv64MdTtACNLlArFVq7BwbTrTVhigsA=s0)\n\n그림과 같이 서비스 안정성 계층 구조에서는 기본적으로 모든 서비스를 모니터링 하는 시스템을 갖추어야 한다고 말한다. 모니터링 시스템이 준비가 되면 서비스에 대한 SLI, SLO를 만들고 모니터링 시스템에서 구현하는것이 첫번째 SRE를 적용하는 방법이라고 할 수 있을 것이다.\n\n## 참고 서적\n* [Site Reliability Engineering: How Google Runs Production Systems(“The SRE Book”)](http://shop.oreilly.com/product/0636920041528.do)\n* [The Site Reliability Workbook: Practical Ways to Implement SRE(“The SRE Workbook”)](http://landing.google.com/sre/workbook/toc/)\n* [Seeking SRE: Conversations About Running Production Systems at Scale](http://shop.oreilly.com/product/0636920063964.do)"
    },
    {
      "id": "retrospective/newyear-plan/",
      "metadata": {
        "permalink": "/retrospective/newyear-plan/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2019-01-03-newyear-plan.md",
        "source": "@site/blog/2019-01-03-newyear-plan.md",
        "title": "2018 Retrospective",
        "description": "18년도를 돌아보며...",
        "date": "2019-01-03T00:00:00.000Z",
        "formattedDate": "January 3, 2019",
        "tags": [
          {
            "label": "Retrospective",
            "permalink": "/tags/retrospective"
          },
          {
            "label": "2018",
            "permalink": "/tags/2018"
          },
          {
            "label": "Planning",
            "permalink": "/tags/planning"
          },
          {
            "label": "Change",
            "permalink": "/tags/change"
          }
        ],
        "readingTime": 5.32,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "2018 Retrospective",
          "comments": true,
          "classes": "wide",
          "description": "18년도를 돌아보며...",
          "slug": "retrospective/newyear-plan/",
          "date": "2019-01-03T00:00:00.000Z",
          "categories": [
            "Retrospective"
          ],
          "tags": [
            "Retrospective",
            "2018",
            "Planning",
            "Change"
          ]
        },
        "prevItem": {
          "title": "SRE (Site Reliablity Engineering)",
          "permalink": "/job/SRE/"
        },
        "nextItem": {
          "title": "Cloud Enginner",
          "permalink": "/cloud/cloud/"
        }
      },
      "content": "한해가 또 훅 가버렸다. 18년도 Retrospective 시작한다.\n\n* Work\n  * 17년까지 팀 막내였지만 새로운 부서로 이동하여 서비스개발 파트장을 맞게 되었음. 17년도에 개발이라고는 python를 사용하여 vmware vm을 openstack instance로 migration하는 프로젝트 및 kubernetes 기반 tensorflow 플랫폼을 개발하는 초보개발자 수준이였던 내가 인프라와 클라우드 플랫폼(openstack, vsphere, kubernetes) 경력이 인정되어 서비스 개발 파트를 맡게 되었다. 인생에서 가장 큰 변혁중 하나였다고 생각하고 항상 감사하는 마음으로 일을 하고 있다.  \n    \n  * CNCF OpenSource들을 활용하여 FaaS(Function as a Service), IaC(Infra as Code) 플랫폼 개발을 진행하였다. 서비스 기획 및 두개 프로젝트의 Scrum Master 역할을 수행하였고 메인 업무는 SRE역할로 Kubernetes 설계 구축, 클러스터 및 스토리지 관리, LB, 인증서 등 클라우드 인프라 자원관리 정도였던것 같다. \n\n* Tech.\n  * Public Cloud : AWS, DigitalOcean, IBM Cloud, GCP, Azure, nCloud\n  * Provisioning : Harbor, Ansible, Portus, Clair, Kubicorn, Terraform, Vault\n  * Runtime : Rook, Minio, OpenEBS, Calico, REX-ray, WeaveNet, Cilium\n  * Orchestration : Kubernetes, Envoy, etcd, gRPC, Kong, HAProxy, Istio, Kong, NGINX, Traefik\n  * App : NATS, Helm, Kafka, Docker-compose, Draft, Gitlab, minikube, Operator, Jenkins, JenkisX, Packer, RabbitMQ, Spinnaker\n  * Serverless : Dispatch, Fission, Knative, Nuclio, OpenWhisk, OpenFaas\n  * Monitoring, Logging : Prometheus, Fluentd, Elastic, Grafana, influx, WeaveScope\n  * 기타 : Cert-Manager, Dex, Agones\n\n* Speak\n  쓰다보니 사내외 발표를 10번을 넘게 한거 같다.\n  * 4월 사내 Cloud Tech : 컨테이너 기반 SaaS 개발을 위한 고려사항\n  * 5월 사내 DT 솔루션 행사, 클라우드사업부 세션 : Advanced Cloud (Road to Serverless)\n  * 5월 사내 통신사업부문 Tech세션 : Road to Serverless (Functions as Applications)\n  * 6월 Open Infra Days Korea 2018 : [Provisioning Dedicated Game Server on Kubernetes Cluster](https://www.slideshare.net/JinwoongKim8/provisioning-dedicated-game-server-on-kubernetes-cluster)\n  * 8월 SK DNA 2018 : [Cloud Z 의 오픈소스 서비스 소개 및 Serverless로 게임 개발하기](https://www.slideshare.net/JinwoongKim8/cloud-z-serverless-118143924)\n  * 10월 DT Labs Meetup : [Continuous Delivery with Spinnaker on K8s(kubernetes) Cluster](https://www.slideshare.net/JinwoongKim8/continuous-delivery-with-spinnaker-on-k8skubernetes-cluster-118140930)\n  * 10월 NCsoft IO : [Cloud Native 오픈소스 서비스 소개 및 Serverless로 실제 게임 개발하기](https://www.slideshare.net/JinwoongKim8/cloud-native-serverless/JinwoongKim8/cloud-native-serverless)\n  * 11월 IBM Developer Day : [Continuous Delivery using Spinnaker on Kubernetes Multi Cluster](http://public.dhe.ibm.com/software/kr/TrackB/B3.pdf)\n  * 11월 \"Kubernetes Meetup\" 1Day : [Spinnaker on Kubernetes](https://www.slideshare.net/JinwoongKim8/spinnaker-on-kubernetes-123752186)\n\n\n쓰고나서 보니 2018년도 회고라기 보다 거의 개발에 필요한 잡일(DevOps라고 두둔하면서)과 발표만 한거 같네...\n19년도 부터는 다른업무를 하게 될거 같은데 초심을 잃지 말아야 하겠다.\n\n최근 화두가 되었던 `노력중독` 이였던것 같은 18년은 털어버리고\n내가 진짜로 좋아하고 하고 싶은 공부와 업무, 그리고 가족을 위해서 로드밸런싱 하면서 지내야 하겠다. \n\n2006년에 쓴 미래의 내 모습과 어느정도 일치하는거 같지만 목표를 좀더 높이 잡아야할듯 하다.\n\n번역에 참여하거나 책을 써본다던지 커뮤니티 활동을 좀더 적극적으로 해보고\n회사에서 시키는 밋업이 아닌 자발적으로 참여하는 외부 강연이나 밋업을 위주로 해봐야겠다. \n\n또한 점점 개발영역과 멀어지는거 같은 내모습이 나이가 들면서 더 심화될거 같아서 지인들과 별도의 개발 프로젝트를 진행해보려고 한다.\n\n내 13년간 직장 생활중 가장 바쁘고 다이내믹하면서 바빴던 한해이자 많은 경험과 공부가 되었던 일년이였던것 같다.\n\n나를 땡겨주셨던 `June`, 항상 분위기를 리드했던 `Joonbum`, 언제나 열정가득하고 최고의 DevOps 플레이어 `Jeongho`, 든든한 Backender `Donghun`, Excellent/Free/Kind(a.k.a EFK)한 `Minyoung`, 내가 인정하는 Frontier `Sanghun`, Best Rookie `Minsoo`, sorry because the hy `HyunSang`, Moontoring `Jinsoo`, Forever Mentor `Jaemoon`, Guru of CNI `Youngchae` 모두에게 감사의 말씀을 전하고 싶다. (무슨 수상소감도 아니고 ㅋㅋ)\n\n19년에도 남들에게 존경을 받는 사람보다는 무슨일을 하든지 대중에게 욕먹지 않고 필요할때 도움을 요청을 할수 있는 그런 편한 사람이 되어야 겠다."
    },
    {
      "id": "cloud/cloud/",
      "metadata": {
        "permalink": "/cloud/cloud/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-12-25-cloud.md",
        "source": "@site/blog/2018-12-25-cloud.md",
        "title": "Cloud Enginner",
        "description": "Cloud Enginner",
        "date": "2018-12-25T00:00:00.000Z",
        "formattedDate": "December 25, 2018",
        "tags": [
          {
            "label": "Cloud",
            "permalink": "/tags/cloud"
          },
          {
            "label": "Enginner",
            "permalink": "/tags/enginner"
          }
        ],
        "readingTime": 4.055,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Cloud Enginner",
          "comments": true,
          "classes": "wide",
          "description": "Cloud Enginner",
          "slug": "cloud/cloud/",
          "date": "2018-12-25T00:00:00.000Z",
          "categories": [
            "Cloud"
          ],
          "tags": [
            "Cloud",
            "Enginner"
          ]
        },
        "prevItem": {
          "title": "2018 Retrospective",
          "permalink": "/retrospective/newyear-plan/"
        },
        "nextItem": {
          "title": "Spinnaker on Kubernetes #3 (Kayenta)",
          "permalink": "/kubernetes/spinnaker-advanced-3/"
        }
      },
      "content": "오늘은 쓸데없는 이야기를 주저리 적어본다. \n\n페친한분이 말씀하신 구절이 떠오른다.\n`\"엔지니어라고 해서 정치력이 필요없는것이 아니다. 오히려 혁신적인 기술은 정치력, 권력이 없으면 적용될 수 없다\"` - 제프리 페퍼교수의 \"권력의 기술\"\n\n나이 40을 넘긴 엔지니어로서 나는 어떤 위치인가?\n\nCloud 관련 오픈소스로 비즈니스를 진행하면서 현 직장에서 2년을 보냈다.\n보람도 있었고 실패도 있었다.  \n올해는 기획한 오픈소스 기반 프로젝트로 여러사람의 힘을 합쳐서 서비스를 런칭하기도 하였지만 몇년간 글로벌 플레이어들을 보면서 느낀점은  \n\n회사가 글로벌 플레이어가 되기위해 노력하는 방법은 다양하지만  \n결국 이윤을 목적으로 회사에서 허용하고 투자하는것이고  \n그걸로 돈을 못벌면 바로 쳐낼수 있다는 경영층의 생각은 변함이 없을것이다. \n\n이런 마음가짐을 변화시키지 않는다면 훌륭한 프로젝트라도 성공이든 실패든 끝까지 가는것은 어렵다고 생각한다. \n\n처음 클라우드 관련 업무를 시작할때를 생각해본다.\n\n* 2010년  \n팀장: 너 클라우드 할래?  \n나 : 무슨일하는데요?  \n팀장: 가상화 프로젝트하는데 같이 한번 비즈니스 만들어보자, 너가 하고 싶은대로 기획하고 해봐~  \n나 : 네^^ 열심히 하겠습니다.  \n\n* `개인 성과 및 팀의 목표는 어느정도 달성하였으나 ROI 달성은 쉽지 않았음`\n\n* 2016년  \n팀장: 너 주간보고 똑바로 안해?, 지난번 장애보고는 어떻게 된거야?  \n나: ㅠㅠ  \n팀장: 우리 팀 수치가 이게 뭐냐... 좀 잘좀 해봐  \n나: ㅠㅠ  \n  \n* `결국 퇴사`\n  \n* 2017년  \n팀장: 하고 싶은대로 해봐\n나: 진짜요? 뚝딱뚝딱  \n회사: 수익모델이 뭐지?  \n나: ...  \n\n* `출시하기도 전에 서비스 종료`\n\n* 2018년  \n팀장: 하고 싶은거 해봐요. 대신 돈벌어와야해  \n나: 진짜요? 뚝딱뚝딱  \n회사: 흠 좀 부족하지만 앞으로 좀더 잘해주세요. 레퍼런스 확보에 힘써주세요.  \n나: 서비스 기획자가 필요합니다.  \n회사: ... 본인이 해야하는거 아닌가요?  \n나: ...  \n  \n* `기획을 해야하나 아직 기술력도 부족한데...`  \n\n벌써 직장생활 13년차에 마흔살이 넘어가고 있다. \n\n엔지니어로 살아온날보다 살아가야할날이 적게 남았고 얼마 남지 않았다는 생각이 드는 우울한 하루다. \n10년간 공부해왔던 것들보다 2년동안 공부한게 더 파격적이고 다양하고 복잡했다.\n\n여러모로 아쉬운 한해였다...\n\n앞으로 내가 부족한 부분을 기술력으로 메꿀것이냐? 협상과 정치력으로 메꿀것이냐?\n\n잠이 안오는 12월이다."
    },
    {
      "id": "kubernetes/spinnaker-advanced-3/",
      "metadata": {
        "permalink": "/kubernetes/spinnaker-advanced-3/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-12-04-spinnaker-advanced-3.md",
        "source": "@site/blog/2018-12-04-spinnaker-advanced-3.md",
        "title": "Spinnaker on Kubernetes #3 (Kayenta)",
        "description": "Spinnaker에 대해 알아봅니다 #3 (Kayenta)",
        "date": "2018-12-04T00:00:00.000Z",
        "formattedDate": "December 4, 2018",
        "tags": [
          {
            "label": "CI/CD",
            "permalink": "/tags/ci-cd"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Spinnaker",
            "permalink": "/tags/spinnaker"
          },
          {
            "label": "Continuous Delivery",
            "permalink": "/tags/continuous-delivery"
          },
          {
            "label": "Continuous Deployment",
            "permalink": "/tags/continuous-deployment"
          },
          {
            "label": "Pipeline",
            "permalink": "/tags/pipeline"
          },
          {
            "label": "Canary",
            "permalink": "/tags/canary"
          },
          {
            "label": "Canary analysis",
            "permalink": "/tags/canary-analysis"
          },
          {
            "label": "Kayenta",
            "permalink": "/tags/kayenta"
          },
          {
            "label": "Automated Canary Service",
            "permalink": "/tags/automated-canary-service"
          }
        ],
        "readingTime": 14.66,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Spinnaker on Kubernetes #3 (Kayenta)",
          "comments": true,
          "classes": "wide",
          "description": "Spinnaker에 대해 알아봅니다 #3 (Kayenta)",
          "slug": "kubernetes/spinnaker-advanced-3/",
          "date": "2018-12-04T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "CI/CD",
            "Kubernetes",
            "Spinnaker",
            "Continuous Delivery",
            "Continuous Deployment",
            "Pipeline",
            "Canary",
            "Canary analysis",
            "Kayenta",
            "Automated Canary Service"
          ]
        },
        "prevItem": {
          "title": "Cloud Enginner",
          "permalink": "/cloud/cloud/"
        },
        "nextItem": {
          "title": "Spinnaker on Kubernetes #2",
          "permalink": "/kubernetes/spinnaker-advanced-2/"
        }
      },
      "content": "3번째 포스팅이다.\n\n11/23 \"Kubernetes Meetup\" 1Day에서 발표한 이야기 연장선으로 작성한다.\n\n고객에게 오퍼링을 위해 준비한 내용과 Kubernetes monitoring과 연계한 내용에 대해서 적어보려고 한다.\n최근 발표를 다니면서 많이 받는 질문이 실제 사용할만 한가?라는 질문과 어떻게 활용해야 하는지에 대한 질문들을 많이 받는다.\n오늘 최근 Spinnaker Summit 2018에서 중요하게 다뤘던 Kayenta 프로젝트를 가지고 이야기 해보려고 한다. \n\n## Kayenta\n\nKayenta는 자동 Canary 분석 오픈소스(Automated Canary Service(ACA))로 Spinnaker의 마이크로서비스중 하나로 동작한다.  \nAutomated Canary Deployments에 사용되고 자세한 내용은 [canary documentation](https://www.spinnaker.io/guides/user/canary/stage/)을 확인하면 된다.  \n\n\n새 종류의 하나인 Canary는 1차 세계대전중에 인간이 해를 입기 전에 독성가스를 탐지하는 용도로 사용되었다고 한다.\nDevOps에서는 CD(Continuous Deployment) 프로세스의 일부로 사용되며 Canary 릴리즈는 새로운 버전의 Software를 Production에 배포하는 위험을 줄여주는 기술이라고 생각하면 된다.\n\nCanary 신규 버전의 Software를 안정적인 기존 버전과 함께 배포하고 특정사용자나 일부 대상에게 트래픽 일부를 흘려 기존 사용자에게 영향을 최소하하고 새로운 버전의 문제를 신속하게 발견하고 이전의 안정된 버전으로 트래픽을 다시 라우팅시키는것이 주요 기능이라고 보면 된다.  \n\n보통 품질 테스트 용도로 현재 운영 버전과 신규 버전의 주요한 지표(주로 Prometheus Metric)를 비교하여 평가를 진행하는데 이를 단위 테스트나 통합 테스트를 대체하여 사용해서는 절대 안된다.  \n위에서 언급하였듯이 예기치 않은 위험을 최소화 하거나 문제를 신속하게 발견하는 것을 주 목적으로 하기 때문이다.\n\nSpinnaker에서는 기본적으로 3가지 Cluster(Logical Application Group)를 사용한다.\n\n* Production Cluster - 현재 실행중인 운영 버전으로 Replica는 임의로 변경할 수 있다. \n* Baseline Cluster - Production Cluster와 동일한 버전으로 실행됨\n* Canary Cluster - 변경된 신규 버전의 Software로 실행됨\n\n기본적으로 수동으로 진행할 경우에는 로그와 수집된 메트릭을 분석하고 객관적인 지표로 평가를 진행하는게 기본이다.\n하지만 직접 사람이 하는 일이라 메트릭 데이터를 보다 보면 편견과 착오가 발생할 수 밖에 없다.\n\n그래서 Netflix는 ACA(Automated Canary Service)라고 하는 자동화된 접근 방식을 통해 카나리 분석을 진행하고 있다.\n수동으로 계산된 여러가지 지표를 가중치 기반으로 점수를 내리고 원하는 점수에 도달하면 배포하는 자동화된 방식이다.\n\n## Requirements\n* spinnaker cluster - 1.8+ (1.9.3 이상 추천)\n* halyard - 1.0+\n* kubernetes cluster - 1.9+\n* metric services - datadog, prometheus, stackdriver, signalfx\n* persistent storage - S3(or compatible S3), minio, GCS\n\n## Kayenta Service 추가하기\n이번 포스팅에서는 아래 환경으로 작성하였다.\n* spinnaker cluster - 1.10.5\n* halyard - 1.11\n* kubernetes cluster - 1.9.7\n* metric services - prometheus\n* persistent storage - compatible S3(IBM Object Storage)\n\n일단 기존 halyard config를 백업하자. \n```\n$ hal backup create\n+ Create backup\n  Success\n+ Successfully created a backup at location:\n/Users/ddii/halbackup-Wed_Nov_28_13-35-31_KST_2018.tar\n```\n\n나중에 복구는 아래와 같이 하면 된다. \n```\n$ hal backup restore --backup-path <backup-name>.tar\n```\n\n기존 halyard config를 살펴보면 아래와 같이 canary.enabled=false 로 되어있는 것을 확인 할수 있다.\n```\ncurrentDeployment: default\ndeploymentConfigurations:\n- name: default\n  canary\n    enabled: false\n```\n\ncanary analysis을 활성화 한다.\n\n```\n$ hal config canary enable\n```\n\n그리고 default judgement algorithm은 `NetflixACAJudge-v1.0` 로 되어있다 다른 걸 이용하려면 다음과 같이 설정할수 있다. \n```\n$ hal config canary edit --default-judge CUSTOM_JUDGE\n```\n\n메트릭 소스로 prometheus를 설정한다. 물론 기존에 사용중인 prometheus endpoint url이 필요하다.\n```\nhal config canary prometheus enable\nhal config canary prometheus account add my-prometheus --base-url http://YOUR_PROMETHEUS_SERVER:PORT\n```\n\n여기서는 IBM Cloud Object Storage(S3 Compatible)을 사용하였지만 aws로 설정한다.\n```\n$ hal config canary aws enable\n$ hal config canary aws account add my-s3 --bucket spin-bucket --endpoint \\\n    s3.seo-ap-geo.objectstorage.service.networklayer.com --access-key-id ACCESS_ID \\\n    --secret-access-key\n$ hal config canary aws edit --s3-enabled=true\n```\n\n여러개의 메트릭을 동시에 설정 및 수집이 가능하므로 그중 prometheus 및 관련 account를 기본으로 설정한다.\n```\n$ hal config canary edit --default-metrics-store prometheus\n$ hal config canary edit --default-metrics-account my-prometheus\n$ hal config canary edit --default-storage-account my-s3\n```\n\n모든 spinnaker cluster가 준비된 상태의 컨피그는 아래와 같다. \n```\ncurrentDeployment: default\ndeploymentConfigurations:\n- name: default\n  canary:\n    enabled: true\n    serviceIntegrations:\n    - name: google\n      enabled: false\n      accounts: []\n      gcsEnabled: false\n      stackdriverEnabled: false\n    - name: prometheus\n      enabled: true\n      accounts:\n      - name: my-prometheus\n        endpoint:\n          baseUrl: http://YOUR_PROMETHEUS_SERVER:PORT\n        supportedTypes:\n        - METRICS_STORE\n    - name: datadog\n      enabled: false\n      accounts: []\n    - name: aws\n      enabled: true\n      accounts:\n      - name: my-s3\n        bucket: spin-bucket\n        rootFolder: kayenta\n        endpoint: s3.seo-ap-geo.objectstorage.service.networklayer.com\n        accessKeyId: ACCESS_ID\n        secretAccessKey: ACCESS_KEY\n        supportedTypes:\n        - OBJECT_STORE\n        - CONFIGURATION_STORE\n      s3Enabled: true\n    reduxLoggerEnabled: true\n    defaultMetricsAccount: my-prometheus\n    defaultStorageAccount: my-s3\n    defaultJudge: NetflixACAJudge-v1.0\n    defaultMetricsStore: prometheus\n    stagesEnabled: true\n    templatesEnabled: true\n    showAllConfigsEnabled: true\n```\n\nSpinnaker Cluster를 재배포하게 되면 아래와 같이 spin-kayenta deployments가 추가된 것을 확인할 수 있다.\n```\n$ hal deploy apply\n\n$ kubectl get pod\nNAME                                READY     STATUS    RESTARTS   AGE\nspin-clouddriver-555cfc9765-kvnl8   1/1       Running   0          6d\nspin-deck-85845b5b48-49ncm          1/1       Running   0          6d\nspin-echo-5f9dd4d8ff-mvt7g          1/1       Running   0          6d\nspin-fiat-5b945645d8-s2qcq          1/1       Running   0          6d\nspin-front50-5c57fcf587-tqz28       1/1       Running   0          6d\nspin-gate-57576b8c45-w5v6r          1/1       Running   0          6d\nspin-kayenta-6dcd7767d6-rgb9w       1/1       Running   0          6d\nspin-orca-788df6b9cc-tk6lk          1/1       Running   0          6d\nspin-redis-6c87c456fc-6qbl2         1/1       Running   0          6d\nspin-rosco-f6f845d49-btjnd          1/1       Running   0          6d\n```\n\n이후 Dashboard에 접속하고 application중 하나의 Config에 들어가면 Features에 Canary메뉴가 생긴것을 확인할 수 있다. 사용설정하고 캐싱하는데 시간이 다소 필요하고 Tasks 메뉴에서 해당 job에 대한 내용을 확인할수 있다. \n\n![canary](/img/canary_config.png)\n\n이후 application delivery 메뉴를 보면 pipelines, canary configs, canary reports라는 메뉴가 생기게 된다. \n\n![delivery](/img/delivery.png)\n\n## simple deploy pipeline 추가\n\n<https://cloud.google.com/solutions/automated-canary-analysis-kubernetes-engine-spinnaker> 가이드 처럼 구성해봐도 되나 `stackdriver`를 써야하고 `prometheus metric`을 활용한 가이드가 필요해서 적어보고자 한다.\n\n새로 파이프라인을 추가한 다음 \n\n![newpipe](/img/newpipe.png)\n\nPipeline Actions - Edit Pipeline JSON 에서 \n<https://raw.githubusercontent.com/ddiiwoong/canary-demo-spinnaker/master/simple-deploy.json> 을 추가해준다.  \n\n해당 json pipeline을 추가하고 나면 다음과 같은 화면을 확인할수 있다. \n![pipe1](/img/samplepipe1.png)\n![pipe2](/img/samplepipe2.png)\n\n반드시 Deploy Config, Deploy Stage에서 배포할 Account 지정을 해야한다. \n\n`sampleapp image - ddiiwoong/canary-demo-spinnaker:latest`\n\n해당 pipeline내의 sampleapp은 python flask 기반으로 구성되어 간단히 internal 500 error를 원하는 비율을 configmap 변수로 구현할 수 있다. [prometheus python client](https://github.com/prometheus/client_python#counter)를 사용하여 Gauge, Counter, Metric 서버를 간단하게 구성을 해보았다. 그리고 코드내에서 500 error rate를 구한 이유는 18년 11월 기준 spinnaker kayenta 버전에서는 PromQL(rate,irate와 같은 함수) 지원이 되지 않는다. 개발중인 코드에 포함이 된것을 확인하였고 12월 kubecon때 정식 릴리즈에 포함될거라 생각한다. \n\n```\n#!/usr/bin/env python\n\nfrom random import randrange\nfrom flask import Flask\nfrom prometheus_client import start_http_server, Gauge, Counter\nimport os\n\napp = Flask('kayenta-tester')\nc = Counter('requests', 'Number of requests served, by http code', ['http_code'])\ng = Gauge('rate_requests', 'Rate of success requests')\n\nresponce_500 = 0\nresponce_200 = 0\nrate_responce = 0\n\n@app.route('/')\ndef hello():\n    global responce_500\n    global responce_200\n    global rate_responce\n    if randrange(1, 100) > int(os.environ['SUCCESS_RATE']):\n        c.labels(http_code='500').inc()\n        responce_500 = responce_500 + 1\n        rate_responce = responce_500 / (responce_500+responce_200) * 100\n        g.set(rate_responce)\n        return \"Internal Server Error\\n\", 500\n    else:\n        c.labels(http_code = '200').inc()\n        responce_200 = responce_200 + 1\n        rate_responce = responce_500 / (responce_500+responce_200) * 100\n        g.set(rate_responce)\n        return \"Hello World!\\n\"\n\nstart_http_server(8000)\napp.run(host = '0.0.0.0', port = 8080)\n\n```\n\n해당앱을 Start Manual Execuction을 통해 배포한다. Comfirm Execution창에서 SUCCESS_RATE를 원하는 값(예:70%)으로 선택하고 배포를 하고 나면 Infrastructure - Clusters 메뉴에서 해당 샘플앱을 확인할 수 있다. \n\n![manaul deploy](/img/manual.png)\n![success rate](/img/successrate.png)\n![manaul deploy2](/img/manual2.png)\n\n실제 해당 서비스를 접속해보면 위에 설정한 SUCCESS_RATE 비율로 200화면과 500에러를 확인할 수 있다. \n\n![flaskweb](/img/flaskweb.png)\n![flaskweb2](/img/flaskweb2.png)\n\n해당 메트릭의 통계를 확인하기 위해 curl을 반복적으로 실행하는 injection container 를 실행한다.\n\n```\nkubectl -n default run injector --image=alpine -- \\\n    /bin/sh -c \"apk add --no-cache --yes curl; \\\n    while true; do curl -sS --max-time 3 \\\n    http://sampleapp:8080/; done\"\n```\n\n5분정도 후에 Prometheus로 접속하여 코드내 작성한 rate_requests 메트릭을 확인해본다.  \nPromQL은 아래 쿼리를 실행하였다.  \n```\nrate_requests{app=\"sampleapp\",version=\"prod\"}\n```\n\n아래 그림과 같이 4개의 pod에서 70% 정도 200 OK, 30% 정도 500 Error가 발생하는 것을 확인할 수 있다. \n\n![500rate](/img/500rate.png)\n\n이 메트릭을 Spinnaker 에서 확인하기 위해 Canary Pipeline을 만들자.\n\n<https://raw.githubusercontent.com/ddiiwoong/canary-demo-spinnaker/master/automated-canary.json>를 JSON으로 Pipeline을 생성한다. \n\n![canary_auto](/img/canary_auto.png)\n\nStage별로 살펴 보기전에 \n\n* Prerequisite  \n  Canary Config 구성이 먼저 필요하다. Delivery - Canary Configs 메뉴에서 신규 컨피그를 작성한다. \n     - Configuration Name - `kayenta-test`\n     - Filter Templates 메뉴를 먼저 생성한다. Canary, Baseline구분을 위해 version 정보를 선택하였다. \n     - Metrics - Add Metric 은 분석을 위한 Prometheus Metric을 설정하는 단계로 `error_rate`가 증가(increase)하면 Pipeline을 중단시키고 Metric은 앞에서 확인한 `rate_requests`를 지정한다. Filter Template은 위에서 지정한 version을 선택한다. ![metric config](/img/metric_config.png)\n     - SCORING - 어짜피 예제는 한가지 Metric분석으로 0점 아니면 100점으로 나올것이므로 Maginal 75, Pass 95를 설정한다.\n\n1. 1st Stage\n    * Configuration - Pipeline 실행시 초기 입력값(0-100, 10단위)으로 설정가능한 successRate 라는 Parameter를 설정한다. \n2. 2nd Stage\n    * Find Baseline - 위에서 작성한 기본 Deploy Pipeline이 선택되었는지와 확인한다.\n    * Deploy Canary Config - 앞에서 선택한 새로운 Parameter(successRate)를 신규 배포할 Canary Pod ConfigMap으로 설정하는 단계이다.\n3. 3rd Stage\n    * Deploy Canary - yaml manifest로 Canary 버전을 배포한다. Replicas는 1로 설정하였고 배포될 Account(K8s Cluster)를 지정한다. \n    * Deploy Baseline - yaml manifest로 Baseline 버전을 배포한다. 위와 동일하게 Replicas는 1로 설정하였고 배포될 Account(K8s Cluster)를 지정한다. \n4. 4th Stage\n    * Canary Analysis - 중요한 Canary 분석 단계로 아래와 같이 설정을 확인한다. Prerequisite에서 설정한 Config(`kayenta-test`)를 선택하고 짧은 분석을 위해 1분(60초) 간격으로 3번 수행을 하도록 한다.   Filter Template에서 지정한 version(`version=\"${scope}\"`) 분석을 위해 Baseline, Canary 설정을 하고 Location은 Namespaces로 생각하면 된다. ![aca config](/img/aca_config.png)\n5. 5th Stage\n    * Deploy to Production - Canary 분석이 통과하였을 경우 운영에 배포\n    * Delete Canary, Delete Baseline - 성공이던 실패이던 Canary, Baseline 배포본을 삭제 \n6. 6th Stage\n    * Successful deployment - Canary 분석이 통과하였을 경우 최종 완료 표기하는 단계\n\n설정이 마무리가 되면 저장을 하고 Canary 분석에 들어간다. \n최초에 `successRate`을 70으로 배포했다면 그 이하로 설정했을 경우에는 아래와 같이 Score 0점으로 배포가 실패하고 Pipeline이 종료된다.   \n\n![fail](/img/canary_fail.png)  \n\n70 이상으로 설정하게 되면 Score 100점으로 정상 배포됨을 확인할 수 있다.\n\n![success](/img/canary_success.png)  \n\n\n## 정리\n간단하게 Spinnaker 와 Prometheus Metric을 활용하여 Kayenta 기반 Canary 배포를 해봤다. 현재 Spinnaker 1.10에서 istio가 지원된다고 하니 다시 한번 확인하고 istio 기반 canary 배포와 함께 사용하는 방법을 더 연구해봐야 할 것 같다.  \n\n올해 AWS re:invent 끝나고 작년보다 큰 현자타임이 왔다. 오픈소스로 먹고사는 사람들의 기분은 다 비슷할거 같다고 생각이 든다. 12월 11일 부터 Kubecon이 열린다고 하니 Kubernetes 관련한 새로운 프로젝트와 기능들에 집중해서 남들보다 한발 나아가야하지 않을까? 오픈소스로 먹고사시는 분들 다들 힘냈으면 좋겠다."
    },
    {
      "id": "kubernetes/spinnaker-advanced-2/",
      "metadata": {
        "permalink": "/kubernetes/spinnaker-advanced-2/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-09-27-spinnaker-advanced-2.md",
        "source": "@site/blog/2018-09-27-spinnaker-advanced-2.md",
        "title": "Spinnaker on Kubernetes #2",
        "description": "Spinnaker에 대해 알아봅니다 #2",
        "date": "2018-09-27T00:00:00.000Z",
        "formattedDate": "September 27, 2018",
        "tags": [
          {
            "label": "CI/CD",
            "permalink": "/tags/ci-cd"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Spinnaker",
            "permalink": "/tags/spinnaker"
          },
          {
            "label": "Continuous Delivery",
            "permalink": "/tags/continuous-delivery"
          },
          {
            "label": "Continuous Deployment",
            "permalink": "/tags/continuous-deployment"
          },
          {
            "label": "Pipeline",
            "permalink": "/tags/pipeline"
          }
        ],
        "readingTime": 7.61,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Spinnaker on Kubernetes #2",
          "comments": true,
          "classes": "wide",
          "description": "Spinnaker에 대해 알아봅니다 #2",
          "slug": "kubernetes/spinnaker-advanced-2/",
          "date": "2018-09-27T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "CI/CD",
            "Kubernetes",
            "Spinnaker",
            "Continuous Delivery",
            "Continuous Deployment",
            "Pipeline"
          ]
        },
        "prevItem": {
          "title": "Spinnaker on Kubernetes #3 (Kayenta)",
          "permalink": "/kubernetes/spinnaker-advanced-3/"
        },
        "nextItem": {
          "title": "knative",
          "permalink": "/kubernetes/knative/"
        }
      },
      "content": "이전 포스팅 [Spinnaker on Kubernetes #1](https://ddii.dev/kubernetes/spinnaker-advanced-1/)에서 검토할때는 많이 개념을 이해하기 어려웠던것 같지만 어느정도 시간이 지났고 또 몇일 후에 발표도 있어서 다른 이야기를 해보고자 한다.  \n\n지난 포스팅에 대충 집고 넘어간 용어들에 대한 정리를 다시 하고 기본적인 사상들을 정리해보고자 한다. 허접한 플랫폼 엔지니어 생각이니 언제든 다른 의견을 환영하는 바이다. \n\n## What is spinnaker? (+History)\n최근 트렌드인 멀티 클라우드를 지향하는 오픈소스 플랫폼이다.  \n2014년 Netflix의 Asgard로 시작되어 2015년에 오픈소스화 되었다.  \n빠른 속도와 신뢰도있는 소프트웨어 릴리즈를 위해 만들어졌으며 대부분의 메이저 클라우드 프로바이더들을 지원한다.(AWS,GCP,Azure,openstack..)  \n현재 Netflix, Google, MS, Veritas등이 Contribution을 하고 있다.\n\n## 왜 Spinnaker를 써야하지?\n여러가지 이유가 있겠지만\n* Multi-Cloud용 Continuous Delivery/Deployment Platform 으로 대체가 가능\n* 다양한 pipeline 형태로 배포가 가능하고 Rollback이 쉬움\n* 빠른 배포가 가능하고 여러번 배포가 용이함\n* 유연한 pipeline management system을 가지고 있음\n* 다양한 배포전략을 가진다(Blue-Green, Rolling Red/Black, Canary)\n* community 활동 활발 (github, slack) - 답은 잘 안해줌 ㅠㅠ\n* VM과 Container 동시에 통합관리 가능\n* CI통합 용이(Jenkins)\n* CLI를 통한 설치 및 관리(halyard)\n* VM, Helm Packaging 가능\n* RBAC 지원\n* Notification - Email, Slack, Hipchat등\n* Safe Deployment - Judgement (승인기능)\n* Chaos Monkey Built-in\n\n이정도면 무조건 써야하지 않을까?\n\n## Jenkins vs Spinnaker\n\n|Jenkins|Spinnaker|\n|:----------:|:----------:|\n|강력한 빌드서버|\t클라우드 자원의 1차 연동|\n|완전한 deployment tool이 아님|vm & deployments 안에 빌드되어 있음|\n|스크립팅이 많이 필요함|별도의 스크립팅이 많이 필요없음|\n|기능들이 모두 플러그인 형태|CI tool이 아님(CI tools이 백엔드로)|\n\n## Kubernetes vs Spinnaker\n\n|Kubernetes|Spinnaker|\n|:----------:|:----------:|\n|리소스 사용 제한|정의한 퍼센트로 rollout|\n|slow rollout|각 단계별 검증 가능|\n|High rollback cost|Fast rollbacks|\n|Linear rollouts|resource 사용량이 큼|\n|검증단계가 없음||\n\n## Deploy Pipeline\n\nSpinnaker를 사용할때 기본적으로 아래와 같은 파이프라인으로 구성한다.  \n수동으로 UI나 API로 트리거링할수 있고, 자동으로 Jenkins 등과 트리거 연동하여 빌드완료시 배포되도록 할수 있다.\n\n![spinnaker-pipeline](/img/spinnaker-pipeline.png)\n\n## Deployment Strategies\nSpinnaker에서의 배포전략은 다음과 같이 제공된다.\n\n![deployment-strategies](https://www.spinnaker.io/concepts/deployment-strategies.png)\n\n### Red / Black (same as Blue / Green)\n* 동일한 양의 instance로 이루어진 새로운 Server Group을 생성한다\n* 신규 Server Group이 정상상태가 되면 LB는 신규 Server Group에 트래픽을 분산한다. \n  \n### Rolling red/black\n* 이전과 동일하지만 인스턴스별 또는 그룹별로 rolling\n  \n### Canary\n* 가장 작은 개수의 인스턴스를 교체시키고\n* 새로운 버전으로 트래픽을 분산시킨다 (1~5프로)\n* 새로운 버전에 이슈가 없을때까지 테스트를 진행하고\n* 특정시간까지 이슈가 없으면 배포를 늘려간다. \n\n## 용어정리 2탄\n\n이전 [post](https://ddiiwoong.github.io/2018/spinnaker-advanced-1/)에서 정리한걸 다시 복기하고 추가적인 내용을들 적어봤다.\n\n사용하면서 혼돈이 많이 생기는 부분이다 이게 GCE나 EC2를 쓰면 용어 매칭이 쉬운데 k8s를 위한 별도의 메뉴가 아닌 기능을 통합하다보니 용어가 조금 혼동스럽게 구성이 되었다.  \n특히 Load Balancer 부분은 Service로 매핑되고 퍼블릭 k8s에서 제공하는 Type LoadBalancer는 미지원한다.  \n그리고 모든 Resource들은 Deploy, Delete, Scale, Rollout(Undo, Pause, Resume)을 지원하며 Versioning이 지원된다.  Versioning은 [여기](https://www.spinnaker.io/reference/providers/kubernetes-v2/#strategy)에 설명된 대로 ```strategy.spinnaker.io/versioned``` annotation을 통해 manifest별로 재정의가 가능하다.\n\n| Spinnaker | Kubernetes | 비고 |\n|:----------:|:----------:|:-----------:|\n| Server Group | [Workloads](https://www.spinnaker.io/reference/providers/kubernetes-v2/#workloads) | [CRD의 경우 별도 Build](https://www.spinnaker.io/guides/developer/crd-extensions/) |\n| Clusters | Logical Server Group\t |  |\n| Load Balancer | Services | LoadBalancer(k8s) 미지원 |\n| Firewall | NetworkPolicies |  |\n\n### Application Management\nSpinnaker에서 Application 이란 배포하려는 서비스를 나타내는 구조라 생각하면 된다.  \n* pipeline\n* Clusters, Server Group의 집합이며, firewall과 loadbalancer를 포함한다.\n* Canary Config\n\n### Cluster\nKubernetes의 Cluster가 아니라 Spinnaker에서 Server Group의 논리적인 그룹\n\n### Server Group\n기본자원인 서버그룹은 배포할수 있는 artifacts(vm image, docker image, source)와 인스턴스(pod) 수, Auto-Scaling, metadata 등 기본 구성등을 가지고 있다.  \n서버그룹은 LoadBalacer나 Firewall 도 선택적으로 연결되고, vm이나 pod 형태로 배포된 application의 집합체라 볼수 있다.\n\n### Cloud Provider\n* IaaS - AWS, GCP, Azure, Oracle, Openstack\n* PaaS -  Google App Engine\n* Orchestrator - K8s, DC/OS\n* Docker v2 Registry\n\n### Account\nCloud Provider에 인증하기 위한 Spinnaker에서만 사용하는 Account Name\n\n### Pipeline\nPipeline은 주요 배포 관리도구로 사용된다. \nStage라고하는 일련의 Action으로 구성되며 파이프라인을 따라 Stage간 매개변수 전달이 가능하다.  \n수동으로 시작하거나, Jenkins 작업완료, Docker Registry 신규 Docker 이미지, Cron일정 또는 다른 Stage와 같은 이벤트에 의해 자동으로 트리거링되도록 구성할수 있다.  \nPipeline 실행중에(시작/완료/실패) mail, slack, hipchat(사라짐)을 통해 Alert가 가능하다.\n\n![pipeline](https://www.spinnaker.io/concepts/pipelines.png)\n\n### Stage (atomic building block)\nPipeline이 수행할 동작을 말한다.  \nDeploy, Resize, Disable, Manual Judgement 등을 수행할수 있다.\n\n![stage](https://www.spinnaker.io/concepts/pipelines/pipeline-tasks.png)\n\n* Stage - Multiple steps\n* Step - 진행되기전에 교정/폴링이 필요한 tasks\n* Task - 특정 Cloud Platform으로 동시에 여러 API호출\n* Operation - 단위 API\n\n## 정리\n\n용어나 개념은 어느정도 정리된듯 하고 다음 포스팅에서는 실제 multi cluster 환경에서 deploy하고 pipeline을 사용하는 내용을 적어볼 예정이다."
    },
    {
      "id": "kubernetes/knative/",
      "metadata": {
        "permalink": "/kubernetes/knative/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-08-01-knative.md",
        "source": "@site/blog/2018-08-01-knative.md",
        "title": "knative",
        "description": "knative 대해 알아봅니다",
        "date": "2018-08-01T00:00:00.000Z",
        "formattedDate": "August 1, 2018",
        "tags": [
          {
            "label": "Knative",
            "permalink": "/tags/knative"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "FaaS",
            "permalink": "/tags/faa-s"
          },
          {
            "label": "Serverless",
            "permalink": "/tags/serverless"
          },
          {
            "label": "CRDs",
            "permalink": "/tags/cr-ds"
          },
          {
            "label": "CloudEvents",
            "permalink": "/tags/cloud-events"
          },
          {
            "label": "Mesh",
            "permalink": "/tags/mesh"
          }
        ],
        "readingTime": 8.77,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "knative",
          "comments": true,
          "classes": "wide",
          "description": "knative 대해 알아봅니다",
          "slug": "kubernetes/knative/",
          "date": "2018-08-01T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Knative",
            "Kubernetes",
            "FaaS",
            "Serverless",
            "CRDs",
            "CloudEvents",
            "Mesh"
          ]
        },
        "prevItem": {
          "title": "Spinnaker on Kubernetes #2",
          "permalink": "/kubernetes/spinnaker-advanced-2/"
        },
        "nextItem": {
          "title": "Dex",
          "permalink": "/kubernetes/dex/"
        }
      },
      "content": "## Knative\n\n미친놈들 모여서 미친것을 만들었군.. ㅎㅎ \n예상했던 내용들을 현실로 만드는 클라스\n\nKubernetes 관련 비지니스를 하고 있는 입장에서 봐도 놀랄일이지만\n인프라 영역부터 개발자 영역까지 모두를 추상화시키는 클라우드 네이티브의 힘이란 참 대단하다.\n\n오늘은 knative에서 주요기능들을 둘러보고자 한다.\n\n개인적인 생각은 Kubernetes 진영이라고 해야하나 CNCF 진영이라고 해야하나..\n그동안 노래를 부르고 주목했던 CRDs, Operators, Serverless Workload, CloudEvents, Mesh Layer 개념과 영역을 흡수해서 그 기반으로 확장시키고 있다. \n\n```\nkubectl apply -f https://storage.googleapis.com/knative-releases/serving/latest/release.yaml\n```\n\n위 코드 내용을 상세하게 보고있지만 담은것들이 정말 많다. 그래도 다 이해하려면 하나하나 챙겨봐야 한다.\n\n오늘은 일단 코어 기능만 살펴본다.\n\n## Build\n\n`build`는 Knative의 주요 `custom resource`이고 이를 이용하여 fetch, build, package를 수행한다. repo의 소스를 빌드하고 컨테이너로 이미지로 만들고 그다음에 Knative `Serving`으로 보낸다고 한다. \n\n\n### Build 용어\n- `Build`는 여러 `steps`을 포함하고 `Builder`로 구체화된다\n- `Builder`는 컨테이너 이미지 유형\n- `Build`내 `steps`은 repository에 push를 할 수 있음\n- `BuildTemplate` 는 재활용가능한 템플릿\n- `Build`내 `source`는 kubernetes Volume으로 mount되는 데이터를 정의할수 있고 git, Google Cloud Storage, 컨테이너 이미지를 지원함.\n- kubernetes Secret을 사용하여 `ServiceAccount`로 인증함\n\n## Serving\n\nKnative `Serving`은 Kubernetes와 Istio를 기반으로 Serverless Workload가 클러스터에서 작동하는 방식을 정의하고 제어하는데 사용된다고 하지만 엄연히 Public FaaS(AWS Lambda등)와는 구별되어야 한다고 생각한다.\n\nServerless 라고 하는 용어를 쉽게 생각하는 사람들이 많은데 결국 나중에는 간단한 애플리케이션들은 다 Serverless 스타일로 전환될것이라는 사상을 서비스나 플랫폼에 넣고 있는 추세다. \n\nCNCF 진형의 Cloud Event라고 하는 이벤트 그리드방식의 표준화를 따라 가는것인지 아니면 새로운 스타일을 정의하려고 하는것인지는 그들의 향후 Cloud Native 로드맵에 달려있다 해도 무방할것 같다. \n\n- Serverless Container의 신속한 배치가 가능\n- Automatic scaling up and down to zero\n- Istio Component\n- 배포 된 코드 및 config의 특정 시점 스냅 샷\n\n### `Serving` Resources\nCRDs로 정의한 Objects 집합체, 이러한 Object들은 Serverless 워크로드 형태로 정의되고 사용된다. \n가장 인상깊고 중요한 문구는 `Request-driven compute that can scale to zero` 인 것 같다.\n\n한동안 유행했던 OpenPaaS, CF기반의 PaaS 플랫폼을 뛰어넘는 구축형 그것도 스프링부트 영역까지도 kubernetes 기반 이벤트 드리븐 서버리스로 간다는 이야기...\n\n이미 퍼블릭으로는 [GA](https://cloud.spring.io/spring-cloud-function/)도 되었다.\n\n우리 팀원들과 함께 열심히 vmware dispatch framework으로 개발하고 있지만 결국 pivotal과 vmware는 거의 한몸이기에 더욱 더 변화가 필요한 순간이다.\n\n![serving](https://github.com/knative/serving/raw/master/docs/spec/img/object_model.png)\n\n- `Route`는 사용자 서비스에 대한 HTTP endpoint를 제공.\n- `Revisions`은 code(function)와 config로 구성된 불변의 스냅샷. Route를 통해 endpoint를 할당받지 못한 Revision은 자동으로 kubernetes resource에서 삭제됨\n- `Configuration`은 요구되는 Revision 최신 상태를 기록하고 생성하고 추적할수 있음. 소스 패키지(git repo나 archive)를 컨테이너로 변환하기 위한 내용이나 메타데이터등을 포함시킬수 있음. \n- `Service`는 `Routes`와 `Configurations` 리소스의 추상화된 집합체. 모든 워크로드의 lifecycle을 관리함. 트래픽을 항상 최신의 revision으로 route되도록 정의할수 있음\n\n\n\n## Events\n\nCNCF의 [CloudEvent Spec.](https://www.cncf.io/events/) 기반으로 하는 이벤트를 produce/comsume 하는 방법을 제공한다. 플러그인 형태로 이벤트를 수신할수 있고 다양한 pub/sub 스타일의 broker service를 통해 제공될수 있다. \n\nAzure는 이미 [EventGrid서비스](https://azure.microsoft.com/ko-kr/services/event-grid/)를 GA를 한 상황이고 [Pivotal](https://pivotal.io/kr/knative) 진영도 Serverless Workload는 `Knative`기반으로 넘어간다고 했으니 [Dispatch](https://github.com/vmware/dispatch) 도 결국 따라가지 않을까 생각해본다. \n\n![eventing_concept](https://github.com/knative/docs/raw/master/eventing/concepts.png)\n\n### Bus\nKafka나 Nats와 같은 메시지 Bus를 통해 K8s기반의 pub/sub을 제공하는 개념. 이벤트는 Channel에 의해 게시되고 관심있는 사람에게 라우팅됨.\n\n- Channel : 여기서 이야기 하는 채널은 특정 bus에 사용되는 이벤트를 받기 위한 네트워크 기반 엔드포인트\n- Subscription : Channel에서 수신한 이벤트를 관심있는 target, DNS이름으로 표현되는 이벤트에 연결함\n- Bus : (kafka topic에 이벤트가 전달되는것 처럼) 특정 지속성 전략을 사용하여 Channel과 Subscription을 구현하는데 필요한 적용 계층을 정의함\n\n\n현재 3가지의 Bus가 제공됨 ([Kafka](https://github.com/knative/eventing/tree/master/pkg/buses/kafka), [Stub](https://github.com/knative/eventing/tree/master/pkg/buses/stub), [GCP PubSub](https://github.com/knative/eventing/tree/master/pkg/buses/gcppubsub))\n\n\n### Sources\n\nSource는 K8s 외부의 데이터 소스를 프로비저닝하고 이를 클러스터로 라우팅하기 위한 추상화 레이어를 제공함. 아래와 같은 소스들을 제공하고 있음\n\n- Feed : EventType과 Action (CloudEvents 호환 HTTP endpoint)간의 연결을 정의하는 기본 객체\n- EventType and ClusterEventType : EventSource에서 분리되는 공통스키마로 특정 이벤트의 집합, EventType은 Namespace 범위내에서 사용되고 ClusterEventType은 모든 Namespace에서 사용될수 있도록 관리자에 의해 설치됨\n- EventSource and ClusterEventSource : 하나 이상의 EventTypes를 생성 할 수있는 외부 시스템을 기술함\n\n현재 3가지 Sources를 제공함\n- K8sevents : [Kubernetes Events](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#event-v1-core)를 수집하고 CloudEvents 타입으로 표시함\n- Github : PR(pull request) notification을 수집하고 CloudEvents 타입으로 표시함\n- GCP PubSub : GCP PubSub topic으로 publish된 이벤트를 수집하고 CloudEvents 타입으로 표시함\n\n### Flows\n\n마지막으로 Source에서 Endpoint까지 묶어주는 Flow라고 부르는 높은 수준의 추상화가 있다.\nSpec으로 이벤트가 라우팅된 Channel과 Bus를 기재하여 사용할 수 있다.\nFlow는 Eventing에서 최상위 개념으로 사용자가 선택할수 있고, 외부 Source 이벤트에서 목적지까지 원하는 경로를 기술할수 있다.\n\n# 정리\n워낙 방대한 양을 가지고 있고 이해하려고 노력하면서 적다보니 번역위주로 되어버렸다. \n원래 다음번에 Nats Streaming을 다룰 예정이였으나, \n당분간은 knative 구성요소를 이해하고 적용하는 위주로 포스팅을 할 예정이다. 아마도 모듈별(Serving, Building, Eventing) 시리즈가 될 듯 하다."
    },
    {
      "id": "kubernetes/dex/",
      "metadata": {
        "permalink": "/kubernetes/dex/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-07-26-dex.md",
        "source": "@site/blog/2018-07-26-dex.md",
        "title": "Dex",
        "description": "Dex 대해 알아봅니다",
        "date": "2018-07-26T00:00:00.000Z",
        "formattedDate": "July 26, 2018",
        "tags": [
          {
            "label": "dex",
            "permalink": "/tags/dex"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "identity service",
            "permalink": "/tags/identity-service"
          },
          {
            "label": "OIDC",
            "permalink": "/tags/oidc"
          },
          {
            "label": "connectors",
            "permalink": "/tags/connectors"
          }
        ],
        "readingTime": 9.39,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Dex",
          "comments": true,
          "classes": "wide",
          "description": "Dex 대해 알아봅니다",
          "slug": "kubernetes/dex/",
          "date": "2018-07-26T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "dex",
            "Kubernetes",
            "identity service",
            "OIDC",
            "connectors"
          ]
        },
        "prevItem": {
          "title": "knative",
          "permalink": "/kubernetes/knative/"
        },
        "nextItem": {
          "title": "Cert-manager",
          "permalink": "/kubernetes/cert-manager/"
        }
      },
      "content": "## Dex - A federated OpenID Connect provider\n----\n### dex는 기본적으로 OpenID Connect를 사용하여 다른 애플리케이션의 인증을 하게 해주는 identity 서비스다.  \nhttps://github.com/coreos/dex\n\n\nDex는 \"connectors\"를 통해 다른 identity provider의 포털(게이트웨이, 중계자) 역할을 한다. 이를 통해 LDAP, SAML또는 GitHub, Google, AD(Active Directory)와 같은 기존 인증을 dex에게 위임할 수 있다. 클라이언트는 일단 인증 로직을 작성하여 dex와 통신하면 dex는 주어진 백엔드(여기서는 예시로 kubernetes 클러스터)에 대한 프로토콜을 처리하게 된다. \n\n하지만 향후 Cloud Native 인증 카탈로그 중 하나로서 생각하고 있는 오픈소스이다 보니 테스트는 진행해봐야 할 것 같다.\n\n### OpenID Connect\n\n기본적으로 Dex는 OIDC(OpenID Connect)를 사용한다.  \n그러므로 [kubernetes 클러스터 사용자 인증용도](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-kubectl)로 Dex를 붙일수 있는 것이다.  \n또한 위에서 말한것과 같이 다른 identity provider를 사용하여 사용자를 인증하는데 아래 그림과 같이 [connectors](https://github.com/coreos/dex#connectors)를 사용한다. \n\n![dex_image](/img/dex-flow.png)\n\n위 그림과 같이 \"connectors\"는 다른 identity provider를 통해 사용자를 인증하기 위해 dex에서 사용하는 기본 도구다. 결국 Dex는 기존에 활용하던 GitHub, LinkedIn 및 Microsoft AD와 같은 벤더 플랫폼이나 LDAP 및 SAML과 같은 기존 프로토콜 방식을 사용하여 kubernetes 클러스터 사용자 인증을 쉽게 구현할 수 있는 것이다. 큰 회사??들은 OpenLDAP이나 Microsoft Active Directory와 같은 사내 디렉토리 서비스를 운용하거나 내가 있는 작은 규모의 팀들은 대부분 Github이나 Google 계정은 가지고 있을 것이기 때문에 Dex와 같은 오픈소스를 사용함으로써 기존 identity provider 와 kubernetes를 쉽게 통합할 수 있다고 본다.\n\n![dex_kube](/img/dex_architect.png)\n\n### Kubernetes OIDC\n\n아래 그림처럼 Identity Provider 위치에 Dex를 구성하면 된다.\n\n![kube_openid](https://d33wubrfki0l68.cloudfront.net/d65bee40cabcf886c89d1015334555540d38f12e/c6a46/img/docs/admin/k8s_oidc_login.svg)\n\npublic managed kubernetes 환경에서는 현실적으로 kubeapi-server를 컨트롤하기 어렵기 때문에 일단 Native 클러스터에 구성을 하도록 한다.\n\n### Github 연동하기\n테스트로 Github을 연동해보자. 일단 아래와 같이 인증서를 생성하는 스크립트를 하나만든다.  \n아래에서 DNS.1 부분은 /etc/hosts로 테스트로 등록할 도메인으로 가상으로 만들었다. (generator.sh)\n\n```\n#!/bin/bash\n\nmkdir -p ssl\n\ncat << EOF > ssl/req.cnf\n[req]\nreq_extensions = v3_req\ndistinguished_name = req_distinguished_name\n[req_distinguished_name]\n[ v3_req ]\nbasicConstraints = CA:FALSE\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = dex.newtech.academy\nEOF\n\nopenssl genrsa -out ssl/ca-key.pem 2048\nopenssl req -x509 -new -nodes -key ssl/ca-key.pem -days 10 -out ssl/ca.pem -subj \"/CN=kube-ca\"\n\nopenssl genrsa -out ssl/key.pem 2048\nopenssl req -new -key ssl/key.pem -out ssl/csr.pem -subj \"/CN=kube-ca\" -config ssl/req.cnf\nopenssl x509 -req -in ssl/csr.pem -CA ssl/ca.pem -CAkey ssl/ca-key.pem -CAcreateserial -out ssl/cert.pem -days 10 -extensions v3_req -extfile ssl/req.cnf\n```\n\nNamespace도 하나 만들자. (dex-ns.yaml)\n\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dex\n```\n\n\nkubernetest 클러스터용 인증서를 생성한다.\n```\n$ ./generator.sh\n$ kubectl create -f dex-ns.yaml\n$ kubectl create secret tls dex.newtech.academy.tls -n dex --cert=ssl/cert.pem --key=ssl/key.pem\n$ mkdir pki\n$ cp ssl/ca.pem pki/openid-ca.pem\n```\n\nGithub에 가서 아래와 같이 Org OAuth App.을 신규로 생성한다.  \n\n![github](/img/dex_github.png)\n\n만든 위 앱정보로 Github-Client Secret을 만든다.\n\n```\n$ export GITHUB_CLIENT_ID=<Client ID>\n$ export GITHUB_CLIENT_SECRET=<Client Secret>\n\n$ kubectl create secret \\\n    generic github-client \\\n    -n dex \\\n    --from-literal=client-id=$GITHUB_CLIENT_ID \\\n    --from-literal=client-secret=$GITHUB_CLIENT_SECRET\n\n$ kubectl get secret\nNAME                  TYPE                                  DATA      AGE\ndefault-token-f29fb   kubernetes.io/service-account-token   3         27m\ndex.newtech.academy.tls    kubernetes.io/tls                     2         27m\ngithub-client         Opaque                                2         6s\n\n$ kubectl get secret github-client -o yaml\napiVersion: v1\ndata:\n  client-id: xxxxx\n  client-secret: xxxxxxx\nkind: Secret\nmetadata:\n  creationTimestamp: 2018-07-24T15:14:29Z\n  name: github-client\n  namespace: dex\n  resourceVersion: \"7044721\"\n  selfLink: /api/v1/namespaces/dex/secrets/github-client\n  uid: 42dfa0e9-8f54-11e8-8d6d-2aaad36eb114\ntype: Opaque\n```\n위에서 이야기 했지만 여기까지 아무생각없이 퍼블릭에서 구성하다가 생각해보니 kube-apiserver를 변경하려면 직접 클러스터를 써야한다는 이런 바보같은 실수를 또 저지르고 말았다. 그래서 다시 VM에 재구성을 ㅜㅜ\n\n그리고 kube-apiserver manifest 파일에 아래 내용을 추가한다.  \n/etc/kubernetes/manifests/kube-apiserver.manifest\n\n```\n    - --oidc-issuer-url=https://dex.newtech.academy:32000\n    - --oidc-client-id=example-app\n    - --oidc-ca-file=/pki/openid-ca.pem\n    - --oidc-username-claim=email\n    - --oidc-groups-claim=groups\n```\n\n그리고 kubelet 재시작을 한다. \n```\n$ systemctl restart kubelet\n```\n\n테스트를 위해 dex sample app을 빌드하고 만든 인증서와 함께 실행하게 되면 아래와 같이 샘플페이지를 볼수 있다. \n```\n$ sudo apt-get install make golang-1.9\n$ git clone https://github.com/coreos/dex.git\n$ cd dex\n$ git checkout v2.10.0\n$ export PATH=$PATH:/usr/lib/go-1.9/bin\n$ go get github.com/coreos/dex\n$ make bin/example-app\n$ export MY_IP=$(curl -s ifconfig.co)\n$ ./bin/example-app --issuer https://dex.newtech.academy:32000 --issuer-root-ca /pki/openid-ca.pem --listen http://${MY_IP}:5555 --redirect-uri http://${MY_IP}:5555/callback\n2018/07/25 14:37:52 listening on http://169.56.94.55:5555\n```\n![dex_sample](/img/dex_sample.png)\n\n그리고 로그인을 하게 되면 아래와 같이 Github 인증을 통해 kubernetes 클러스터에서 사용하려고 하는 Token을 획득할 수 있다.\n\n![kube_dex](/img/kube_dex.png)\n\n\n\n![dex_token](/img/dex_token.png)\n\n\n다음으로 위에서 만든 token을 사용하기 위해서 Role, RoleBinding과 실제 로그인할 github 계정을 User로 사용하도록 아래와 같이 작성한다.\n\n```\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: exampleUser\n  namespace: default\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: exampleUser\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: exampleUser\nsubjects:\n- kind: User\n  name: ddiiwoong@gmail.com\n  namespace: default\n```\n\n계정생성을 하고 위에서 만든 token을 가지고 developer 사용자를 설정하고 context설정로 dev-default로 생성한다.\n\n```\n$ kubectl create -f user.yaml\nrole.rbac.authorization.k8s.io \"exampleUser\" created\nrolebinding.rbac.authorization.k8s.io \"exampleUser\" created\n\n$ export TOKEN=<dex_sample_app_token>\n\n$ kubectl config set-credentials developer --auth-provider=oidc --auth-provider-arg=idp-issuer-url=https://dex.newtech.academy:32000 --auth-provider-arg=client-id=example-app --auth-provider-arg=idp-certificate-authority=/pki/openid-ca.pem  --auth-provider-arg=id-token=${TOKEN}\nUser \"developer\" set.\n\n$ kubectl config set-context dev-default --cluster=kubernetes --namespace=default --user=developer\nContext \"dev-default\" created.\n```\n\n위처럼 만들고 나면 현재 클러스터의 context들을 확인할수 있다. 확인후에 context를 dev-default로 바꿔보자.  \n새로운 context를 확인할수 있다. \n\n```\n$ kubectl config get-contexts\nCURRENT   NAME                  CLUSTER         AUTHINFO              NAMESPACE\n*         admin-cluster.local   cluster.local   admin-cluster.local   \n          dev-default           kubernetes      developer             default\n\n$ kubectl config use-context dev-default\nSwitched to context \"dev-default\".\n\n$ kubectl config get-contexts\nCURRENT   NAME                  CLUSTER         AUTHINFO              NAMESPACE\n          admin-cluster.local   cluster.local   admin-cluster.local   \n*         dev-default           kubernetes      developer             default\n\n$ kubectl get pod --all-namespaces\nNAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE\ndex           dex-64c4fb5b44-42d44                    1/1       Running   0          1h\ndex           dex-64c4fb5b44-r9p9d                    1/1       Running   0          1h\ndex           dex-64c4fb5b44-w5s2m                    1/1       Running   0          1h\nkube-system   calico-node-2bqll                       1/1       Running   0          13h\nkube-system   calico-node-cljb2                       1/1       Running   0          13h\nkube-system   calico-node-svh5n                       1/1       Running   0          13h\nkube-system   kube-apiserver-node1                    1/1       Running   0          1h\nkube-system   kube-controller-manager-node1           1/1       Running   2          13h\nkube-system   kube-dns-7bd4d5fbb6-7bb2j               3/3       Running   0          13h\nkube-system   kube-dns-7bd4d5fbb6-g8lz8               3/3       Running   0          13h\nkube-system   kube-proxy-node1                        1/1       Running   0          13h\nkube-system   kube-proxy-node2                        1/1       Running   0          13h\nkube-system   kube-proxy-node3                        1/1       Running   0          13h\nkube-system   kube-scheduler-node1                    1/1       Running   2          13h\nkube-system   kubedns-autoscaler-679b8b455-qxlvw      1/1       Running   0          13h\nkube-system   kubernetes-dashboard-55fdfd74b4-mbkm8   1/1       Running   0          13h\nkube-system   nginx-proxy-node2                       1/1       Running   0          13h\nkube-system   nginx-proxy-node3                       1/1       Running   0          13h\nkube-system   tiller-deploy-5c688d5f9b-52tls          1/1       Running   0          13h\n```\n\n### 정리\n처음에도 언급했듯이 kubernetes 인증을 위해 무거운 keycloak을 사용을 하는 경우들이 많은데 간단하게 사용할 수 있는 dex를 사용하여 여러가지 identity provider의 connector 역할로 사용하기에는 무난한것 같다.  \n결국 Dex를 이용하면 GitHub를 비롯해 다양한 OpenID, OAuth 2.0 인증 서비스와 Kubernetes 클러스터를 엮기 쉬울수 있다.  \n단, 최근 commit이 3-4달전이라는것과 처음 stable helm에 등록된 후 업데이트가 없었다는 점이 걱정되는 부분이긴 하다. (묘하게 Redhat과 합병 일정이 오버랩 되긴한다...)\n\n\n거의 2주만에 포스팅인데 '18 Google Next 키노트에서 [knative](https://github.com/knative/)라는 현재 개발중인 프로젝트와 유사한 오픈소스가 공개되었다. 다음번엔 knative도 한번 테스트를 해봐야 할듯 하다. \n\n퍼블릭과 프라이빗 클러스터에 대한 고민이 생기기 시작한다. 다시금 오픈소스 플랫폼에 집중하기 위해 프라이빗으로 돌아가야 하나라는 고민에도 빠져있는 상태에서 knative, GKE on Premise 같은 엔터프라이즈에서 프라이빗 구축을 위한 것들도 나오고 있어서 더더욱 고민에 빠진 요즘이다..."
    },
    {
      "id": "kubernetes/cert-manager/",
      "metadata": {
        "permalink": "/kubernetes/cert-manager/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-07-13-cert-manager.md",
        "source": "@site/blog/2018-07-13-cert-manager.md",
        "title": "Cert-manager",
        "description": "Cert-manager에 대해 알아봅니다",
        "date": "2018-07-13T00:00:00.000Z",
        "formattedDate": "July 13, 2018",
        "tags": [
          {
            "label": "Cert-manager",
            "permalink": "/tags/cert-manager"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "TLS",
            "permalink": "/tags/tls"
          },
          {
            "label": "Manage TLS Certificates",
            "permalink": "/tags/manage-tls-certificates"
          },
          {
            "label": "Security",
            "permalink": "/tags/security"
          },
          {
            "label": "HTTPS",
            "permalink": "/tags/https"
          }
        ],
        "readingTime": 8.14,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Cert-manager",
          "comments": true,
          "classes": "wide",
          "description": "Cert-manager에 대해 알아봅니다",
          "slug": "kubernetes/cert-manager/",
          "date": "2018-07-13T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Cert-manager",
            "Kubernetes",
            "TLS",
            "Manage TLS Certificates",
            "Security",
            "HTTPS"
          ]
        },
        "prevItem": {
          "title": "Dex",
          "permalink": "/kubernetes/dex/"
        },
        "nextItem": {
          "title": "Operators & CRDs(CustomResourceDefinitions) on Kubernetes",
          "permalink": "/kubernetes/operator/"
        }
      },
      "content": "## TLS 인증서\n\n제한된 예산과 시간, 인력으로 서비스를 만들다 보니 어려운 점들이 참 많다. 특히나 TLS 인증서 같은 경우 관리하는것이 은근히 귀찮다. 인증서 만료일 관리하는것 부터 어느 페이지까지를 평문으로만 두어야 하는지도... 그런데 바보같은 이슈를 저지르고 말았다. 도메인을 신청하고 fanout, Name based virtual hosting 방식중에 고민하다가 설계를 virtual host로 하였다.  \n\n그런데 문제는 어이없는곳에서 나왔다. *.xxxxx.io 라고 wildcard SSL 인증서를 신청하였는데 virtual host방식으로 2차 subdomin (예, api.stg.xxxxx.io)으로 ingress를 마구잡이로 생성하고 인증서를 적용을 했다. 하지만 인증서 오류 ㅋㅋ\n\n인터넷 비지니스를 거의 해보지 않아서 인지, 이런 기본적인 내용도 간과하고 서비스를 구상한 내가 모든걸 책임져야하는 상황이 와버렸다. \n\n설계를 이상하게 해서 wildcard 도메인을 몇십개를 신규로 계약해야하는 쓸데없는 비용을 들어야하는 처지가 되었다. 하지만 그냥 죽으란 법은 없는게 사람사는 세상이고 이런 고민들을 분명 다른사람들도 했을거야라고 생각하면서 떠오른게 let's encrypt 였고 찾아보니 이걸 또 kubernetes에서 발급부터 갱신까지 해주는 오픈소스를 찾아 적용까지 하고 이렇게 포스팅을 한다. \n\n\n### Cert-manager\n\n인증서 없이 구성할수도 있지만 기본적으로 kubernetes 는 secret과 ingress의 간단한 설정만으로 TLS를 구성할 수 있다.  \n\n\n* Cert-manager : K8s 클러스터내에서 TLS인증서를 자동으로 프로비저닝 및 관리하는 오픈소스\n* Let’s Encrypt :  자율적으로 작동하는 개방된 CA(Certificate authority-인증기관)으로 공공성(공공의 이익)을 위해서 운영되는 오픈소스\n\n\n\n[Cert-manager](http://docs.cert-manager.io/en/latest/)는 AWS의 Certificate Manager와 유사하게 인증서를 발급하고 설정할수 있다. 3개월 유효기간을 가지는 [let's encrypt](https://letsencrypt.org/)를 사용하기에 인증서 만료시 문제가 될거라 생각했지만 이것도 자동으로 갱신을 해주는 관리 app.을 같이 배포하기 때문에 인증서 관리에 대한 부담을 덜수 있다.  \n\n기본적으로 Cert-manager는 [let's encrypt](https://letsencrypt.org/), [Vault](https://www.vaultproject.io/) 같은 것을들 사용하여 인증서 발급을 할 수 있다. 인증서 발급후에도 만료가 되기 전에 바로 갱신을 자동으로 한다. \n\n\n### 구성\n![certmanager](http://docs.cert-manager.io/en/latest/_images/high-level-overview.png)\n\n### 사전준비사항\n* [helm](https://docs.helm.sh/using_helm/#installing-helm-client)\n* k8s cluster (1.7+) - CRD 사용이 가능해야함\n\n### 설치\n```\n$ helm install --name cert-manager --version v0.3.1 \\\n    --namespace kube-system stable/cert-manager\n\n$ helm status cert-manager\nLAST DEPLOYED: Fri Jun 15 10:02:45 2018\nNAMESPACE: kube-system\nSTATUS: DEPLOYED\n\nRESOURCES:\n==> v1/Pod(related)\nNAME                                        READY  STATUS   RESTARTS  AGE\ncert-manager-cert-manager-5f76b676b4-5tdh8  1/1    Running  0         2d\n\n==> v1/ServiceAccount\nNAME                       SECRETS  AGE\ncert-manager-cert-manager  1        28d\n\n==> v1beta1/CustomResourceDefinition\nNAME                               AGE\ncertificates.certmanager.k8s.io    28d\nclusterissuers.certmanager.k8s.io  28d\nissuers.certmanager.k8s.io         28d\n\n==> v1beta1/ClusterRole\ncert-manager-cert-manager  28d\n\n==> v1beta1/ClusterRoleBinding\nNAME                       AGE\ncert-manager-cert-manager  28d\n\n==> v1beta1/Deployment\nNAME                       DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\ncert-manager-cert-manager  1        1        1           1          28d\n\n\nNOTES:\ncert-manager has been deployed successfully!\n\nIn order to begin issuing certificates, you will need to set up a ClusterIssuer\nor Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).\n\nMore information on the different types of issuers and how to configure them\ncan be found in our documentation:\n\nhttps://cert-manager.readthedocs.io/en/latest/reference/issuers.html\n\nFor information on how to configure cert-manager to automatically provision\nCertificates for Ingress resources, take a look at the `ingress-shim`\ndocumentation:\n\nhttps://cert-manager.readthedocs.io/en/latest/reference/ingress-shim.html\n```\n\n### 인증서 설정 및 배포\n\n인증서 등록 및 만료시 noti를 받기 위한 변수 설정을 한다.\n```\n$ export EMAIL=zaction@sk.com\n```\n그리고 issuer manifest를 배포한다.  \n(letsencrypt-issuer.yaml)\n```\n# Copyright 2018 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\napiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: ''\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    http01: {}\n---\napiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: ''\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    http01: {}\n```\n\n두개의 endpoint를 생성한다. (Staging, Prod)\n\n```\n$ sed -e \"s/email: ''/email: $EMAIL/g\" letsencrypt-issuer.yaml | \\\n    kubectl apply -f-\nclusterissuer \"letsencrypt-staging\" created\nclusterissuer \"letsencrypt-prod\" created\n```\n\nWeb App을 HTTP Ingress 형태로 배포한다. TLS발급전에 미리 HTTP(80) Ingress를 배포해야 한다. 이후 Let's encrypt TLS배포 후 TLS spec을 추가하고 재배포를 해야한다. \n\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: auth-ingress\nspec:\n  rules:\n  - host: auth.dev.action.cloudz.co.kr\n    http:\n      paths:\n      - backend:\n          serviceName: faas-auth-service\n          servicePort: 8081\n        path: /\n``` \n\n### TLS 인증서 발급\n인증서를 발급할때는 위에서 만든 Ingress로 생성하고namespace별 생성이 필요하다.\n\n```\napiVersion: certmanager.k8s.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: auth-dev-tls\n  namespace: auth #### namespace별 생성\nspec:\n  secretName: auth-dev-tls\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n  commonName: auth.dev.action.cloudz.co.kr\n  dnsNames:\n  - auth.dev.action.cloudz.co.kr\n  acme:\n    config:\n    - http01:\n        ingress: auth-ingress ### 기존에 만든 HTTP ingress 명\n      domains:\n      - auth.dev.action.cloudz.co.kr\n```\n\n```\n$ kubectl apply -f auth-dev-tls.yaml\ncertificate \"auth-dev-tls\" created\n```\n\n이후 TLS 생성 상태를 확인한다. 이\u0010때 Message에서 ```Normal   CeritifcateIssued     Certificated issued successfully``` 문구가 확인되면 인증서 발급이 정상적으로 되었다고 볼 수 있다.  \n이때 발급되는 시간이 네트워크 환경에 따라 1~5분정도 소요될때로 있는것으로 보인다. \n``` \n$ kubectl describe -f auth-dev-tls.yaml\n...\nType     Reason                Message\n----     ------                -------\nWarning  ErrorCheckCertificate Error checking existing TLS certificate: secret \"auth-dev-tls\" not found\nNormal   PrepareCertificate    Preparing certificate with issuer\nNormal   PresentChallenge      Presenting http-01 challenge for domain foo.kubernetes.tips\nNormal   SelfCheck             Performing self-check for domain auth.dev.action.cloudz.co.kr\nNormal   ObtainAuthorization   Obtained authorization for domain auth.dev.action.cloudz.co.kr\nNormal   IssueCertificate      Issuing certificate...\nNormal   CeritifcateIssued     Certificated issued successfully\n```\n\nsecret이 생성된것을 확인할 수 있다. \n\n```\n$ kubectl get secret | grep tls\nauth-dev-tls                 kubernetes.io/tls                     2         3h\n```\n\n이후 위에서 만든 Ingress에 TLS를 적용한다.\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: auth-ingress\n  annotations:\n      ingress.bluemix.net/hsts: enabled=true maxAge=<31536000> includeSubdomains=true\n      ingress.bluemix.net/redirect-to-https: \"True\"\nspec:\n  rules:\n  - host: auth.dev.action.cloudz.co.kr\n    http:\n      paths:\n      - backend:\n          serviceName: faas-auth-service\n          servicePort: 8081\n        path: /\n  tls:\n  - hosts:\n    - auth.dev.action.cloudz.co.kr\n    secretName: auth-dev-tls\n```\n\n위 설정을 적용하고 상태를 확인한다.\n```\n$ kubectl apply -f auth-tls-ingress.yaml\n\n$ kubectl get ing\nNAME           HOSTS                          ADDRESS         PORTS     AGE\nauth-ingress   auth.dev.action.cloudz.co.kr   10.1.1.182   80, 443   14h\n```\n\n![cert-tls](/img/cert-tls.png)\n\n## 정리\n설계의 실수로 시작되었지만 정리하다보니 인증서 갱신이 자동을 된다는 점과 또한 Kubernetes환경에서 TLS관리가 간편하다는것을 알게 되었다. 인증서 비용을 아낀것도 하나의 성과이기도 하다. 인증서는 써야하겠고 정말 많은 애플리케이션들을 생성/삭제를 상시 해야하는 환경이라면 한번 적용해볼만한 가치가 있는것 같다."
    },
    {
      "id": "kubernetes/operator/",
      "metadata": {
        "permalink": "/kubernetes/operator/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-07-09-operator.md",
        "source": "@site/blog/2018-07-09-operator.md",
        "title": "Operators & CRDs(CustomResourceDefinitions) on Kubernetes",
        "description": "Operators, CRD(Custom Resource Definitions)에 대해서 알 수 있다.",
        "date": "2018-07-09T00:00:00.000Z",
        "formattedDate": "July 9, 2018",
        "tags": [
          {
            "label": "Operators",
            "permalink": "/tags/operators"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "CRDs",
            "permalink": "/tags/cr-ds"
          },
          {
            "label": "CustomResourceDefinitions",
            "permalink": "/tags/custom-resource-definitions"
          }
        ],
        "readingTime": 10.355,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Operators & CRDs(CustomResourceDefinitions) on Kubernetes",
          "comments": true,
          "classes": "wide",
          "description": "Operators, CRD(Custom Resource Definitions)에 대해서 알 수 있다.",
          "slug": "kubernetes/operator/",
          "date": "2018-07-09T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Operators",
            "Kubernetes",
            "CRDs",
            "CustomResourceDefinitions"
          ]
        },
        "prevItem": {
          "title": "Cert-manager",
          "permalink": "/kubernetes/cert-manager/"
        },
        "nextItem": {
          "title": "Cilium",
          "permalink": "/kubernetes/cilium-1/"
        }
      },
      "content": "## Operators in Kubernetes\n### Operators 란?\n\n정의([Definition](https://coreos.com/operators/)) : Kubernetes Application 을 패키징, 배포, 관리하는 방법. Helm과는 조금 달라 따로 이야기 해보고자 한다.\n\n기본적으로 Kubernetest Application은 kubernetes에 의해 배포되고 kubect과 kube-API를 사용하여 관리한다.\n\n결론적으로 말하면 Kubernetes에 내가 만든 application을 서비스하고 관리하려면 결국 Kubernetes의 API들을 모두 이해하고 사용할수 있어야 한다. 일반적인 개발자에게 진입장벽이 어느정도 있다고 보여지며 이를 모두 이해시키는것도 조직적인 측면에서는 낭비일수 도 있다. 그래서 Helm등을 통한 application배포 전략을 세우기도 하지만 이것도 한계가 있을수 있다. 그래서 Operator는 Kubernetes상에서 application을 관리하는 런타임이라고 생각하는게 맞는것 같다.\n\n![etcd](https://coreos.com/sites/default/files/inline-images/Overview-etcd_0.png)\n\n## Operators\n\nOperators는 application마다 운영정보를 넣을수 있다.\n\nKubernetes 에 배포된 응용프로그램의 모든 특성을 알 필요가 없고 이를 통해 사용자는 매니지드 클라우드 서비스 경험(기존 IaaS/PaaS관리와 유사)과 유사하게 운영이 가능하다.  \n\nOperator가 배포되면 Kubernetes API확장 개념인 CRDs(Custom\nResource Definitions)을 사용하여 관리할수 있다.\nKubernetes에 Stateful 서비스를 배포하는 단순하고 좋은 방법이 될수 있다.  \n\n예를 들면, Postgres 클러스터, Prometheus 클러스터, Etcd 클러스터 같이 운영측면의 application들을 유지, 운영하는데 쓰일수 있다. (자세한건 뒤쪽 예시로 설명하겠다)\n\n그러면 먼저 CRDs(Custom Resource Definitions)에 대해서 먼저 알아본다. CRDs에 대해서는 별도로 정리하려 하다가 내용이 많이 않아 핵심적인 내용만 같이 적어본다.\n\n* Custom Resource Definitions(a.k.a CRDs)\n* k8s Object를 확장해서 사용할 수 있는 가장 간단한 방법\n* CRDs는 Kubernetes의 확장 기능\n* Kubernetes 사용자가 클러스터내에서 직접 custom Object를 yaml형태로 생성, 수정, 삭제, 사용할수 있도록 하는 기능  \n  * K8s database(etcd)와 API를 그대로 활용할 수 있음)\n  * CRUD 가능 (Create, Read, Update, Delete)\n* 모든 클러스터에 동적으로 resource 등록/삭제 가능\n* Operators는 CRDs를 포함하고 있음\n  * Operator를 추가하면 CRDs가 등록됨\n* Resource 당 하나의 API 버전만 지원(~1.10버전, 1.11버전 이후 개수 제한 없어짐)\n* 1.8+ 이후부터 JSON 스키마 유효성 검증 가능\n** *주의사항 : etcd가 별도 분리된 managed서비스나 etcd 인스턴스가 분리된 환경에서 사용권고**\n\n### 주요 활용용도\n* Operators\n* Application 정보 저장\n* RouteRule on istio\n* GameServer on Agones\n\n## CRDs 관련 발표자료\nhttps://ddiiwoong.github.io/2018/openinfraday18/\n\n### Operators example\n[etcd](https://coreos.com/operators/etcd/docs/latest), [Rook](https://github.com/rook/rook), [Prometheus](https://coreos.com/operators/prometheus/docs/latest), [Vault](https://www.vaultproject.io/), [MySQL](https://github.com/oracle/mysql-operator), [Postgres](https://github.com/CrunchyData/postgres-operator), [Redis](https://github.com/spotahome/redis-operator), [kafka-비공식](https://github.com/nbogojevic/kafka-operator) 등 Operator로 배포될수 있는 예시들은 해당 링크를 보면 자세히 확인할 수 있다. (오라클이 공격적으로 K8s비지니스로 뛰어드는듯한 그림이다…)  \n\n주로 저장소나 키-밸류 스토어, RDB등의 운영안정성을 위한 클러스터 구성을 위한것들이 대부분이며 점점 도입을 해나가는 추세이긴것 같긴 하다. 이번에 진행해보고자 하는건 분산 키-밸류 스토어이자 kubernetes의 메인저장소로 쓰이는 etcd 이다.\n\n기본적으로 etcd cluster objects는 CRDs로 생성한다. Kubernetes 기본 resource가 아닌 CRDs 이기 때문에 안정성 측면에서 불안하다고 생각할수도 있다. 하지만 [User Aggregated API Servers](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/aggregated-api-servers.md) 를 적용하여 안정성, 유효성 검사 및 버전 관리가 개선되었다고 한다. Aggregated API를 사용하면 사용자에게 최소한의 영향을 주면서 Kubernetes objects가 생성되거나 사용자가 etcd operator를 배포,관리할수 있다. (말이 길어서 그렇지 그냥 etcd 클러스터 구성)\n\n현재 프로젝트는 베타로 0.9.2까지 나와 있으며 RedHat이 CoreOS를 합병하는 바람에 문서들이 업데이트가 늦어지는것 같긴하지만 조만간에 1.0이 나올것 같긴 하다.\n\n### [etcd-operator](https://github.com/coreos/etcd-operator/#overview)\netcd operator는 기본적으로 다음과 같은 기능을 한다.\n\n* Create and Destroy\n* Resize\n* Failover\n* Rolling upgrade\n* Backup and Restore\n\n#### Requirements\n* Kubernetes 1.8+\n* etcd 3.2.13+\n\n### Installation guide\n설치는 단순하다. 먼저 RBAC설정을 한다.\n```\n$ git clone https://github.com/coreos/etcd-operator.git\n$ cd etcd-operator\n$ example/rbac/create_role.sh\n```\n그리고 etcd-operator 배포\n\n```\n$ kubectl create -f example/deployment.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: etcd-operator\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: etcd-operator\n    spec:\n      containers:\n      - name: etcd-operator\n        image: quay.io/coreos/etcd-operator:v0.9.2\n        command:\n        - etcd-operator\n        # Uncomment to act for resources in all namespaces. More information in doc/clusterwide.md\n        #- -cluster-wide\n        env:\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n```\ndeployment.yaml 내용을 보면 CustomResourceDefinition이 존재하지 않는다.\netcd-operator가 자동으로 CRD를 생성하기 때문에 아래와 같이 CRD를 확인할 수 있다.\n\n```\n$ kubectl get customresourcedefinitions\nNAME                                    KIND\netcdclusters.etcd.database.coreos.com   CustomResourceDefinition.v1beta1.apiextensions.k8s.io\n```\n\n### etcd cluster create/resize/failover/upgrade\noperator를 이용하여 etc cluster를 구성한다. operator를 통한 클러스터 구성내용을 확인하면 아주 단순하다. 버전과 사이즈뿐이다.\n```\n$ cat example/example-etcd-cluster.yaml\napiVersion: \"etcd.database.coreos.com/v1beta2\"\nkind: \"EtcdCluster\"\nmetadata:\n  name: \"example-etcd-cluster\"\n  ## Adding this annotation make this cluster managed by clusterwide operators\n  ## namespaced operators ignore it\n  # annotations:\n  #   etcd.database.coreos.com/scope: clusterwide\nspec:\n  size: 3\n  version: \"3.2.13\"\n$ kubectl create -f example/example-etcd-cluster.yaml\netcdcluster.etcd.database.coreos.com/example-etcd-cluster created\n```\n\n순차적으로 etcd cluster가 구성되는것을 볼수 있다.\n\n```\n$ kubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\netcd-operator-69b559656f-wrhxz    1/1       Running   0          3h\nexample-etcd-cluster-2kd4t667j5   1/1       Running   0          1m\nexample-etcd-cluster-k4lxm96v7h   1/1       Running   0          1m\nexample-etcd-cluster-lm7mkhvldw   1/1       Running   0          1m\n```\n\n이번에는 scale-out 테스트를 진행한다.  \nexample/example-etcd-cluster.yaml 내용에서 ```size: 3``` 을 ```size: 5``` 로 변경하고 다시 적용하면 2 node가 추가로 생성된다.\n\n```\n$ kubectl apply -f example/example-etcd-cluster.yaml\n$ kubectl get pods\nNAME                              READY     STATUS    RESTARTS   AGE\netcd-operator-69b559656f-wrhxz    1/1       Running   0          3h\nexample-etcd-cluster-2kd4t667j5   1/1       Running   0          9m\nexample-etcd-cluster-2pwm84lrf4   1/1       Running   0          34s\nexample-etcd-cluster-97qk6gs4sp   1/1       Running   0          50s\nexample-etcd-cluster-k4lxm96v7h   1/1       Running   0          9m\nexample-etcd-cluster-lm7mkhvldw   1/1       Running   0          8m\n```\n\n반대로 줄이는것도 동일하다.\n\nfailover 테스트의 경우에도 그냥 pod를 삭제하거나 worker를 날리는것으로도 동일하게 spec을 유지하는 kubernetes resource 특성으로 인해 바로 생성이 되는것을 확인할수 있다.\n\n이번에는 operator만 날려보겠다. 아래처럼 operator deployment만 삭제를 해도 pods는 남아있는것을 볼수 있다.\n\n```\n$ kubectl delete -f example/deployment.yaml\ndeployment \"etcd-operator\" deleted\ndeployment.extensions \"etcd-operator\" deleted\n$ kubectl get pod\nNAME                              READY     STATUS    RESTARTS   AGE\nexample-etcd-cluster-2kd4t667j5   1/1       Running   0          14m\nexample-etcd-cluster-2pwm84lrf4   1/1       Running   0          5m\nexample-etcd-cluster-97qk6gs4sp   1/1       Running   0          5m\nexample-etcd-cluster-k4lxm96v7h   1/1       Running   0          14m\nexample-etcd-cluster-lm7mkhvldw   1/1       Running   0          13m\n```\n\netcd pod 하나를 날려본다. 4개 pod만 남은것을 확인할 수 있다.\n\n```\n$ kubectl delete pod example-etcd-cluster-2kd4t667j5 --now\npod \"example-etcd-cluster-2kd4t667j5\" deleted\n$ kubectl get pod\nNAME                              READY     STATUS    RESTARTS   AGE\nexample-etcd-cluster-2pwm84lrf4   1/1       Running   0          9m\nexample-etcd-cluster-97qk6gs4sp   1/1       Running   0          9m\nexample-etcd-cluster-k4lxm96v7h   1/1       Running   0          18m\nexample-etcd-cluster-lm7mkhvldw   1/1       Running   0          17m\n```\n\n다시한번 operator를 배포하게 되면 잠시후에 5개로 다시 pod가 복원된것을 확인할수 있다.\n\n```\n$ kubectl create -f example/deployment.yaml\ndeployment.extensions/etcd-operator created\n$ kubectl get pod\nNAME                              READY     STATUS    RESTARTS   AGE\netcd-operator-69b559656f-8ks8m    1/1       Running   0          44s\nexample-etcd-cluster-2pwm84lrf4   1/1       Running   0          11m\nexample-etcd-cluster-97qk6gs4sp   1/1       Running   0          11m\nexample-etcd-cluster-k4lxm96v7h   1/1       Running   0          20m\nexample-etcd-cluster-kskgvlbsm9   1/1       Running   0          10s\nexample-etcd-cluster-lm7mkhvldw   1/1       Running   0          19m\n```\n\n업그레이드의 경우도 다른 resource와 동일하게 version 부분을 변경하여 rollout이 가능하다.\n\n## 정리\n다음 소스를 통해 직접 operator를 개발이 가능하다.  \n(Source: https://coreos.com/operators/)\n\nOperator SDK를 사용하면 Kubernetes API 상세스펙을 배우지 않고도 쉽게 operator 빌드가 가능하다.\n또한 관리를 위한 Operator Lifecycle Manager나 Operator Metering 같은 기능을 사용하여 좀더 관리측면에서 강화하고자 하는 CoreOS진영의 노력이 보이는듯 하다.\n\n아직 alpha, beta단계의 operator 프로젝트들이 대부분이지만 helm차트와 같이 kubernetes API 스펙을 이해하고 사용하지 않고도 쉽게 운영자가 기반 어플리케이션을 관리 할수 있다는것으로 보아 향후 RedHat이나 Oracle 진영에서 본인들 kubernetes 관련 제품들을 홍보하고 서비스라인에 포함시키는 방향으로 적극적으로 개발을 하고 있는것으로 보인다. 앞으로 mysql, redis, kakfa 등을 operator로 배포하고 관리하는 일이 더 많아 질것 같다."
    },
    {
      "id": "kubernetes/cilium-1/",
      "metadata": {
        "permalink": "/kubernetes/cilium-1/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-07-06-cilium-1.md",
        "source": "@site/blog/2018-07-06-cilium-1.md",
        "title": "Cilium",
        "description": "Cilium에 대해 알아봅니다",
        "date": "2018-07-06T00:00:00.000Z",
        "formattedDate": "July 6, 2018",
        "tags": [
          {
            "label": "Cilium",
            "permalink": "/tags/cilium"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "network policy",
            "permalink": "/tags/network-policy"
          },
          {
            "label": "Istio",
            "permalink": "/tags/istio"
          },
          {
            "label": "BPF",
            "permalink": "/tags/bpf"
          },
          {
            "label": "CNI",
            "permalink": "/tags/cni"
          }
        ],
        "readingTime": 16.09,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Cilium",
          "comments": true,
          "classes": "wide",
          "description": "Cilium에 대해 알아봅니다",
          "slug": "kubernetes/cilium-1/",
          "date": "2018-07-06T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Cilium",
            "Kubernetes",
            "network policy",
            "Istio",
            "BPF",
            "CNI"
          ]
        },
        "prevItem": {
          "title": "Operators & CRDs(CustomResourceDefinitions) on Kubernetes",
          "permalink": "/kubernetes/operator/"
        },
        "nextItem": {
          "title": "Spinnaker on Kubernetes #1",
          "permalink": "/kubernetes/spinnaker-advanced-1/"
        }
      },
      "content": "## Cilium 누구냐 넌?\n처음에 놀랐다. 번역하면 '자궁', '섬모', '속눈썹'등 익숙치 않은 단어라서 놀랐지만 차근히 보다보니 결국 OS내부(리눅스 커널)에서부터 컨테이너간 또는 외부와의 연결을 보호하는 역할이라고 하니 이해가 되는 것 같다.\n\n홈페이지에 정의된 내용을 보면 아래 내용과 같다. \n\n>[Cilium](https://cilium.readthedocs.io/en/v1.1/intro/#what-is-cilium) - Docker 및 Kubernetes와 같은 Linux 컨테이너 관리 플랫폼을 사용하여 배포된 응용 프로그램 서비스 간의 네트워크 연결을 보호하는 오픈 소스 소프트웨어\n\n페북에 [한국 리눅스 사용자 그룹](https://www.facebook.com/groups/korelnxuser/)에서도 [Tommy Lee](https://www.facebook.com/tommy.lee.98229)님이나 [송창안](https://www.facebook.com/changan.song)님이 몇몇 게시물을 올려주셔서 관심있게 보던중에 아래와 같은 내용을 보고 이거다 하고 파보기 시작했다.\n\n>Cilium은 Docker 및 Kubernetes와 같은 리눅스 컨테이너 프레임 워크에 API 기반의 네트워크 보안 필터링을 제공하며, \n또한, BPF라는 새로운 Linux 커널 기술을 사용하여 컨테이너/POD ID를 기반으로 네트워크 계층 및 응용 프로그램 계층 보안 정책을 정의 및 적용하는 것에 있어서 간단하면서 효율적인 방법을 제공하고 있습니다.\n\n윗분들처럼 커널 자체를 분석하고 공부하고 기여하려면 소요되는 시간이 더 걸릴거 같고 서비스를 운영하고 개발하는 입장에서는 내 프로젝트에 적용 가능성을 검증하는것이 나을것 같아 정리를 해본다.\n\niptables을 기반으로 IP와 Port기반의 전통적인 포워딩 기술은 벌써 20년이라는 세월동안 널리 사용되어 왔다. 특히 퍼블릭/프라이빗 클라우드 제품군들 모두 iptables기반의 Security Group등을 기본으로 제공하고 있고 Kubernetes 마저도 CNI 핵심으로 iptables을 활용하고 있다.  \n\n동적으로 변화하고 매우 복잡한 마이크로서비스를 사용하는 시대에 전통적인 방식의 IP, Port관리는 비효율적인 측면이 없지 않다. BPF(아래인용)을 활용하여 리눅스 커널내에서 데이터 포워딩을 할 수 있고 Kubernetes Service기반 Load Balancing이나 istio와 같은 Service Mesh를 위한 Proxy Injection 을 통해 여러 활용을 할 수 있을거라고 Cilium 프로젝트는 이야기 하고 있다. \n\n> 버클리 패킷 필터(Berkeley Packet Filter, BPF)\n> \n>BPF는 버클리 패킷 필터(Berkeley Packet Filter)의 줄임말이다. 이름 그대로 패킷을 걸러내는 필터이다. 그런데 BSD에서의 BPF는 네트워크 탭(리눅스의 PF_PACKET)까지 아우르는 개념이다. 옛날 옛적에 유닉스에는 CSPF(CMU/Stanford Packet Filter)라는 게 있었는데 BPF라는 새 구조가 이를 대체했다. 이후 리눅스에서는 네트워크 탭을 나름의 방식으로 구현하고 패킷 필터 부분만 가져왔다. 리눅스의 패킷 필터를 리눅스 소켓 필터링(LSF: Linux Socket Filtering)이라고도 한다.  \n(발췌) https://wariua.github.io/facility/extended-bpf.html \n\nCilium은 Dockercon 2017에서 최초 [announce](https://www.youtube.com/watch?v=ilKlmTDdFgk)를 하였고 2018년 4월 24일에 1.0이 정식 Release된 이후 많은 관심을 받을것으로 예상되어 실제 서비스에 적용해볼 필요가 있을거 같아 minikube로 테스트한 내용을 끄젹여 본다. \n\n## Cilium Architecture\n![Cilium Architecture](/img/cilium_arch.png)\n\n## Main Feature\n* 고효율 BPF Datapath  \n  * 모든 데이터 경로가 클러스터 전체에 완전 분산\n  * Envoy같은 proxy injection 제공, 추후 sidecar proxy 형태 제공예정\n* CNI, CMM plugins  \n  * Kubernetes, Mesos, Docker에 쉽게 통합가능\n* Packet, API 네트워크 보안  \n  패킷기반 네트워크 보안과 API 인증을 결합하여 전통적인 패킷기반 네트워크 보안과 마이크로서비스 아키텍처 모두에게 보안 제공가능  \n  * [ID기반](http://docs.cilium.io/en/doc-1.0/concepts/#arch-id-security) - Source IP에만 의존하지 않고 모든패킷에 workload identity를 encoding하여 식별성 강화  \n  * [IP/CIDR기반](http://docs.cilium.io/en/doc-1.0/policy/language/#ip-cidr-based)이외에도 [Kubernetes Service기반](http://docs.cilium.io/en/doc-1.0/policy/language/#services-based)으로 정책 설정가능\n  * L7기반 [API보안](http://docs.cilium.io/en/doc-1.0/policy/language/#layer-7-examples) 적용가능\n* 분산,확장가능한 Load Balacing\n  BPF를 사용한 고성능 L3,L4 Load Balancer 제공\n  (Hasing, Weighted round-robin)\n  * kube-proxy 대체 - Kubernetes ClusterIP가 생성될때 BPF기반으로 자동으로 적용됨 \n  * [API driven](http://docs.cilium.io/en/doc-1.0/api/) - 직접 API를 활용하여 확장가능\n* 단순화된 네트워크 모델  \n  Overlay/VXLAN, Direct/Native Routing 지원\n* 가시성  \n   * [Microscope](https://github.com/cilium/microscope) - 클러스터 레벨에서 모든 이벤트 필터링 가능\n   * API기반 가시성 제공\n* 운영\n  * 클러스터 헬스체크 - HTTP, ICMP기준 클러스터 latency 체크가능\n  * Prometheus 통합 - 모든 메트릭을 Prometheus로 전달가능\n  * 클러스터 분석 및 [리포트툴](http://docs.cilium.io/en/doc-1.0/troubleshooting/#cluster-diagnosis-tool) \n\n## Requirement\n- kubectl >= 1.7.0\n- minikube >= 0.22.3\n\n[kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 설치와 [minikube](https://github.com/kubernetes/minikube/releases) 구성은 별도로 해야한다.\n\n## Start minikube\n넉넉하게 4GB 이상 메모리로 minikube를 구동한다.\n```\n$ minikube start --kubernetes-version v1.9.0 --network-plugin=cni --extra-config=kubelet.network-plugin=cni --memory=5120 \n```\n\n## Check minikube cluster status\nminikube구성이 완료되면 아래와 같이 클러스터 상태를 확인할수 있다. \n```\n$ kubectl get cs\nNAME                 STATUS    MESSAGE              ERROR\ncontroller-manager   Healthy   ok\nscheduler            Healthy   ok\netcd-0               Healthy   {\"health\": \"true\"}\n```\n\n## Install etcd (dependency of cilium)\ncilium 의존성을 위해 etcd를 별도로 배포한다. \n```\n$ kubectl create -n kube-system -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes/addons/etcd/standalone-etcd.yaml  \nservice \"etcd-cilium\" created\nstatefulset.apps \"etcd-cilium\" created\n```\n\n## Check all pods (etcd)\n모든 pod가 ```Running``` 상태인지 확인한다.\n```\n$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                               READY     STATUS    RESTARTS   AGE\nkube-system   etcd-cilium-0                      1/1       Running   0          1m\nkube-system   etcd-minikube                      1/1       Running   0          3m\nkube-system   kube-addon-manager-minikube        1/1       Running   0          4m\nkube-system   kube-apiserver-minikube            1/1       Running   0          3m\nkube-system   kube-controller-manager-minikube   1/1       Running   0          3m\nkube-system   kube-dns-86f4d74b45-lhzfv          3/3       Running   0          4m\nkube-system   kube-proxy-tcd7h                   1/1       Running   0          4m\nkube-system   kube-scheduler-minikube            1/1       Running   0          4m\nkube-system   storage-provisioner                1/1       Running   0          4m\n```\n\n## Install Cilium\nKubernetes 클러스터에 Cilium을 인스톨한다. 기본적으로 DaemonSet 형태로 배포되기 때문에 Node당 한개의 Cilium Pod를 볼 수 있다. Cilium은 ```kube-system``` namespace에서 실행된다.\n```\n$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/kubernetes/1.9/cilium.yaml\nconfigmap \"cilium-config\" created\nsecret \"cilium-etcd-secrets\" created\ndaemonset.extensions \"cilium\" created\nclusterrolebinding.rbac.authorization.k8s.io \"cilium\" created\nclusterrole.rbac.authorization.k8s.io \"cilium\" created\nserviceaccount \"cilium\" created\n```\nkube-system namespace에 RBAC설정과 함께 Cilium이 배포되고, ConfigMap, DaemonSet 형태로 배포가 된다. \n\n## Check deployment\n\nCilium Deployment가 ```READY``` 상태로 바뀔때까지 기다린다.\n\n\n```\n$ kubectl get daemonsets -n kube-system\nNAME      DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ncilium    1         1         1         1            0           <none>          2m\n```\n\n## Deploy Demo App.\n\n아래 데모그림을 보면 스타워즈의 영감을 받아서인지 deathstar, xwing 등으로 구분한것을 확인할수 있다. deathstar deployments의 경우 80포트로 http 웹서버가 2개의 pod replica로 Load Balancing되고 있다. deathstar 서비스는 우주선이 착륙할수 있도록 활주로를 서비스하고 있다. 하지만 제국군의 tiefighter 만 착륙하도록 해야하므로 보안설정을 해야하는 상황이다. \n\n![starwars](https://cilium.readthedocs.io/en/v1.1/_images/cilium_http_gsg.png)\n\n아래 ```http-sw-app.yaml``` 은 세가지 deployment를 가지고 있고 각각의 deployment는 ```(org=empire, class=deathstar)```, ```(org=empire, class=tiefighter)```, ```(org=alliance, class=xwing)```와 같이 label 정보를 가진다. deathstar Service는 ```(org=empire, class=deathstar)``` label을 가지고 Load Balancing을 한다. \n\n```\n$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/minikube/http-sw-app.yaml\nservice \"deathstar\" created\ndeployment \"deathstar\" created\ndeployment \"tiefighter\" created\ndeployment \"xwing\" created\n```\n\n총 4개의 pod와 1개의 서비스를 확인할수 있다.\n```\n$ kubectl get pods,svc\nNAME                             READY     STATUS    RESTARTS   AGE\npod/deathstar-566c89f458-mqgfs   1/1       Running   0          1h\npod/deathstar-566c89f458-wlc4c   1/1       Running   0          1h\npod/tiefighter                   1/1       Running   0          1h\npod/xwing                        1/1       Running   0          1h\n\nNAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/deathstar    ClusterIP   10.109.80.174   <none>        80/TCP    1h\nservice/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   4h\n```\n\n각각의 pod는 Cilium에서는 [Endpoints](https://cilium.readthedocs.io/en/v1.1/concepts/#endpoint)형태로 표현된다. 아래와 같이 ingress, egress policy를 확인할수 있고 아직 아무런 network policy 적용을 하지 않았기 때문에 모두 ```Disabled``` 상태로 보인다.\n\n```\n$ kubectl -n kube-system get pods -l k8s-app=cilium\nNAME           READY     STATUS    RESTARTS   AGE\ncilium-jmxk2   1/1       Running   0          4h\n\n$ kubectl -n kube-system exec cilium-jmxk2 -- cilium endpoint list\nENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])                                                    IPv6                 IPv4            STATUS\n           ENFORCEMENT        ENFORCEMENT\n5023       Disabled           Disabled          2008       k8s:io.cilium.k8s.policy.serviceaccount=istio-ingress-service-account          f00d::a0f:0:0:139f   10.15.170.241   ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio=ingress\n7415       Disabled           Disabled          9270       k8s:class=deathstar                                                            f00d::a0f:0:0:1cf7   10.15.16.224    ready\n                                                           k8s:io.cilium.k8s.policy.serviceaccount=default\n                                                           k8s:io.kubernetes.pod.namespace=default\n                                                           k8s:org=empire\n7979       Disabled           Disabled          4          reserved:health                                                                f00d::a0f:0:0:1f2b   10.15.96.215    ready\n17917      Disabled           Disabled          60941      k8s:class=tiefighter                                                           f00d::a0f:0:0:45fd   10.15.58.61     ready\n                                                           k8s:io.cilium.k8s.policy.serviceaccount=default\n                                                           k8s:io.kubernetes.pod.namespace=default\n                                                           k8s:org=empire\n22602      Disabled           Disabled          53004      k8s:io.cilium.k8s.policy.serviceaccount=istio-mixer-service-account            f00d::a0f:0:0:584a   10.15.190.2     ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio-mixer-type=telemetry\n                                                           k8s:istio=mixer\n31992      Disabled           Disabled          33709      k8s:io.cilium.k8s.policy.serviceaccount=istio-egressgateway-service-account    f00d::a0f:0:0:7cf8   10.15.85.192    ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio=egressgateway\n33958      Disabled           Disabled          64389      k8s:io.cilium.k8s.policy.serviceaccount=istio-citadel-service-account          f00d::a0f:0:0:84a6   10.15.59.151    ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio=citadel\n49215      Disabled           Disabled          40629      k8s:io.cilium.k8s.policy.serviceaccount=istio-mixer-service-account            f00d::a0f:0:0:c03f   10.15.48.171    ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio-mixer-type=policy\n                                                           k8s:istio=mixer\n55129      Disabled           Disabled          9270       k8s:class=deathstar                                                            f00d::a0f:0:0:d759   10.15.17.253    ready\n                                                           k8s:io.cilium.k8s.policy.serviceaccount=default\n                                                           k8s:io.kubernetes.pod.namespace=default\n                                                           k8s:org=empire\n55930      Disabled           Disabled          46893      k8s:app=prometheus                                                             f00d::a0f:0:0:da7a   10.15.196.220   ready\n                                                           k8s:io.cilium.k8s.policy.serviceaccount=prometheus\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n57491      Disabled           Disabled          775        k8s:io.cilium.k8s.policy.serviceaccount=istio-mixer-service-account            f00d::a0f:0:0:e093   10.15.253.210   ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio=statsd-prom-bridge\n57651      Disabled           Disabled          44171      k8s:io.cilium.k8s.policy.serviceaccount=istio-ingressgateway-service-account   f00d::a0f:0:0:e133   10.15.74.164    ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio=ingressgateway\n61352      Disabled           Disabled          888        k8s:io.cilium.k8s.policy.serviceaccount=istio-pilot-service-account            f00d::a0f:0:0:efa8   10.15.118.68    ready\n                                                           k8s:io.kubernetes.pod.namespace=istio-system\n                                                           k8s:istio=pilot\n61355      Disabled           Disabled          36797      k8s:class=xwing                                                                f00d::a0f:0:0:efab   10.15.56.79     ready\n                                                           k8s:io.cilium.k8s.policy.serviceaccount=default\n                                                           k8s:io.kubernetes.pod.namespace=default\n                                                           k8s:org=alliance\n```\n\n현재 상태에서는 모든 우주선이 deathstar에 착륙이 가능하다.\n```\n$ kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n\n$ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n```\n\n## L3/L4 Policy 적용\n\n결국 하고 싶은건 제국군 우주선 즉, tiefighter만 접근이 가능해야 하므로 아래처럼 정책을 설정한다. \n\n정책은 직관적으로 설정이 가능하다.\n\n```org=empire, class=deathstar``` label을 가진 endpoint로의 ingress 방향의 80포트 접근은 ```org=empire``` label을 가진 pod만 가능하도록 한다는 의미이다. \n\n```\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\ndescription: \"L3-L4 policy to restrict deathstar access to empire ships only\"\nmetadata:\n  name: \"rule1\"\nspec:\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: deathstar\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        org: empire\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n```\n\n또는 \n```\n$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/minikube/sw_l3_l4_policy.yaml\n```\n\n위와 같이 설정하고 나서 tiefighter를 착륙시켜보자.\n```\n$ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n```\n정상착륙!  \n이번에는 xwing 차례\n```\n$ kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\n\n```\n착륙실패!\n\n이어서 정책을 다시 확인해보면 Enabled로 변한 정책 2개를 확인할 수 있다.  \n(pod가 2개이므로 2개의 정책을 볼 수 있다)\n```\n$ kubectl -n kube-system exec cilium-jmxk2 -- cilium endpoint list\n```\n\n상세 정책 확인 (Very Simple!!)\n```\n$ kubectl get cnp\nNAME            AGE\nistio-sidecar   2h\nrule1           5m\n\n$  kubectl describe cnp rule1\nName:         rule1\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  cilium.io/v2\nKind:         CiliumNetworkPolicy\nMetadata:\n  Cluster Name:\n  Creation Timestamp:  2018-07-06T07:25:15Z\n  Generation:          0\n  Resource Version:    30312\n  Self Link:           /apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/rule1\n  UID:                 ba0d7964-80ed-11e8-8077-080027b1075c\nSpec:\n  Endpoint Selector:\n    Match Labels:\n      Any : Class:  deathstar\n      Any : Org:    empire\n  Ingress:\n    From Endpoints:\n      Match Labels:\n        Any : Org:  empire\n    To Ports:\n      Ports:\n        Port:      80\n        Protocol:  TCP\nStatus:\n  Nodes:\n    Minikube:\n      Enforcing:              true\n      Last Updated:           0001-01-01T00:00:00Z\n      Local Policy Revision:  65\n      Ok:                     true\nEvents:                       <none>\n```\n\n## L7 정책 적용\n마지막으로 deathstar API를 호출하는 서비스에 대한 정책을 제어하는것을 테스트해본다.\n\n\n아래 예시처럼 exhaust-port API(포트를 소진시키는 API)를 수행하면 특정 pod가 에러가 나고 재기동 되는것을 확인할수 있다. \n```\n$ kubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port\n\nPanic: deathstar exploded\n\ngoroutine 1 [running]:\nmain.HandleGarbage(0x2080c3f50, 0x2, 0x4, 0x425c0, 0x5, 0xa)\n        /code/src/github.com/empire/deathstar/\n        temp/main.go:9 +0x64\nmain.main()\n        /code/src/github.com/empire/deathstar/\n        temp/main.go:5 +0x85\n\n$ kubectl get pod\nNAME                         READY     STATUS    RESTARTS   AGE\ndeathstar-566c89f458-mqgfs   0/1       Error     0          1h\ndeathstar-566c89f458-wlc4c   1/1       Running   0          1h\ntiefighter                   1/1       Running   0          1h\nxwing                        1/1       Running   0          1h\n\n$ kubectl get pod\nNAME                         READY     STATUS    RESTARTS   AGE\ndeathstar-566c89f458-mqgfs   1/1       Running   1          1h\ndeathstar-566c89f458-wlc4c   1/1       Running   0          1h\ntiefighter                   1/1       Running   0          1h\nxwing                        1/1       Running   0          1h\n```\n\n그래서 이번에는 exhaust-port API는 차단하고 request-landing API만 허용하는 정책을 테스트해본다.\n```\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\ndescription: \"L7 policy to restrict access to specific HTTP call\"\nmetadata:\n  name: \"rule1\"\nspec:\n  endpointSelector:\n    matchLabels:\n      org: empire\n      class: deathstar\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        org: empire\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"POST\"\n          path: \"/v1/request-landing\"\n```\n또는\n```\n$ kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/v1.1/examples/minikube/sw_l3_l4_l7_policy.yaml\n\nciliumnetworkpolicy.cilium.io/rule1 configured\n```\n\n이후 동일한 테스트를 해보면 다른 결과를 확인할 수 있다. \n```\n$ kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nShip landed\n\n$ kubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port\nAccess denied\n```\n\n다시한번 상세 정책 확인을 해보면 ingress POST 허용정책을 확인할 수 있다.\n```\n$ kubectl describe ciliumnetworkpolicies rule1\nName:         rule1\nNamespace:    default\nLabels:       <none>\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"cilium.io/v2\",\"description\":\"L7 policy to restrict access to specific HTTP call\",\"kind\":\"CiliumNetworkPolicy\",\"metadata\":{\"annotations\":...\nAPI Version:  cilium.io/v2\nKind:         CiliumNetworkPolicy\nMetadata:\n  Cluster Name:\n  Creation Timestamp:  2018-07-06T07:25:15Z\n  Generation:          0\n  Resource Version:    32814\n  Self Link:           /apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/rule1\n  UID:                 ba0d7964-80ed-11e8-8077-080027b1075c\nSpec:\n  Endpoint Selector:\n    Match Labels:\n      Any : Class:  deathstar\n      Any : Org:    empire\n  Ingress:\n    From Endpoints:\n      Match Labels:\n        Any : Org:  empire\n    To Ports:\n      Ports:\n        Port:      80\n        Protocol:  TCP\n      Rules:\n        Http:\n          Method:  POST\n          Path:    /v1/request-landing\nStatus:\n  Nodes:\n    Minikube:\n      Annotations:\n        Kubectl . Kubernetes . Io / Last - Applied - Configuration:  {\"apiVersion\":\"cilium.io/v2\",\"description\":\"L7 policy to restrict access to specific HTTP call\",\"kind\":\"CiliumNetworkPolicy\",\"metadata\":{\"annotations\":{},\"name\":\"rule1\",\"namespace\":\"default\"},\"spec\":{\"endpointSelector\":{\"matchLabels\":{\"class\":\"deathstar\",\"org\":\"empire\"}},\"ingress\":[{\"fromEndpoints\":[{\"matchLabels\":{\"org\":\"empire\"}}],\"toPorts\":[{\"ports\":[{\"port\":\"80\",\"protocol\":\"TCP\"}],\"rules\":{\"http\":[{\"method\":\"POST\",\"path\":\"/v1/request-landing\"}]}}]}]}}\n\n      Enforcing:              true\n      Last Updated:           0001-01-01T00:00:00Z\n      Local Policy Revision:  70\n      Ok:                     true\nEvents:                       <none>\n```\n\ncilium CLI로도 확인이 가능하다.\n```\n kubectl -n kube-system exec cilium-jmxk2 cilium policy get\n[\n  {\n    \"endpointSelector\": {\n      \"matchLabels\": {\n        \"reserved:init\": \"\"\n      }\n    },\n    \"ingress\": [\n      {\n        \"fromEntities\": [\n          \"host\"\n        ]\n      }\n    ],\n    \"egress\": [\n      {\n        \"toPorts\": [\n          {\n            \"ports\": [\n              {\n                \"port\": \"53\",\n                \"protocol\": \"UDP\"\n              }\n            ]\n          }\n        ],\n        \"toEntities\": [\n          \"all\"\n        ]\n      },\n      {\n        \"toEndpoints\": [\n          {\n            \"matchLabels\": {\n              \"k8s:io.kubernetes.pod.namespace\": \"istio-system\"\n            }\n          }\n        ]\n      }\n    ],\n    \"labels\": [\n      {\n        \"key\": \"io.cilium.k8s.policy.name\",\n        \"value\": \"istio-sidecar\",\n        \"source\": \"k8s\"\n      },\n      {\n        \"key\": \"io.cilium.k8s.policy.namespace\",\n        \"value\": \"default\",\n        \"source\": \"k8s\"\n      }\n    ]\n  },\n  {\n    \"endpointSelector\": {\n      \"matchLabels\": {\n        \"any:class\": \"deathstar\",\n        \"any:org\": \"empire\",\n        \"k8s:io.kubernetes.pod.namespace\": \"default\"\n      }\n    },\n    \"ingress\": [\n      {\n        \"fromEndpoints\": [\n          {\n            \"matchLabels\": {\n              \"any:org\": \"empire\",\n              \"k8s:io.kubernetes.pod.namespace\": \"default\"\n            }\n          }\n        ],\n        \"toPorts\": [\n          {\n            \"ports\": [\n              {\n                \"port\": \"80\",\n                \"protocol\": \"TCP\"\n              }\n            ],\n            \"rules\": {\n              \"http\": [\n                {\n                  \"path\": \"/v1/request-landing\",\n                  \"method\": \"POST\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    ],\n    \"labels\": [\n      {\n        \"key\": \"io.cilium.k8s.policy.name\",\n        \"value\": \"rule1\",\n        \"source\": \"k8s\"\n      },\n      {\n        \"key\": \"io.cilium.k8s.policy.namespace\",\n        \"value\": \"default\",\n        \"source\": \"k8s\"\n      }\n    ]\n  }\n]\nRevision: 71\n```\n\n이외에도 Cilium Metric을 Prometheus에서 확인하는것도 간단하게 할수 있다고 한다. \n\n여기까지가 기본적으로 L3/L4, L7 기반 network policy를 적용해본것이고 다음번에는 istio와 연계 부분이나 실제 Cluster 구성방법에 대해서 다뤄보도록 하겠다."
    },
    {
      "id": "kubernetes/spinnaker-advanced-1/",
      "metadata": {
        "permalink": "/kubernetes/spinnaker-advanced-1/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-07-03-spinnaker-advanced-1.md",
        "source": "@site/blog/2018-07-03-spinnaker-advanced-1.md",
        "title": "Spinnaker on Kubernetes #1",
        "description": "Spinnaker에 대해 알아봅니다 #1",
        "date": "2018-07-03T00:00:00.000Z",
        "formattedDate": "July 3, 2018",
        "tags": [
          {
            "label": "CI/CD",
            "permalink": "/tags/ci-cd"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Spinnaker",
            "permalink": "/tags/spinnaker"
          }
        ],
        "readingTime": 3.725,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Spinnaker on Kubernetes #1",
          "comments": true,
          "classes": "wide",
          "description": "Spinnaker에 대해 알아봅니다 #1",
          "slug": "kubernetes/spinnaker-advanced-1/",
          "date": "2018-07-03T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "CI/CD",
            "Kubernetes",
            "Spinnaker"
          ]
        },
        "prevItem": {
          "title": "Cilium",
          "permalink": "/kubernetes/cilium-1/"
        },
        "nextItem": {
          "title": "Provisioning Dedicated Game Server on Kubernetes Cluster",
          "permalink": "/kubernetes/openinfraday18/"
        }
      },
      "content": "지난번 [OpenInfraDay발표](https://www.slideshare.net/openstack_kr/openinfra-days-korea-2018-track-4-provisioning-dedicated-game-server-on-kubernetes-cluster)때 질문을 해주셨는데 요즘 Spinnaker를 많이들 쓰거나 검토를 많이 하는것으로 알고 있다.  \n\nSpinnaker를 설치하는 내용은 많이 있으니 아래 halyard로 설치하는 포스트를 참고하면 된다.\n\n[윤상준의 기술 블로그 - Spinnaker 설치하기](https://yunsangjun.github.io/blog/spinnaker/2018/06/03/installing-spinnaker.html)\n\n이번에 이야기하고자 하는 부분은 실제 Spinnaker를 설치하고 난 후 운영상에 고려해야할 부분들과 팁들을 공유해보려고 한다.\n\n사실 Spinnaker를 구성하면서 가장 어려웠던 부분들은 용어, halyard config 관리와 custom resourse 인식부분 이였다. 나머지들은 뭐 튜토리얼을 따라가면 별로 어렵진 않으니 아래 내용을 차근차근 따라가면서 이해하면 될것 같다. \n\n## 용어들\n\n사용하면서 혼돈이 많이 생기는 부분이다 이게 GCE나 EC2를 쓰면 용어 매칭이 쉬운데 k8s를 위한 별도의 메뉴가 아닌 기능을 통합하다보니 용어가 조금 혼동스럽게 구성이 되었다.  \n특히 Load Balancer 부분은 Service로 매핑되고 퍼블릭 k8s에서 제공하는 Type LoadBalancer는 미지원한다.  \n그리고 모든 Resource들은 Deploy, Delete, Scale, Rollout(Undo, Pause, Resume)을 지원하며 Versioning이 지원된다.  Versioning은 [여기](https://www.spinnaker.io/reference/providers/kubernetes-v2/#strategy)에 설명된 대로 ```strategy.spinnaker.io/versioned``` annotation을 통해 manifest별로 재정의가 가능하다.\n\n| Spinnaker | Kubernetes | 비고 |\n|:----------:|:----------:|:-----------:|\n| Server Group | [Workloads](https://www.spinnaker.io/reference/providers/kubernetes-v2/#workloads) | [CRD의 경우 별도 Build](https://www.spinnaker.io/guides/developer/crd-extensions/) |\n| Clusters | Logical Server Group\t |  |\n| Load Balancer | Services | LoadBalancer(k8s) 미지원 |\n| Firewall | NetworkPolicies |  |\n\n\nSpinnaker Server Group으로 분류 된 항목은 모두 Spinnaker의 클러스터 탭에 표시가 된다. 가능한 경우 모든 포드가 표기되지만, 해당 [Workloads](https://www.spinnaker.io/reference/providers/kubernetes-v2/#workloads) 이외의 CRD(Custom Resource Definition)는 halconfig에서 아래와 같이 customResources config를 추가하면 deploy는 가능하나 Spinnaker UI에서 보이지는 않는다.  \n\n```\nkubernetes:\n      enabled: true\n      accounts:\n      - name: my-k8s-account\n        customResources:\n        - kubernetesKind: GameServer\n```\n\n이유는 바로 다음 링크처럼 [(CRD의 경우 별도 Build필요)](https://www.spinnaker.io/guides/developer/crd-extensions/) \n별도로 Java를 빌드해야 한다. Spinnaker Slack에 문의를 몇번했는데 질문하는 사람만 있고 답은 아무도 안해준다는...  \nhttps://spinnakerteam.slack.com/\n\n\n\n## Jenkins 연동\n\n[Spinnaker, Jenkins integration 상세내용 공식문서 ](https://www.spinnaker.io/setup/ci/jenkins/)\n\nJenkins와 연동하면서 가장 어이없이 헤맨부분은 아래 그림처럼 되어있어야 하는데 Security Realm을 Jenkins Default admin 계정만을 가지고 integration 하려다가 계속 실패하였다. Delegate to servlet container 말고 Jenkins 자체 사용자 DB로 별도 계정을 생성하고 아래 그림처럼 설정을 해야한다.\n\n![Jenkins Config](/img/jenkins_config.png)\n\n위 설정 이후 아래와 같이 Spinnaker UI에서 Jenkins API연동이 가능하다.  \n\n![Spinnaker Jenkins](/img/spin_jenkins.png)\n\n\n오늘은 여기까지만 하고 다음글에서는 배포전략이나 Network Policy 연동등을 상세히 적어볼 예정이다."
    },
    {
      "id": "kubernetes/openinfraday18/",
      "metadata": {
        "permalink": "/kubernetes/openinfraday18/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-07-01-openinfraday18.md",
        "source": "@site/blog/2018-07-01-openinfraday18.md",
        "title": "Provisioning Dedicated Game Server on Kubernetes Cluster",
        "description": "Provisioning Dedicated Game Server on Kubernetes Cluster",
        "date": "2018-07-01T00:00:00.000Z",
        "formattedDate": "July 1, 2018",
        "tags": [
          {
            "label": "CI/CD",
            "permalink": "/tags/ci-cd"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Agones",
            "permalink": "/tags/agones"
          },
          {
            "label": "Spinnaker",
            "permalink": "/tags/spinnaker"
          }
        ],
        "readingTime": 0.2,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "title": "Provisioning Dedicated Game Server on Kubernetes Cluster",
          "comments": true,
          "classes": "wide",
          "description": "Provisioning Dedicated Game Server on Kubernetes Cluster",
          "slug": "kubernetes/openinfraday18/",
          "date": "2018-07-01T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "CI/CD",
            "Kubernetes",
            "Agones",
            "Spinnaker"
          ]
        },
        "prevItem": {
          "title": "Spinnaker on Kubernetes #1",
          "permalink": "/kubernetes/spinnaker-advanced-1/"
        },
        "nextItem": {
          "title": "Mutating Webhook",
          "permalink": "/kubernetes/mutating-web-hook/"
        }
      },
      "content": "6월 28일에 Openinfraday에서 발표한 내용\n\n[![Slideshare](https://image.slidesharecdn.com/43dgs06270307-180628163517/95/openinfra-days-korea-2018-track-4-provisioning-dedicated-game-server-on-kubernetes-cluster-1-638.jpg?cb=1530203766)\n](https://www.slideshare.net/openstack_kr/openinfra-days-korea-2018-track-4-provisioning-dedicated-game-server-on-kubernetes-cluster)\n\n\n## 발표 영상\n  \n[![Video](http://img.youtube.com/vi/LtGGzKBoVZQ/0.jpg)](https://youtu.be/LtGGzKBoVZQ?t=0s)\n\nCustom Resource 에 대한 내용은 상세하게 더 정리해야겠다."
    },
    {
      "id": "kubernetes/mutating-web-hook/",
      "metadata": {
        "permalink": "/kubernetes/mutating-web-hook/",
        "editUrl": "https://github.com/ddiiwoong/newblog/tree/main/blog/2018-06-27-mutating-web-hook.md",
        "source": "@site/blog/2018-06-27-mutating-web-hook.md",
        "title": "Mutating Webhook",
        "description": "Mutating Webhook에 대해 알아봅니다",
        "date": "2018-06-27T00:00:00.000Z",
        "formattedDate": "June 27, 2018",
        "tags": [
          {
            "label": "Mutating Webhook",
            "permalink": "/tags/mutating-webhook"
          },
          {
            "label": "Kubernetes",
            "permalink": "/tags/kubernetes"
          },
          {
            "label": "Admission Controller",
            "permalink": "/tags/admission-controller"
          },
          {
            "label": "Istio",
            "permalink": "/tags/istio"
          },
          {
            "label": "Agones",
            "permalink": "/tags/agones"
          }
        ],
        "readingTime": 8.435,
        "truncated": false,
        "authors": [],
        "frontMatter": {
          "layout": "single",
          "classes": "wide",
          "title": "Mutating Webhook",
          "description": "Mutating Webhook에 대해 알아봅니다",
          "slug": "kubernetes/mutating-web-hook/",
          "date": "2018-06-27T00:00:00.000Z",
          "categories": [
            "Kubernetes"
          ],
          "tags": [
            "Mutating Webhook",
            "Kubernetes",
            "Admission Controller",
            "Istio",
            "Agones"
          ]
        },
        "prevItem": {
          "title": "Provisioning Dedicated Game Server on Kubernetes Cluster",
          "permalink": "/kubernetes/openinfraday18/"
        }
      },
      "content": "## Admission controller 확장\n\nKubernetes(이하 k8s)기반 개발 과제를 수행하다보니 Custom Resource를 사용할수 밖에 없는 상황들이 발생하였다.  \n그런 와중에 istio와 같은 Service Mesh Layer를 리서치하던 중에 튀어나온 MutatingAdmissionWebhook 용어를 이해하기 위에 조사한 내용을 정리해본다.\n\n<https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/>\n\n\nAdmission controller는 쿠버네티스 api-server의 오브젝트(Pod,등) 생성 요청을 가로체어 제어를 할 수 있는 확장 기능으로 플러그인 형태로 사용자가 추가 할 수 있다.   \n좀더 자세히 확인해보자 \n- 클러스터 관리자가 kube-api를 직접 컴파일 하고 구성해야 하기 때문에 유연하게 사용하기 어려움\n- 1.7 버전 이후부터는 이러한 제한사항을 해결하기 위해 alpha feature로  [Initializers](https://v1-9.docs.kubernetes.io/docs/admin/extensible-admission-controllers/#initializers) 와 [External Admission Webhooks](https://v1-9.docs.kubernetes.io/docs/admin/extensible-admission-controllers/#external-admission-webhooks) 기능이 도입됨\n- External Admission Webhooks 는 k8s 리소스의 유효성 검사를 하는데 활용, 유효성 검사를 통과 하지 못하면 해 당 리소스는 쿠버네티스에서 생성되질 않게 할 수 있음.\n- 1.9 버전에서는 External Admission Webhooks 은 beta로 승격되어 MutatingAdmissionWebhook 및 ValidatingAdmissionWebhook으로 나눠졌지만 Initializers 는 alpha 로 유지됨\n- MutatingAdmissionWebhook 은 유효성 검사 이외에도 승인 과정시 k8s object에 변경을 할수 있음\n  - 예를 들면 resource quota를 변경한다던지  Agones 및 istio와 같은 Custom Resource 를 수정하여 object를 생성이 가능함\n  - Webhook 방식은 gRPC 프로토콜을 사용하는 데 개발언어에 구애 받지 않고 확장을 할 수 있다는 장점이 있음\n\n\n## Webhook을 언제 쓰는가?\nWebhook을 사용하여 k8s cluster-admin이 api-server를 다시 컴파일하지 않고도 object생성 요청시 mutating(변경) 및 validation(유효성검증) 을 하는 플러그인을 만들 수 있다. \n\n이를 통해 개발자는 모든 resource 에서 여러 작업 ( \"CREATE\", \"UPDATE\", \"DELETE\"...)에 대한 승인 로직에 대해 사용자 정의 할 수있는 유연성을 제공받는다.\n\n\n## Use-Case\n- resource를 생성하기 전에 변경  \n(예, Istio 에서 처럼 traffic management 와 policy enforcement 을 위해 Envoy sidecar container를 injection)\n- StorageClass Provisioning 자동화  \n(PersistentVolumeClaim object 생성을 모니터링하고 미리 정의 된 정책에 따라 객체에 storage를 자동으로 추가. 사용자는 StorageClass 생성 에 신경 쓸 필요가 없음)\n- 복잡한 custom resource 검증 (Agones와 같은)namespace 제한  \n멀티 테넌트 시스템에서는 reserved namespace에 resource생성을 금지시킬때 사용할수 있음\n\n- 참고 예시  \n<https://github.com/kelseyhightower/denyenv-validating-admission-webhook>\n\n\n\n## 어떻게 동작하는가?\nMutatingWebhookConfiguration 내에 정의된 룰에 따라 etcd로 전달되기 전에 request를 intercept한다.  \nwebhook 서버에 승인 요청을 전송하여 변이를 실행한다.  \nwebhook 서버는 API를 준수하는 단순한 http서버.\n\n![Alt text](/img/mutating.jpg)\n\n## 튜토리얼\n<https://github.com/morvencao/kube-mutating-webhook-tutorial>  \n\n위 튜토리얼은 object가 생성되기 전에 pod에 nginx sidecar container를 inject하는 MutatingAdmissionWebhook을 배포하는 내용을 담고 있다.\n\n우선 admissionregistration.k8s.io/v1beta1 API를 사용할수 있는 k8s 1.9+ 이상의 클러스터가 필요하다.  \n\n## 확인방법\n```\n$ kubectl api-versions | grep admissionregistration.k8s.io/v1beta1\n```\n아래와 같은 결과가 나와야함\n```\nadmissionregistration.k8s.io/v1beta1\n```\n\n## Build하기\n\n일단 Go가 설치되어 있어야 한다.\n```~/go/src``` 아래에 clone을 하였음.\n\n```\n$ cd ~/go/src\n$ git clone https://github.com/morvencao/kube-mutating-webhook-tutorial.git\n```\n\n의존성 관리를 위해 repo는 dep를 사용함\n```\n$ cd kube-mutating-webhook-tutorial\n$ go get -u github.com/golang/dep/cmd/dep\n```\n\nbuild 파일 확인하고 registry 위치를 바꿈\n```\ndep ensure\nCGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o kube-mutating-webhook-tutorial .\ndocker build --no-cache -t registry.*****.io/agones/sidecar-injector:v1 .\nrm -rf kube-mutating-webhook-tutorial\n\ndocker push registry.*****.io/agones/sidecar-injector:v1\n```\n\nbuild하고 docker image push\n```\n$ ./build\nSending build context to Docker daemon  44.29MB\nStep 1/3 : FROM alpine:latest\n ---> 3fd9065eaf02\nStep 2/3 : ADD kube-mutating-webhook-tutorial /kube-mutating-webhook-tutorial\n ---> 8679ccbab536\nStep 3/3 : ENTRYPOINT [\"./kube-mutating-webhook-tutorial\"]\n ---> Running in 7699ff5c0885\nRemoving intermediate container 7699ff5c0885\n ---> 2014100d460e\nSuccessfully built 2014100d460e\nSuccessfully tagged registry.*****.io/agones/sidecar-injector:v1\nThe push refers to repository [registry.*****.io/agones/sidecar-injector]\n2456c1309a51: Pushed\ncd7100a72410: Layer already exists\nv1: digest: sha256:15c335daeba40ddcbfbc3631ab6daa7cf623b63420f0ae8b657755322ef0582d size: 739\n```\n\n\nsidecar deployment에 사용되는 secret(cert/key)을 생성한다.\n```\n./deployment/webhook-create-signed-cert.sh \\\n    --service sidecar-injector-webhook-svc \\\n    --secret sidecar-injector-webhook-certs \\\n    --namespace default\n```\n\n위에서 생성된 클러스터의 caBundle값을 가지고 MutatingWebhookConfiguration 생성한다.\n```\ncat deployment/mutatingwebhook.yaml | \\\n    deployment/webhook-patch-ca-bundle.sh > \\\n    deployment/mutatingwebhook-ca-bundle.yaml\n```\n\nresource들 deploy\n```\n$ kubectl create -f deployment/nginxconfigmap.yaml\nkubectl create -f deployment/configmap.yaml\nkubectl create -f deployment/deployment.yaml\nkubectl create -f deployment/service.yaml\nkubectl create -f deployment/mutatingwebhook-ca-bundle.yaml\n\nconfigmap \"nginx-configmap\" created\nconfigmap \"sidecar-injector-webhook-configmap\" created\ndeployment.extensions \"sidecar-injector-webhook-deployment\" created\nservice \"sidecar-injector-webhook-svc\" created\nmutatingwebhookconfiguration.admissionregistration.k8s.io \"sidecar-injector-webhook-cfg\" created\n```\n\nwebhook deployment 확인\n\n```\n$ kubectl get pods\nNAME                                                   READY     STATUS    RESTARTS   AGE\nsidecar-injector-webhook-deployment-796955558f-js6bb   1/1       Running   0          3m\n\n$ kubectl get deployment\nNAME                                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nsidecar-injector-webhook-deployment   1         1         1            1           3m\n```\n\ndefault 네임스페이스에 sidecar-injector 라벨링을 한다. 이렇게 하면 해당 네임스페이스에 생성되는 모든 app에 자동으로 injection하게 됨\n```\n$ kubectl get namespace -L sidecar-injector\nNAME             STATUS    AGE       SIDECAR-INJECTOR\nagones-system    Active    1d\ndefault          Active    19d       enabled\nibm-cert-store   Active    19d\nibm-system       Active    19d\ningress-test     Active    6d\nkube-public      Active    19d\nkube-system      Active    19d\nspinnaker        Active    12d\nxonotic          Active    1d\n```\n\n샘플앱을 디플로이 해보자\n```\n$ cat <<EOF | kubectl create -f -\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  replicas: 1\n  template:\n    metadata:\n      annotations:\n        sidecar-injector-webhook.morven.me/inject: \"yes\"\n      labels:\n        app: sleep\n    spec:\n      containers:\n      - name: sleep\n        image: tutum/curl\n        command: [\"/bin/sleep\",\"infinity\"]\n        imagePullPolicy:\nEOF\ndeployment.extensions \"sleep\" created\n```\n\nsidecar container injection 확인  \n아래 결과를 보면 하나의 deployment 하나의 container 생성을 요청했지만 nginx sidecar 컨테이너가 injection 된것을 확인할 수 있다.\n\n```\n$ kubectl get pod\nNAME                                                   READY     STATUS    RESTARTS   AGE\nsidecar-injector-webhook-deployment-796955558f-js6bb   1/1       Running   0          2h\nsleep-74b46f8bd7-r9l7f                                 2/2       Running   0          42s\n\n$ kubectl describe pod sleep-74b46f8bd7-r9l7f\nName:           sleep-74b46f8bd7-r9l7f\nNamespace:      default\nNode:           10.178.188.16/10.178.188.16\nStart Time:     Wed, 27 Jun 2018 13:12:47 +0900\nLabels:         app=sleep\n                pod-template-hash=3060294683\nAnnotations:    kubernetes.io/psp=ibm-privileged-psp\n                sidecar-injector-webhook.morven.me/inject=yes\n                sidecar-injector-webhook.morven.me/status=injected\nStatus:         Running\nIP:             172.30.169.30\nControlled By:  ReplicaSet/sleep-74b46f8bd7\nContainers:\n  sleep:\n    Container ID:  docker://728ca7f8e741ad29369312bc006c79683e7e605f3b04586df2477e233f93e451\n    Image:         tutum/curl\n    Image ID:      docker-pullable://tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      /bin/sleep\n      infinity\n    State:          Running\n      Started:      Wed, 27 Jun 2018 13:13:01 +0900\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-czzpj (ro)\n  sidecar-nginx:\n    Container ID:   docker://94fd41a0e153de6d5639873ccbd6b6325cee1ea8351dd02ab4a48ab4004d0b58\n    Image:          nginx:1.12.2\n    Image ID:       docker-pullable://nginx@sha256:72daaf46f11cc753c4eab981cbf869919bd1fee3d2170a2adeac12400f494728\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 27 Jun 2018 13:13:08 +0900\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /etc/nginx from nginx-conf (rw)\nConditions:\n  Type           Status\n  Initialized    True\n  Ready          True\n  PodScheduled   True\nVolumes:\n  default-token-czzpj:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-czzpj\n    Optional:    false\n  nginx-conf:\n    Type:        ConfigMap (a volume populated by a ConfigMap)\n    Name:        nginx-configmap\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason                 Age   From                    Message\n  ----    ------                 ----  ----                    -------\n  Normal  Scheduled              1m    default-scheduler       Successfully assigned sleep-74b46f8bd7-r9l7f to 10.178.188.16\n  Normal  SuccessfulMountVolume  1m    kubelet, 10.178.188.16  MountVolume.SetUp succeeded for volume \"nginx-conf\"\n  Normal  SuccessfulMountVolume  1m    kubelet, 10.178.188.16  MountVolume.SetUp succeeded for volume \"default-token-czzpj\"\n  Normal  Pulling                1m    kubelet, 10.178.188.16  pulling image \"tutum/curl\"\n  Normal  Pulled                 55s   kubelet, 10.178.188.16  Successfully pulled image \"tutum/curl\"\n  Normal  Created                55s   kubelet, 10.178.188.16  Created container\n  Normal  Started                55s   kubelet, 10.178.188.16  Started container\n  Normal  Pulling                55s   kubelet, 10.178.188.16  pulling image \"nginx:1.12.2\"\n  Normal  Pulled                 48s   kubelet, 10.178.188.16  Successfully pulled image \"nginx:1.12.2\"\n  Normal  Created                48s   kubelet, 10.178.188.16  Created container\n  Normal  Started                48s   kubelet, 10.178.188.16  Started container\n```\n\n## 정리\n결국 위에서 언급한것 처럼 MutationWebhook은 istio RouteRule같은 별도의 CustomResource등을 injection 하거나 agones 등과 같이 게임서버외에 client sdk 통신을 위한 injection 형태로 기존 resource에 추가적인 변경(mutation) 또는 검증(validation)등의 추가적인 작업을 kube-api의 컴파일없이 가능하다는데 목적이 있다고 볼 수 있다. 추가적으로 기능에 대한 내용은 이후 다시 정리해볼 예정이다."
    }
  ]
}